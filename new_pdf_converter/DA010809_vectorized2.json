{
  "document_metadata": {
    "document_id": "e0d325ff-56bd-471a-bbd9-de90cf32adbb",
    "filename": "DA010809.pdf",
    "file_path": "C:\\Users\\USER\\Downloads\\DA010809.pdf",
    "file_size": 52350687,
    "processed_at": "2025-10-16T17:32:50.193048",
    "source_hash": "5d74a62648357eefad732906554e8a388e73397ec5ca625b9f6828752c24a34d",
    "processor_version": "pymupdf4llm_v1.0",
    "document_type": "general",
    "language": "en"
  },
  "chunks": [
    {
      "chunk_id": "f7718fe0-30a8-4326-a953-38f08249e890",
      "content": "#### E(n)-Equivariant Graph Neural Networks Emulating Mesh-Discretized Physics March 2023 Masanobu Horie",
      "chunk_metadata": {
        "section_title": "E(n)-Equivariant Graph Neural Networks Emulating Mesh-Discretized Physics March 2023 Masanobu Horie",
        "section_level": 4,
        "page": 1,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 12,
        "char_count": 104,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "e4c68bfb-be11-4588-b265-19731d52f21d",
      "content": "#### E(n)-Equivariant Graph Neural Networks Emulating Mesh-Discretized Physics",
      "chunk_metadata": {
        "section_title": "E(n)-Equivariant Graph Neural Networks Emulating Mesh-Discretized Physics",
        "section_level": 4,
        "page": 2,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 8,
        "char_count": 78,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "66602baf-a2e3-40b3-b3ba-1f5c76b6dbe7",
      "content": "###### Graduate School of Science and Technology Degree Programs in Systems and Information Engineering University of Tsukuba",
      "chunk_metadata": {
        "section_title": "Graduate School of Science and Technology Degree Programs in Systems and Information Engineering University of Tsukuba",
        "section_level": 6,
        "page": 2,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 17,
        "char_count": 125,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "4e2503f0-7f12-48e1-ac31-b5aac91116e0",
      "content": "#### March 2023 Masanobu Horie",
      "chunk_metadata": {
        "section_title": "March 2023 Masanobu Horie",
        "section_level": 4,
        "page": 2,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 5,
        "char_count": 30,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "0c6692ae-4bf6-4a8a-be58-f1f5770c3799",
      "content": "# **Acknowledgment**\n\nFirst and foremost, I would like to thank my supervisor Asst. Prof. Naoto Mitsume for all\n\n\nhis support. I tremendously appreciate his inspiring advice for research, countless fruitful\n\n\ndiscussions, and enjoyable time. In addition to technical matters, he has shown me the\n\n\nimportance of telling a clear story, thinking about the relationship between my and others’\n\n\nresearch, and having social connections with other researchers.\n\n\nI am grateful to Prof. Daigoro Isobe, Assoc. Prof. Tetsuya Matsuda, Assoc. Prof.\n\n\nMayuko Nishio, and Assoc. Prof. Mitsuteru Asai for reviewing my research and providing\n\n\ninsightful feedback as my dissertation committee.\n\n\nI would like to thank my collaborators, Asst. Prof. Naoki Morita, Dr. Toshiaki Hish\n\ninuma, and Mr. Yu Ihara. I learned a lot of technical things thanks to their support.\n\n\nThis work was supported by JSPS KAKENHI (Grant Number 19H01098), JSPS Grant",
      "chunk_metadata": {
        "section_title": "**Acknowledgment**",
        "section_level": 1,
        "page": 3,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 140,
        "char_count": 930,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "76cf39a2-04ec-4f62-a451-3c4fcd6a975b",
      "content": "in-Aid for Scientific Research (B) (Grant Number 22H03601), JST PRESTO (Grant Num\n\nber JPMJPR21O9), and NEDO (Grant Number JPNP14012). I gratefully acknowledge\n\n\ntheir support.\n\n\nRICOS Co. Ltd. also supports the work in terms of computational resources and pro\n\nvides me an opportunity to pursue this Ph.D. project.\n\n\nFinally, I would like to thank my family, my father Jun-ichi, mother Naoko, sister\n\n\nYuriko, and wife Suzuka, for their continuous support. My wife, Suzuka, has been encour\n\naging me at any time. She made me excellent food like tempura, karaage, and ozouni when\n\n\nI was happy or unhappy with my research. It is awesome to share my life with you.\n\n\ni",
      "chunk_metadata": {
        "section_title": "**Acknowledgment**",
        "section_level": 1,
        "page": 3,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 109,
        "char_count": 667,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "73d65c32-d653-498c-b1f9-93476be404fd",
      "content": "# **Contents**\n\n**List of Figures** **ix**\n\n\n**List of Tables** **xv**\n\n\n**Nomenclature** **xix**\n\n\n**1** **Introduction** **1**\n\n\n1.1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\n\n\n1.2 Objective and Scope . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n\n\n1.3 Outline of Dissertation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n\n\n**2** **Background** **7**\n\n\n2.1 Machine Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n\n\n2.1.1 Foundations of Supervised Learning . . . . . . . . . . . . . . . . . 7\n\n\n2.1.2 Graph Neural Networks (GNNs) . . . . . . . . . . . . . . . . . . . 9\n\n\n2.1.2.1 Graph . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n\n\n2.1.2.2 Pointwise MLP . . . . . . . . . . . . . . . . . . . . . . 12\n\n\n2.1.2.3 Message Passing Neural Networks (MPNNs) . . . . . . . 14\n\n\n2.1.2.4 Graph Convolutional Network (GCN) . . . . . . . . . . 15",
      "chunk_metadata": {
        "section_title": "**Contents**",
        "section_level": 1,
        "page": 5,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 292,
        "char_count": 940,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "cd374a31-d1ca-4a6c-baf8-89d3f7ab9e23",
      "content": "2.1.3 Equivariance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n\n\niii",
      "chunk_metadata": {
        "section_title": "**Contents**",
        "section_level": 1,
        "page": 5,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 33,
        "char_count": 85,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "06dad22b-59a0-45a0-9df0-34ba285e0e9c",
      "content": "2.1.3.1 Group Theory . . . . . . . . . . . . . . . . . . . . . . . 17\n\n\n2.1.3.2 Equivariant Model . . . . . . . . . . . . . . . . . . . . . 18\n\n\n2.2 Numerical Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n\n\n2.2.1 Partial Differential Equations (PDEs) with Boundary Conditions . . 21\n\n\n2.2.2 Discretization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\n\n\n2.2.3 Nonlinear Solver and Optimization . . . . . . . . . . . . . . . . . 24\n\n\n2.2.3.1 Basic Formula for Iterative Methods . . . . . . . . . . . 24\n\n\n2.2.3.2 Newton–Raphson Method and Quasi-Newton Method . . 25\n\n\n2.2.3.3 Gradient Descent Method . . . . . . . . . . . . . . . . . 26\n\n\n2.2.3.4 Barzilai–Borwein Method . . . . . . . . . . . . . . . . . 27\n\n\n2.2.4 Numerical Analysis from a Graph Representation View . . . . . . . 29\n\n\n2.2.4.1 Finite Difference Method . . . . . . . . . . . . . . . . . 29\n\n\n2.2.4.2 Finite Element Method (FEM) . . . . . . . . . . . . . . 32",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 6,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 280,
        "char_count": 965,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "0435f4f5-6f46-4400-aefa-15eb8a6527c6",
      "content": "2.2.4.3 Least Squares Moving Particle Semi-Implicit (LSMPS)\n\n\nMethod . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\n\n\n**3** **IsoGCN: E(** _**n**_ **)-Equivariant Graph Convolutional Network** **39**\n\n\n3.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\n\n\n3.2 Related Prior Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41\n\n\n3.2.1 GCN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41\n\n\n3.2.2 TFN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41\n\n\n3.2.3 GNN Model for Physical Simulation . . . . . . . . . . . . . . . . . 42\n\n\n3.3 Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42\n\n\n3.3.1 Discrete Tensor Field . . . . . . . . . . . . . . . . . . . . . . . . . 42\n\n\n3.3.2 Isometric Adjacency Matrix (IsoAM) . . . . . . . . . . . . . . . . 44",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 6,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 305,
        "char_count": 879,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "f0758aa6-4637-4fc7-a9de-f60394d89b73",
      "content": "3.3.2.1 Definition of IsoAM . . . . . . . . . . . . . . . . . . . . 44\n\n\n3.3.2.2 Property of IsoAM . . . . . . . . . . . . . . . . . . . . . 46\n\n\n3.3.3 Construction of IsoGCN . . . . . . . . . . . . . . . . . . . . . . . 50\n\n\n3.3.3.1 E( _n_ )-Invariant Layer . . . . . . . . . . . . . . . . . . . 51\n\n\n3.3.3.2 E( _n_ )-Equivariant Layer . . . . . . . . . . . . . . . . . . 51\n\n\n3.3.4 IsoAM Refined for Numerical Analysis . . . . . . . . . . . . . . . 54\n\n\n3.3.4.1 Definition of Differential IsoAM . . . . . . . . . . . . . 54\n\n\n3.3.4.2 Partial Derivative . . . . . . . . . . . . . . . . . . . . . 55\n\n\n3.3.4.3 Gradient . . . . . . . . . . . . . . . . . . . . . . . . . . 56\n\n\n3.3.4.4 Divergence . . . . . . . . . . . . . . . . . . . . . . . . . 56\n\n\n3.3.4.5 Laplacian Operator . . . . . . . . . . . . . . . . . . . . 57\n\n\n3.3.4.6 Jacobian and Hessian Operators . . . . . . . . . . . . . . 58\n\n\n3.3.5 IsoGCN Modeling Details . . . . . . . . . . . . . . . . . . . . . . 59",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 7,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 322,
        "char_count": 970,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "376060b9-b3ab-432a-a342-fed987eb2ba5",
      "content": "3.3.5.1 Activation and Bias . . . . . . . . . . . . . . . . . . . . 59\n\n\n3.3.5.2 Preprocessing of Input Feature . . . . . . . . . . . . . . 59\n\n\n3.3.5.3 Scaling . . . . . . . . . . . . . . . . . . . . . . . . . . . 60\n\n\n3.3.5.4 Tensor Rank . . . . . . . . . . . . . . . . . . . . . . . . 60\n\n\n3.3.5.5 Implementation . . . . . . . . . . . . . . . . . . . . . . 60\n\n\n3.4 Numerical Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61\n\n\n3.4.1 Differential Operator Dataset . . . . . . . . . . . . . . . . . . . . . 62\n\n\n3.4.1.1 Task Definition . . . . . . . . . . . . . . . . . . . . . . 62\n\n\n3.4.1.2 Model Architectures . . . . . . . . . . . . . . . . . . . . 63\n\n\n3.4.1.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . 64\n\n\n3.4.2 Anisotropic Nonlinear Heat Equation Dataset . . . . . . . . . . . . 68",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 7,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 285,
        "char_count": 832,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "fded2d54-9cc4-4444-980e-75a1f348f9ef",
      "content": "3.4.2.1 Task Definition . . . . . . . . . . . . . . . . . . . . . . 68\n\n\n3.4.2.2 Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . 69\n\n\n3.4.2.3 Input and Output Features . . . . . . . . . . . . . . . . . 71\n\n\n3.4.2.4 Model Architectures . . . . . . . . . . . . . . . . . . . . 72\n\n\n3.4.2.5 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . 74\n\n\n3.5 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79\n\n\n**4** **Physics-Embedded Neural Network:** **Boundary Condition and Implicit**\n\n\n**Method** **81**\n\n\n4.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81\n\n\n4.2 Related Prior Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83\n\n\n4.2.1 Physics-Informed Neural Network (PINN) . . . . . . . . . . . . . 83\n\n\n4.2.2 Graph Neural Network Based PDE Solver . . . . . . . . . . . . . . 84\n\n\n4.3 Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 8,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 332,
        "char_count": 974,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "80bfc980-9a0c-4e8c-ab4d-987f5713f63d",
      "content": "4.3.1 Dirichlet Boundary Model . . . . . . . . . . . . . . . . . . . . . . 85\n\n\n4.3.1.1 Boundary Encoder . . . . . . . . . . . . . . . . . . . . . 85\n\n\n4.3.1.2 Dirichlet Layer . . . . . . . . . . . . . . . . . . . . . . . 86\n\n\n4.3.1.3 Pseudoinverse Decoder . . . . . . . . . . . . . . . . . . 86\n\n\n4.3.2 Neumann Boundary Model . . . . . . . . . . . . . . . . . . . . . . 87\n\n\n4.3.2.1 Definition of NeumannIsoGCN (NIsoGCN) . . . . . . . 88\n\n\n4.3.2.2 Derivation of NIsoGCN . . . . . . . . . . . . . . . . . . 88\n\n\n4.3.2.3 Generalization of NIsoGCN . . . . . . . . . . . . . . . . 91\n\n\n4.3.3 Neural Nonlinear Solver . . . . . . . . . . . . . . . . . . . . . . . 92\n\n\n4.3.3.1 Implicit Euler Method in Encoded Space . . . . . . . . . 92\n\n\n4.3.3.2 Barzilai–Borwein Method for Neural Nonlinear Solver . 93",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 8,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 239,
        "char_count": 798,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "d7f8eaca-2628-44d2-9e7f-be1d78a745a7",
      "content": "4.3.3.3 Formulation of Neural Nonlinear Solver . . . . . . . . . 94\n\n\n4.4 Numerical Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95\n\n\n4.4.1 Gradient Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . 95\n\n\n4.4.1.1 Taks Definition . . . . . . . . . . . . . . . . . . . . . . 95\n\n\n4.4.1.2 Model Architecture . . . . . . . . . . . . . . . . . . . . 96\n\n\n4.4.1.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . 97\n\n\n4.4.2 Advection-Diffusion Dataset . . . . . . . . . . . . . . . . . . . . . 98\n\n\n4.4.2.1 Task Definition . . . . . . . . . . . . . . . . . . . . . . 98\n\n\n4.4.2.2 Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . 98\n\n\n4.4.2.3 Model Architecture . . . . . . . . . . . . . . . . . . . . 99\n\n\n4.4.2.4 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . 104\n\n\n4.4.3 Incompressible Flow Dataset . . . . . . . . . . . . . . . . . . . . . 108\n\n\n4.4.3.1 Task Definition . . . . . . . . . . . . . . . . . . . . . . 108",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 9,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 346,
        "char_count": 989,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "d730d882-fafb-45bc-86d1-8e871548ebfa",
      "content": "4.4.3.2 Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . 109\n\n\n4.4.3.3 Machine Learning Models . . . . . . . . . . . . . . . . . 112\n\n\n4.4.3.4 Training Details . . . . . . . . . . . . . . . . . . . . . . 114\n\n\n4.4.3.5 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . 118\n\n\n4.4.3.6 Ablation Study Results . . . . . . . . . . . . . . . . . . 121\n\n\n4.4.3.7 Detailed Results . . . . . . . . . . . . . . . . . . . . . . 124\n\n\n4.4.3.8 Evaluation of Out-of-Distribution Generalization . . . . . 128\n\n\n4.5 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130\n\n\n**5** **Conclusion** **133**\n\n\n**Bibliography** **137**",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 9,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 210,
        "char_count": 664,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "3d9c37c8-7d18-4ff1-bce3-14648b8cda08",
      "content": "**Index** **145**",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 10,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 2,
        "char_count": 17,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "71d8fa69-6256-4652-91c3-4874ef9aab5b",
      "content": "# **List of Figures**\n\n2.1 (a) An example of a graph, (b) the same graph with permutated indices,\n\n\nand (c) corresponding adjacency and permutation matrices. . . . . . . . . . 11\n\n\n2.2 A path graph with five vertices. . . . . . . . . . . . . . . . . . . . . . . . . 12\n\n\n2.3 Schematic diagrams of (a) pointwise MLP, (b) MPNN, and (c) GCN. . . . . 14\n\n\n2.4 An example of a graph and its corresponding adjacency matrix _**A**_, degree\n\nmatrix _**D**_ [˜], renormalized adjacency matrix _**A**_ [ˆ], and resulting output _**h**_ out _,_ 1 .\n\n\ntheir can be seen that the GCN model considers information on neighboring\n\n\nvertices through a weighted sum determined from the graph structure. . . . 16\n\n\n2.5 Examples of (a) a domain Ω and (b) a mesh representing the corresponding\n\n\ndiscretized domain. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\n\n\n2.6 An example of a 1D _u_ field spatially discretized using FDM. . . . . . . . . 29",
      "chunk_metadata": {
        "section_title": "**List of Figures**",
        "section_level": 1,
        "page": 11,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 207,
        "char_count": 944,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "cfff2db2-dd1c-4178-b776-17dc72602f69",
      "content": "2.7 An example of 2D _u_ field spatially discretized using FDM and its corre\n\nsponding edge connectivity. . . . . . . . . . . . . . . . . . . . . . . . . . 31\n\n\n2.8 An example of a 1D _u_ field spatially discretized using FEM. . . . . . . . . 34\n\n\n2.9 An example of 2D spatially discretized unstructured grid for FEM (black)\n\n\nand its corresponding edge connectivity (blue). The connectivity of the\n\n\ngraph is not necessarily the same as the edges of the mesh. . . . . . . . . . 35\n\n\n3.1 Schematic diagrams of (a) rank-1 tensor field _**H**_ [(1)] with the number of\n\n\nfeatures equaling 2 and (b) the simplest case of _**G**_ _ij_ ;;: = _δ_ _il_ _δ_ _jk_ _A_ _ij_ _**I**_ ( _**x**_ _k_ _−_\n\n\n_**x**_ _l_ ) = _A_ _ij_ ( _**x**_ _j_ _−_ _**x**_ _i_ ). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42\n\n\nix",
      "chunk_metadata": {
        "section_title": "**List of Figures**",
        "section_level": 1,
        "page": 11,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 190,
        "char_count": 815,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "c4bc856c-c950-4ebe-a0fa-0a70c47d7329",
      "content": "3.2 The IsoGCN models used for (a) the scalar field to the gradient field, (b) the\n\n\nscalar field to the Hessian field, (c) the gradient field to the Laplacian field,\n\n\n(d) the gradient field to the Hessian field of the gradient operator dataset.\n\n\nGray boxes are trainable components. In each trainable cell, we put the\n\n\nnumber of units in each layer along with the activation functions used. _∗⃝_\n\n\ndenotes the multiplication in the feature direction. . . . . . . . . . . . . . 63\n\n\n3.3 (Top) the gradient field and (bottom) the error vector between the prediction\n\n\nand the ground truth of a test data sample. The error vectors are exaggerated\n\n\nby a factor of 2 for clear visualization. . . . . . . . . . . . . . . . . . . . . 65\n\n\n3.4 The process of generating the dataset. A smaller clscale parameter gener\n\nates smaller meshes. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70\n\n\n3.5 The IsoGCN model used for the anisotropic nonlinear heat equation",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 12,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 201,
        "char_count": 968,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "dcc566dc-3876-4c10-add0-49a1b2eb1d6d",
      "content": "dataset. Gray boxes are trainable components. In each trainable cell, we\n\n\nput the number of units in each layer along with the activation functions\n\n\nused. Below the unit numbers, the activation function used for each layer\n\n\nis also shown. _∗⃝_ denotes the multiplication in the feature direction, _⊙_\n\n\ndenotes the contraction, and _⊕_ denotes the addition in the feature direction. 72\n\n\n3.6 (Top) the temperature field of the ground truth and inference results and\n\n\n(bottom) the error between the prediction and the ground truth of a test\n\n\ndata sample. The error is exaggerated by a factor of 2 for clear visualization. 75\n\n\n3.7 Comparison between (left) samples in the training dataset, (center) ground\n\n\ntruth computed through FEA, and (right) IsoGCN inference result. For\n\n\nboth the ground truth and inference result, _|V|_ = 1 _,_ 011 _,_ 301. One can see\n\n\nthat IsoGCN can predict the temperature field for a mesh, which is much",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 12,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 153,
        "char_count": 939,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "45712b41-97a3-45b1-96d8-29dce63971ef",
      "content": "larger than these in the training dataset. . . . . . . . . . . . . . . . . . . . 76",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 12,
        "chunk_index": 2,
        "chunk_type": "text",
        "token_count": 27,
        "char_count": 83,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "5151a9f2-4ef9-48c6-902f-4127d1fbf451",
      "content": "4.1 Overview of the proposed method. On decoding input features, we apply\n\n\nboundary encoders to boundary conditions. Thereafter, we apply a nonlin\n\near solver consisting of an E( _n_ )-equivariant graph neural network in the\n\n\nencoded space. Here, we apply encoded boundary conditions for each it\n\neration of the nonlinear solver. After the solver stops, we apply the pseu\n\ndoinverse decoder to satisfy Dirichlet boundary conditions. . . . . . . . . . 83\n\n\n4.2 Architecture used for (a) original IsoGCN and (b) NIsoGCN training. In\n\n\neach trainable cell, we put the number of units in each layer along with the\n\n\nactivation functions used. . . . . . . . . . . . . . . . . . . . . . . . . . . 96\n\n\n4.3 Gradient field (top) and the magnitude of error between the predicted gradi\n\nent and the ground truth (bottom) of a test data sample, sliced on the center\n\n\nof the mesh. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 13,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 198,
        "char_count": 940,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "39ef9849-3c1b-48e1-b784-4c9e6c1b6493",
      "content": "4.4 The concept of the neural nonlinear solver for time series data with au\n\ntoregressive architecture. The solver’s output is fed to the same solver to\n\n\nobtain the state at the next time step (bold red arrow). Please note that this\n\n\narchitecture can be applied to arbitrary time series lengths. . . . . . . . . . 101\n\n\n4.5 The overview of the PENN architecture for the advection-diffusion dataset.\n\n\nGray boxes with continuous (dotted) lines are trainable (untrainable) com\n\nponents. Arrows with dotted lines correspond to the loop. In each trainable\n\n\ncell, we put the number of units in each layer along with the activation\n\n\nfunctions used. The bold red arrow corresponds to the one in Figure 4.4. . 102\n\n\n4.6 The overview of the PENN architecture for the advection-diffusion dataset.\n\n\nGray boxes with continuous (dotted) lines are trainable (untrainable) com\n\nponents. In each trainable cell, we put the number of units in each layer",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 13,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 157,
        "char_count": 941,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "cd2f668d-6c32-4e00-bf62-7bff642647e4",
      "content": "along with the activation functions used. . . . . . . . . . . . . . . . . . . 103\n\n\n4.7 Visual comparison on a test sample between (left) ground truth obtained\n\n\nfrom OpenFOAM computation with fine spatial-temporal resolution and\n\n(right) prediction by PENN. Here, _c_ = 0 _._ 9, _D_ = 0 _._ 0, and _T_ [ˆ] = 0 _._ 4. . . . . . 105",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 13,
        "chunk_index": 2,
        "chunk_type": "text",
        "token_count": 73,
        "char_count": 331,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "ad326672-36c7-4a31-b140-a2c704d214f3",
      "content": "4.8 Visual comparison on a test sample between (left) ground truth obtained\n\n\nfrom OpenFOAM computation with fine spatial-temporal resolution and\n\n(right) prediction by PENN. Here, _c_ = 0 _._ 0, _D_ = 0 _._ 4, and _T_ [ˆ] = 0 _._ 3. . . . . . 106\n\n\n4.9 Visual comparison on a test sample between (left) ground truth obtained\n\n\nfrom OpenFOAM computation with fine spatial-temporal resolution and\n\n(right) prediction by PENN. Here, _c_ = 0 _._ 6, _D_ = 0 _._ 3, and _T_ [ˆ] = 0 _._ 8. . . . . . 107\n\n\n4.10 Three template shapes used to generate the dataset. _a_ 1, _b_ 1, _b_ 2, _c_ 1, and _c_ 2 are\n\n\nthe design parameters. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110\n\n\n4.11 Boundary conditions of _**u**_ used to generate the dataset. The continuous\n\n\nlines and dotted lines correspond to Dirichlet and Neumann boundaries. . . 111\n\n\n4.12 Boundary conditions of _p_ used to generate the dataset. The continuous lines",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 14,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 187,
        "char_count": 932,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "790a3374-dfc5-4f1f-a8e3-1453d2e32f09",
      "content": "and dotted lines correspond to Dirichlet and Neumann boundaries. . . . . . 111\n\n\n4.13 The overview of the PENN architecture for the incompressible flow dataset.\n\n\nGray boxes with continuous (dotted) lines are trainable (untrainable) com\n\nponents. Arrows with dotted lines correspond to the loop. In each trainable\n\n\ncell, we put the number of units in each layer along with the activation\n\n\nfunctions used. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115\n\n\n4.14 The neural nonlinear solver for velocity. Gray boxes with continuous (dot\n\nted) lines are trainable (untrainable) components. Arrows with dotted lines\n\n\ncorrespond to the loop. In each trainable cell, we put the number of units\n\n\nin each layer along with the activation functions used. . . . . . . . . . . . 116\n\n\n4.15 The neural nonlinear solver for pressure. Gray boxes with continuous (dot\n\nted) lines are trainable (untrainable) components. In each trainable cell, we",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 14,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 178,
        "char_count": 953,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "9629a685-fbb9-4e6e-b364-a565c434e950",
      "content": "put the number of units in each layer along with the activation functions\n\n\nused. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 14,
        "chunk_index": 2,
        "chunk_type": "text",
        "token_count": 52,
        "char_count": 159,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "72e680b9-ae46-436f-8450-f0deb7154460",
      "content": "4.16 Comparison of the velocity field (top two rows) and the pressure field (bot\n\ntom two rows) without (first and third rows) and with (second and fourth\n\n\nrows) random rotation and translation. PENN prediction is consistent under\n\n\nrotation and translation due to the E( _n_ )-equivariance nature of the model,\n\n\nwhile MP-PDE’s predictive performance degrades under transformations. . 119\n\n\n4.17 Comparison of computation time and total MSE loss ( _**u**_ and _p_ ) on the\n\n\ntest dataset (with and without transformation) between OpenFOAM, MP\n\nPDE, and PENN. The error bar represents the standard error of the mean.\n\n\nAll computation was done using one core of Intel Xeon CPU E5-2695\n\n\nv2@2.40GHz. Data used to plot this figure are shown in Tables 4.6, 4.7,\n\n\nand 4.8. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120\n\n\n4.18 Visual comparison of the ablation study of (i) ground truth, (ii) the model",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 15,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 175,
        "char_count": 929,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "ff17c07f-2bb3-40ef-98c1-115e0e56a944",
      "content": "without the neural nonlinear solver (Model (C)), (iii) the model without\n\n\npseudoinverse decoder with Dirichlet layer after decoding (Model (G)), and\n\n\n(iv) PENN. It can be observed that PENN improves the prediction smooth\n\nness, especially for the velocity field. . . . . . . . . . . . . . . . . . . . . 123\n\n\n4.19 The relationship between the relative MSE of the velocity _**u**_ and inlet velocity.128\n\n\n4.20 The relationship between the relative MSE of the pressure _p_ and inlet velocity.129\n\n\n4.21 The visualization of velocity fields with inlet velocities _u_ inlet of 2.0 and 0.5. 129\n\n\n4.22 The visualization of velocity fields for a larger sample. . . . . . . . . . . . 130\n\n\n4.23 The visualization of pressure fields for a larger sample. . . . . . . . . . . . 131",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 15,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 148,
        "char_count": 774,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "fc97e2bc-6cf4-4886-a8a4-31caa1852dca",
      "content": "# **List of Tables**\n\n3.1 Correspondence between the differential operators and the expressions us\ning the IsoAM _**G**_ [˜] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55\n\n\n3.2 Summary of the hyperparameter setting for both the TFN and SE(3)\n\nTransformer. For the parameters not in the table, we used the default setting\n\n\n[in the implementation of https://github.com/FabianFuchsML/](https://github.com/FabianFuchsML/se3-transformer-public)\n\n\n[se3-transformer-public.](https://github.com/FabianFuchsML/se3-transformer-public) . . . . . . . . . . . . . . . . . . . . . . 64\n\n\n3.3 Summary of the test losses (mean squared error _±_ the standard error of the\n\n\nmean in the original scale) of the differential operator dataset: 0 _→_ 1 (the\n\n\nscalar field to the gradient field), 0 _→_ 2 (the scalar field to the Hessian\n\n\nfield), 1 _→_ 0 (the gradient field to the Laplacian field), and 1 _→_ 2 (the",
      "chunk_metadata": {
        "section_title": "**List of Tables**",
        "section_level": 1,
        "page": 17,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 166,
        "char_count": 917,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "efd8b7ef-d6a3-49c7-b094-f46578ad3516",
      "content": "gradient field to the Hessian field). Here, if “ _**x**_ ” is “Yes”, _**x**_ is also in the\n\n\ninput feature. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66\n\n\n3.4 Summary of the prediction time on the test dataset. 0 _→_ 1 corresponds to\n\n\nthe scalar field to the gradient field, and 0 _→_ 2 corresponds to the scalar\n\n\nfield to the Hessian field. Each computation was run on the same GPU\n\n\n(NVIDIA Tesla V100 with 32 GiB memory). OOM denotes the out-of\n\nmemory of the GPU. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67\n\n\n3.5 Summary of the hyperparameter setting for both the TFN and SE(3)\n\nTransformer. For the parameters not written in the table, we used\n\n\n[the default setting in the implementation of https://github.com/](https://github.com/FabianFuchsML/se3-transformer-public)\n\n\n[FabianFuchsML/se3-transformer-public. . . . . . . . . . . .](https://github.com/FabianFuchsML/se3-transformer-public) 74\n\n\nxv",
      "chunk_metadata": {
        "section_title": "**List of Tables**",
        "section_level": 1,
        "page": 17,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 187,
        "char_count": 948,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "0d07428c-8ba2-42d9-8e76-8cf1f50bfbb7",
      "content": "3.6 Summary of the test losses (mean squared error _±_ the standard error of the\n\n\nmean in the original scale) of the anisotropic nonlinear heat dataset. Here,\n\n\nif “ _**x**_ ” is “Yes”, _**x**_ is also in the input feature. OOM denotes the out-of\n\nmemory on the applied GPU (32 GiB). . . . . . . . . . . . . . . . . . . . . 77\n\n\n3.7 Comparison of computation time. To generate the test data, we sampled\n\n\nCAD data from the test dataset and then generated the mesh for the graph\n\n\nto expand while retaining the element volume at almost the same size. The\n\n\ninitial temperature field and the material properties are set randomly using\n\n\nthe same methodology as the dataset sample generation. For a fair com\n\nparison, each computation was run on the same CPU (Intel Xeon E5-2695\n\n\nv2@2.40GHz) using one core, and we excluded file I/O time from the mea\n\nsured time. OOM denotes the out-of-memory (500 GiB). . . . . . . . . . . 78",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 18,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 178,
        "char_count": 926,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "06dbd226-09cb-4794-98fa-0d53eb7b30b3",
      "content": "4.1 MSE loss ( _±_ the standard error of the mean) on test dataset of gradient\n\n\nprediction. ˆ _g_ Neumann is the loss computed only on the boundary where the\n\n\nNeuman condition is set. . . . . . . . . . . . . . . . . . . . . . . . . . . . 97\n\n\n4.2 MSE loss ( _±_ the standard error of the mean) on test dataset of the\n\n\nadvection-diffusion dataset. . . . . . . . . . . . . . . . . . . . . . . . . . 104\n\n\n4.3 MSE loss ( _±_ the standard error of the mean) on test dataset of incompress\n\nible flow. If ”Trans.” is ”Yes,” it means evaluation is done on randomly\n\n\nrotated and transformed test dataset. ˆ _·_ Dirichlet is the loss computed only on\n\n\nthe boundary where the Dirichlet condition is set for each _**u**_ and _p_ . MP\n\nPDE’s results are based on the time window size equaling 40 as it showed\n\n\nthe best performance in the tested MP-PDEs. For complete results, see\n\n\nTable 4.5. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 18,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 227,
        "char_count": 960,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "d6118e06-9100-45ef-9889-ba49ecd7cb05",
      "content": "4.4 Ablation study on the incompressible flow dataset. The value represents\n\n\nMSE loss ( _±_ standard error of the mean) on the test dataset. ”Divergent”\n\n\nmeans the implicit solver does not converge and the loss gets extreme value\n\n\n( _∼_ 10 [14] ). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 18,
        "chunk_index": 2,
        "chunk_type": "text",
        "token_count": 79,
        "char_count": 324,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "2c85036b-b63e-4bbe-9b9a-329d0e38321f",
      "content": "4.5 MSE loss ( _±_ the standard error of the mean) on test dataset of incompress\n\nible flow. If ”Trans.” is ”Yes”, it means evaluation on randomly rotated and\n\n\ntransformed test dataset. _n_ denotes the number of hidden features, _r_ de\n\nnotes the number of iterations in the neural nonlinear solver used in PENN\n\n\nmodels, and TW denotes the time window size used in MP-PDE models. . 125\n\n\n4.6 MSE loss ( _±_ the standard error of the mean) of PENN models on test\n\n\ndataset of incompressible flow. . . . . . . . . . . . . . . . . . . . . . . . . 126\n\n\n4.7 MSE loss ( _±_ the standard error of the mean) of MP-PDE models on test\n\n\ndataset of incompressible flow. . . . . . . . . . . . . . . . . . . . . . . . . 126\n\n\n4.8 MSE loss ( _±_ the standard error of the mean) of OpenFOAM computations\n\n\non test dataset of incompressible flow. . . . . . . . . . . . . . . . . . . . . 127\n\n\n4.9 MSE loss ( _±_ the standard error of the mean) on the dataset with larger",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 19,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 215,
        "char_count": 957,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "0273ae4d-defa-4018-9eae-2ae274eb2836",
      "content": "samples. ˆ _g_ Neumann is the loss computed only on the boundary where the\n\n\nNeuman condition is set. . . . . . . . . . . . . . . . . . . . . . . . . . . . 130",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 19,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 46,
        "char_count": 159,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "de59718e-86ed-45ec-838c-b5f08ffb5386",
      "content": "# **Nomenclature**\n\n**General**\n\n\nR Set of real numbers\n\n\nZ Set of integers\n\n\nZ [+] Set of positive integers\n\n\nZ [0+] Set of non-negative integers\n\n\n_M_ _ij_ Element ( _i, j_ ) of matrix _**M**_\n\n\n_v_ _i_ Element _i_ of vector _**v**_\n\n\n**Chapter 2**\n\n\n_G_ Graph\n\n\n_V_ Vertex set\n\n\n_|V|_ Number of vertices\n\n\n_N_ _v_ Set of neighboring vertices of vertex _v_ (Equation 2.12)\n\n\n_**A**_ Adjacency matrix (Equation 2.14)\n\n\n_J_ Index set\n\n\n_π_ : _J →_ _J_ Permutation\n\n\n_**P**_ Permutation matrix (Equation 2.15)\n\n\n_**L**_ Graph Laplacian matrix (Equation 2.17)\n\n\nxix",
      "chunk_metadata": {
        "section_title": "**Nomenclature**",
        "section_level": 1,
        "page": 21,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 89,
        "char_count": 563,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "1c43bba2-ea69-45fb-98c7-383362f1fb42",
      "content": "_**D**_ Degree matrix (Equation 2.18)\n\n\n_**W**_ Weight matrix\n\n\n_**b**_ Bias\n\n\n_σ_ Activation function\n\n\n_**H**_ _∈_ R _[V×][d]_ Vertex features\n\n\n_{_ _**h**_ _i_ _}_ _i∈V_ Set of vertex features\n\n\n_{_ _**e**_ _ij_ _}_ ( _i,j_ ) _∈E_ Set of edge features\n\n\n_**A**_ ˆ Renormalized adjacency matrix\n\n\n_**A**_ ˜ Adjacency matrix with added self-connections\n\n\n_**D**_ ˜ Degree matrix of _**A**_ [˜]\n\n\n_**I**_ _N_ Identity matrix of size _N_\n\n\n_G_ Group\n\n\nGL( _n_ ) General linear group\n\n\nSO( _n_ ) Special orthogonal group\n\n\n_S_ _n_ Symmetric group\n\n\nE( _n_ ) Euclidean group\n\n\n_α_ ( _g, x_ ) Group action of _g ∈_ _G_ to _x ∈_ _X_ (also denoted as _g · x_ )\n\n\n_δ_ _ij_ Kronecker delta (Equation 2.48)\n\n\n_D_ Nonlinear differential operator (Equation 2.52)\n\n\nΩ Analysis domain (Equation 2.52)\n\n\n_∂_ Ω Boundary of Ω\n\n\n_∂_ Ω Dirichlet Dirichlet boundary (Equation 2.54)\n\n\n_∂_ Ω Neumann Neumann boundary (Equation 2.55)\n\n\n_**r**_ Residual (Equations 2.60, 2.62)",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 22,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 152,
        "char_count": 953,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "6c19c535-b3a2-4cc6-b886-424ff7de1023",
      "content": "_**R**_ Residual vector (Equations 2.63)\n\n\n_**e**_ _x_ _,_ _**e**_ _y_ The unit vectors in the _X_ and _Y_ directions\n\n\n**Chapter 3**\n\n\n_**H**_ [(] _[p]_ [)] _∈_ R _[|V|×][d]_ [f] _[×][n]_ _[p]_ A rank- _p_ discrete tensor field (Equation 3.5)\n\n\n_**G**_ _∈_ R _[|V|×]_ [1] _[×][d]_ [1] IsoAM (Equation 3.8)\n\n\n_**G**_ ˜ _∈_ R _[|V|×]_ [1] _[×][d]_ [1] Differential IsoAM (Equation 3.46)\n\n\n_**G**_ _ij_ ;;: = _**g**_ _ij_ _∈_ R _[n]_ Slice in the spatial index of _**G**_\n\n\n**Chapter 4**\n\n\n_D_ NIsoGCN Nonlinear differential operator constructed using NIsoGCN",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 23,
        "chunk_index": 0,
        "chunk_type": "table",
        "token_count": 87,
        "char_count": 557,
        "contains_formulas": false,
        "contains_tables": true,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "c6618f50-6169-413e-861b-5bc09cfe7e55",
      "content": "# **Chapter 1** **Introduction**\n\n1.1 M OTIVATION\n\n\nPartial differential equations (PDEs) are of great interest to many scientists due to their\n\n\nwide-ranging applications in fields such as mathematics, physics, and engineering. Numer\n\nical analysis is commonly used to solve PDEs since most real-life PDE problems cannot be\n\n\nsolved analytically. For instance, predicting fluid behavior in complex shapes is of particu\n\nlar significance in various fields, including product design, disaster reduction, and weather\n\n\nforecasting. However, solving these problems using classical solvers is time-consuming\n\n\nand challenging. Machine learning has emerged as a promising alternative for addressing\n\n\nthese complex problems because, unlike classical solvers, it can leverage data that is similar\n\n\nto the state being predicted.\n\n\nThe main challenge in tackling complex phenomena like fluids mechanics using ma\n\nchine learning is to achieve good generalization performance, mainly owing to the follow",
      "chunk_metadata": {
        "section_title": "**Chapter 1** **Introduction**",
        "section_level": 1,
        "page": 25,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 138,
        "char_count": 994,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "cc37d45d-c7e3-4f45-bc81-c0cc9d04bffc",
      "content": "ing two reasons:\n\n\n   - **Variable degrees of freedom** : Classical numerical analysis methods discretize con\n\ntinuous fields of physical quantities (e.g., temperature or velocity fields) into vari\n\nables at finite points in a mesh. The number of points, which correspond to the\n\n\ndegrees of freedom of the analysis model, can vary depending on the shape of inter\n\n1",
      "chunk_metadata": {
        "section_title": "**Chapter 1** **Introduction**",
        "section_level": 1,
        "page": 25,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 58,
        "char_count": 366,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "dc9326d3-15b7-4a66-9fa1-b80187594b56",
      "content": "**2** 1. Introduction\n\n\nest, which requires some flexibility of the machine learning model to tolerate such\n\n\nuncertainty.\n\n\n   - **Large number of degrees of freedom** : A practical analysis often consists of a\n\n\nhuge number of degrees of freedom, typically over a million. This is considerably\n\n\nlarger than typical machine learning datasets, such as CIFAR-10 (Krizhevsky et al.,\n\n\n2009), which has 3072 features per sample. The number of possible states in such\n\n\na complex system can be large and a purely data-driven approach may not cover\n\n\nthem due to the curse of dimensionality.\n\n\nTo address these challenges, we must incorporate appropriate assumptions and knowl\n\nedge about the phenomena of interest into the machine learning model, which is known\n\n\nas inductive bias. Numerous studies have successfully introduced various inductive biases,\n\n\nsuch as local connectedness using graph neural networks (GNNs) (Chang & Cheng, 2020;",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 26,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 141,
        "char_count": 938,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "b74314fe-4190-4c48-aa63-5c3d4f78b461",
      "content": "Sanchez-Gonzalez et al., 2020; Pfaff et al., 2021; Brandstetter et al., 2022). These studies\n\n\nhave shown that GNNs are effective in constructing PDE solvers as they can handle inputs\n\n\nwith an arbitrary number of degrees of freedom.\n\n\nAlthough these methods have made significant progress in solving PDEs using ma\n\nchine learning, there is still room for improvement. Specifically, we can incorporate more\n\n\ninductive biases to reduce the numbers of degrees of freedom, for example, by considering\n\n\nonly half of the analysis domain if the phenomenon has bilateral symmetry, such as in the\n\n\naerodynamic analysis of a symmetric aircraft.\n\n\nFirst, the physical symmetry regarding isometric transformation, i.e., E( _n_ ) transforma\n\ntions, must be addressed when considering PDEs in Euclidean spaces because the nature\n\n\nof physical phenomena in such spaces does not change under these transformations. Thus,\n\n\nmodels that can accurately reflect physical symmetries, which are known as _equivariant_",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 26,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 148,
        "char_count": 999,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "9c1813b8-1f2c-49c8-9643-34f5eac345a2",
      "content": "functions regarding the transformation of interest, must be used.\n\n\nSecond, there is a need for an efficient and provable way to satisfy mixed boundary\n\n\nconditions, i.e., Dirichlet and Neumann. Rigorous fulfillment of Dirichlet boundary condi\n\ntions is indispensable because they are hard constraints, with different Dirichlet conditions\n\n\ncorresponding to different problems users would like to solve.",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 26,
        "chunk_index": 2,
        "chunk_type": "text",
        "token_count": 56,
        "char_count": 403,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "d0fc57a5-96a2-4ab0-8fb3-1fd78b275929",
      "content": "1.2. Objective and Scope **3**\n\n\nFinally, we need to enhance the treatment of global interactions to predict the state\n\n\nafter a long time, when interactions tend to be global. GNNs have excellent generalization\n\n\nproperties because of their locally connected nature, but they may miss global interactions\n\n\nowing to their localness.\n\n\n1.2 O BJECTIVE AND S COPE\n\n\nIn this dissertation, we focus on mesh-based time-dependent numerical analysis. Mesh\n\nbased methods are widely utilized in practical numerical analysis due to their ability to\n\n\nhandle complex shapes often encountered in industrial design. Time-dependent analysis\n\n\ntypically demands a significant amount of computational time, as compared with steady\n\nstate analysis, because of the small time step required to ensure stable computation of the\n\n\ntime evolution. Therefore, we aim to exploit the full potential of machine learning for\n\n\nconducting mesh-based time-dependent analyses.",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 27,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 136,
        "char_count": 947,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "dbacb75f-500a-452a-b55a-7e2527bdaa3d",
      "content": "The objective of this study is to develop a machine learning method that addresses\n\n\nthe challenges previously discussed. We aim to build a machine learning model with the\n\n\nfollowing key features:\n\n\n1. Flexibility to handle arbitrary meshes using GNNs\n\n\n2. E( _n_ )-equivariance to account for physical symmetries\n\n\n3. Computational efficiency to provide faster predictions than conventional numerical\n\n\nanalysis methods\n\n\n4. Capability to rigorously consider boundary conditions\n\n\n5. Stability for predicting over long time steps by considering global interactions\n\n\nIn a previous study (Horie et al., 2021), we introduced _IsoGCN_, a computationally ef\n\nficient GNN that features E( _n_ )- invariance and equivariance, hence, complying with the\n\n\nfirst three requirements outlined above. Specifically, this model simply modifies the defini\n\ntion of an adjacency matrix essential for describing a graph, to realize E( _n_ )-equivariance.",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 27,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 134,
        "char_count": 939,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "07fcea4c-d001-41c5-8291-23d90573354b",
      "content": "Because the proposed approach relies on graphs, it can handle complex shapes that are usu\n\nally modeled using mesh or point cloud data structures. Furthermore, a specific form of the",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 27,
        "chunk_index": 2,
        "chunk_type": "text",
        "token_count": 30,
        "char_count": 182,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "3d950502-573a-45a7-84b7-95e38745f790",
      "content": "**4** 1. Introduction\n\n\nIsoGCN layer can describe essential physical laws by acting as a spatial differential opera\n\ntor. Additionally, we demonstrated the computational efficiency of the proposed approach\n\n\nfor processing graphs with up to 1M vertices, which are common in real physical simula\n\ntions, as well as its capacity to produce faster prediction with the same level of accuracy\n\n\ncompared with conventional finite element methods. Consequently, an IsoGCN can suit\n\nably replace physical simulations thanks to its power to express physical laws and faster,\n\n\nscalable computation. The corresponding implementation code and dataset are available\n\n\nonline [1] .\n\n\nSimilarly, in a follow-up study (Horie & Mitsume (2022)), we proposed a _physics-_\n\n\n_embedded neural network (PENN)_, which is a machine learning framework featuring prop\n\nerties 3, 4, and 5 in the above list. We built PENN based on an IsoGCN to capture physical",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 28,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 141,
        "char_count": 934,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "3e6c69d7-c4bf-4668-8cee-eefb53f697f7",
      "content": "symmetry and ensure fast prediction. Furthermore, we developed a method for consid\n\nering mixed boundary conditions and modified the stacking of GNNs using a nonlinear\n\n\nsolver, enabling the natural inclusion of global interactions in GNNs through global pool\n\ning and improving their interpretability. By conducting numerical experiments, we demon\n\nstrated the improved predictive performance of the model when dealing with Neumann\n\n\nboundary conditions, as well as its ability to correctly fulfill Dirichlet boundary condi\n\ntions. This method displayed state-of-the-art performance compared with that of a clas\n\nsical, well-optimized numerical solver and a baseline machine learning model in terms of\n\n\nspeed-accuracy trade-off. The implementation code and dataset used for the experiments\n\n\nare also available online [2] .\n\n\n1.3 O UTLINE OF D ISSERTATION\n\n\nIn Chapter 2, we provide an overview of the background necessary for discussing our",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 28,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 136,
        "char_count": 943,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "2ce65f5c-144e-416f-bd28-904f6c19e408",
      "content": "research. We introduce essential machine learning models, particularly GNNs, that can\n\n\nlearn PDEs on complex shapes. In addition, we establish the concept of equivariance,\n\n\nwhich is the focus of this study, and review the basics of numerical analysis and its rela\n\ntionship to graphs.\n\n\n1 [https://github.com/yellowshippo/isogcn-iclr2021](https://github.com/yellowshippo/isogcn-iclr2021)\n2 [https://github.com/yellowshippo/penn-neurips2022](https://github.com/yellowshippo/penn-neurips2022)",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 28,
        "chunk_index": 2,
        "chunk_type": "text",
        "token_count": 48,
        "char_count": 492,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "161efaf3-9abb-406c-a1dc-442a47d5f6d3",
      "content": "1.3. Outline of Dissertation **5**\n\n\nChapter 3 presents IsoGCN, our essential computationally efficient GNN model with\n\n\nE(n)-equivariance. First, we elaborate on the motivation for equivariance. Then, we explain\n\n\nthe method and prove its equivariance and its relationship to numerical analysis. Finally,\n\n\nwe report numerical experiments that demonstrate the effectiveness of IsoGCNs.\n\n\nChapter 4 discusses PENNs, which can correctly satisfy boundary conditions and global\n\n\ninteractions based on IsoGCNs. Here, we describe the methods for handling Dirichlet\n\n\nand Neumann boundary conditions, and for including global interactions using a nonlin\n\near solver. Subsequently, we demonstrate the superiority of the proposed method through\n\n\nnumerical experiments.\n\n\nFinally, in Chapter 5, we summarize the main conclusions of this dissertation and men\n\ntion the limitations of the research, pointing out an interesting future direction to address\n\n\nthem.",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 29,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 130,
        "char_count": 953,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "898c39c6-a1fa-4fac-b5c2-140b8efe7994",
      "content": "**6** 1. Introduction",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 30,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 3,
        "char_count": 21,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "36eec10b-4e98-4929-933f-4be1a1cf2463",
      "content": "# **Chapter 2** **Background**\n\n2.1 M ACHINE L EARNING\n\n\nIn this section, we review the basics of machine learning and the essential models for\n\n\nlearning numerical analysis.\n\n\n2.1.1 F OUNDATIONS OF S UPERVISED L EARNING\n\n\nFor the purposes of this study, we focus on supervised learning, which is informally\n\n\ndefined as constructing a function that maps a given input to a given output as accurately\n\n\nas possible.\n\n\nSupervised learning involves minimizing the error between the given target and the\n\n\nprediction from the machine learning model. Let D _n_ := _{_ ( _**x**_ _i_ _∈X_ _,_ _**y**_ _i_ _∈Y_ ) _}_ _[n]_ _i_ =1 [denote]\n\n\na given training dataset, where _X_ and _Y_ are the input and output spaces, respectively. A\n\n\nmachine learning model with a set of learnable parameters _θ_ is defined as _**f**_ _θ_ : _X →Y_ .\n\n\nTraining is expressed as follows:\n\n\n_θ_ _[∗]_ := arg min _R_ _n_ ( _θ_ ) _,_ (2.1)\n_θ_\n\n\n7",
      "chunk_metadata": {
        "section_title": "**Chapter 2** **Background**",
        "section_level": 1,
        "page": 31,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 158,
        "char_count": 920,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "5283a20a-98d0-45f5-bcee-b40db5350b37",
      "content": "**8** 2. Background\n\n\nwhere the training loss _R_ _n_ ( _θ_ ) is:\n\n\n_R_ _n_ ( _θ_ ) := [1]\n\n_n_\n\n\n\n_n_\nY _L_ ( _**f**_ _θ_ ( _**x**_ _i_ ) _,_ _**y**_ _i_ ) _._ (2.2)\n\n\n_i_ =1\n\n\n\nand _L_ : _Y × Y →_ R is the _loss function_, which serves as an error scale.\n\n\nAlthough training is performed using a training dataset, the goal of supervised learn\n\ning is to obtain a model applicable to the statistical population behind the dataset, unlike\n\n\na typical optimization problem where it is sufficient to obtain an optimal model for the\n\n\ngiven data. However, because evaluating a model using a population in a practical set\n\nting is not feasible, we evaluate the trained model using a test dataset D [test] _n_ [test] [ :=] _[ {]_ [(] _**[x]**_ _i_ [test] _∈_\n\n_X_ _,_ _**y**_ _i_ [test] _∈Y_ ) _}_ _[n]_ _i_ =1 [test] [, Which is different from the training dataset but sampled from the same]\n\n\ndistribution. The population loss is approximated as follows:",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 32,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 173,
        "char_count": 951,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "fd762c50-3784-45ce-9a74-389477d7441f",
      "content": "_R_ ( _θ_ _[∗]_ ) := _E_ [ _L_ ( _**f**_ _θ_ _∗_ ( _**x**_ ) _,_ _**y**_ )] (2.3)\n\n\n\n_≈_ [1]\n\n_n_ [test]\n\n\n\n_n_ [test]\nY _L_ ( _**f**_ _θ_ _∗_ ( _**x**_ [test] _i_ ) _,_ _**y**_ _i_ [test] ) _,_ (2.4)\n\n\n_i_ =1\n\n\n\nwhere _E_ [ _·_ ] is the expected value.\n\n\nAs an example of supervised learning, let us consider linear regression. If _X_ = _Y_ = R,\n\n\nit becomes a one-dimensional linear regression, which is the simplest case. In this case, the\n\n\nmachine learning model is expressed as:\n\n\n_f_ _θ_ ( _x_ ) = _wx_ + _b_ (2.5)\n\n\n_θ_ = ( _w ∈_ R _, b ∈_ R) _._ (2.6)\n\n\nUsing the least squares method, we define the loss function as\n\n\n_L_ ( _y_ prediction _, y_ target ) = ( _y_ prediction _−_ _y_ target ) [2] _._ (2.7)",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 32,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 145,
        "char_count": 713,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "506cd251-43cf-4f5f-8bb4-cf2c494eccf1",
      "content": "2.1. Machine Learning **9**\n\n\nThis can be easily generalized to higher-dimensional cases by letting\n\n\n_X_ = R _[d]_ [in] (2.8)\n\n\n_Y_ = R _[d]_ [out] (2.9)\n\n\n_**f**_ _θ_ ( _x_ ) = _**W x**_ + _**b**_ (2.10)\n\n\n_θ_ = ( _**W**_ _∈_ R _[d]_ [out] _[×][d]_ [in] _,_ _**b**_ _∈_ R _[d]_ [out] ) _,_ (2.11)\n\n\nwhere _d_ in and _d_ out are the input and output dimensions, respectively. For more information\n\n\non machine learning, including supervised learning, see Bishop (2006).\n\n\n2.1.2 G RAPH N EURAL N ETWORKS (GNN S )\n\n\nThis section provides an overview of the foundations of graph neural networks (GNNs),\n\n\nwhich are a class of neural networks designed to handle graph-structured data. GNNs were\n\n\nfirst proposed by Baskin et al. (1997); Sperduti & Starita (1997), and subsequently im\n\nproved by (Gori et al., 2005; Scarselli et al., 2008). Because various data can be regarded\n\n\nas graphs, GNNs have a broad range of application domains such as 3D shape recogni",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 33,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 163,
        "char_count": 958,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "03a37f44-7438-4573-b32a-9f4bde7e2e55",
      "content": "tion (Fey et al., 2018; Monti et al., 2017), structural chemistry (Gilmer et al., 2017; Klicpera\n\n\net al., 2020), and social network analysis (Fan et al., 2019).\n\n\n2.1.2.1 G RAPH\n\n\nA finite _graph G_ = ( _V, E_ ) is defined as a tuple of a finite set of vertices (nodes) _V_ and\n\n\nedges _E ⊂V × V_ . In general, note that the edges are _directed_, i.e., that ( _u, v_ ) _∈E_ does not\n\n\nimply ( _v, u_ ) _∈E_ . However, in this dissertation we assume that all graphs are _undirected_\n\n\n(i.e., ( _u, v_ ) _∈E_ implies ( _v, u_ ) _∈E_ for all _u, v ∈V_ ) because of Newton’s third law of\n\n\nmotion, which states that every action has an equal and opposite to reaction. The set of\n\n\nneighboring vertices of _v_ is defined as:\n\n\n_N_ _v_ := _{u ∈V|_ ( _v, u_ ) _∈E} ._ (2.12)",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 33,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 153,
        "char_count": 768,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "61c30539-4ace-4023-ac0d-7681de8b93a1",
      "content": "**10** 2. Background\n\n\nThe square matrix describing the edge connectivity of a graph is called the _adjacency_\n\n\n_matrix_ and defined as:\n\n\n_**A**_ _∈_ R _[|V|×|V|]_ (2.13)\n\n\n\n(2.14)\n0 otherwise _,_\n\n\n\n_A_ _ij_ =\n\n\n\n\n\n\n\n\n 1 if edge ( _v_ _i_ _, v_ _j_ ) _∈E_\n\n 0 otherwise _,_\n\n\n\nwhere _|V|_ denotes the number of vertices in the index set _J_ = _{_ 1 _,_ 2 _, . . ., |V|}_ . Although\n\n\nthe definition of an adjacency matrix depends on the indexing of the vertices, adjacency\n\n\nmatrices of the same graph but with different indexing can be shown to be isomorphic\n\n\nusing the _permutation π_ : _J →_ _J_ to describe the changes in indices. Using the permutation\n\n\nmatrix, _**P**_ _∈_ R _[|V|×|V|]_ defined as:\n\n\n\n(2.15)\n0 otherwise _,_\n\n\n\n_P_ _ij_ =\n\n\n\n\n\n\n\n\n1 if _π_ ( _i_ ) = _j_\n\n\n\n 0 otherwise _,_\n\n\n\none can show that:\n\n\n_**A**_ _[′]_ = _**P AP**_ _[⊤]_ _,_ (2.16)",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 34,
        "chunk_index": 0,
        "chunk_type": "table",
        "token_count": 162,
        "char_count": 875,
        "contains_formulas": false,
        "contains_tables": true,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "f78a487f-58af-4bfc-9dd0-0c7f6288b3ec",
      "content": "where _A_ _[′]_ _ij_ [=] _[ A]_ _[π]_ [(] _[i]_ [)] _[ π]_ [(] _[j]_ [)] [is the adjacency matrix with permutated indices. Figure 2.1 presents]\n\n\nan example of a graph and its permutated representation. Because the discussion regarding\n\n\npermutations of graph vertex indices is well defined, in the subsequent discussions we\n\n\nrepresent vertices using an index, i.e., _v_ _i_ _8→_ _i_ .\n\n\nThe _graph Laplacian matrix_ _**L**_ can be defined as:\n\n\n_**L**_ = _**D**_ _−_ _**A**_ _,_ (2.17)\n\n\nwhere _**D**_ is the _degree matrix_ of the graph, which is defined by:\n\n\n\n(2.18)\n0 otherwise _._\n\n\n\n_D_ _ij_ :=\n\n\n\n\n\n\n\n\n\n\nQ\n\n\n\n_k_ _[A]_ _[ik]_ if _i_ = _j_",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 34,
        "chunk_index": 1,
        "chunk_type": "formula",
        "token_count": 110,
        "char_count": 650,
        "contains_formulas": true,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "1743c1fd-94e7-4ff9-885d-484385c3e533",
      "content": "主要ノード：1, 2, 3, 4  \nクラスタ：1-2-3-4  \n強い接続：1と2、2と3、3と4、4と1、1と3\n\n主要ノード：ノード2（中心）、ノード3、ノード5  \nクラスタ：{2, 3, 5}  \n強い接続：2-3、2-5、3-5（辺4）",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 35,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 11,
        "char_count": 124,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "392a2f18-b80c-44a9-8b67-8d3cb169c0cf",
      "content": "###### 4 5\n\n2.1. Machine Learning **11**",
      "chunk_metadata": {
        "section_title": "4 5",
        "section_level": 6,
        "page": 35,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 7,
        "char_count": 40,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "b9fcbbef-3945-410a-818d-0de352fceec3",
      "content": "###### 2 1",
      "chunk_metadata": {
        "section_title": "2 1",
        "section_level": 6,
        "page": 35,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 3,
        "char_count": 10,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "8cc7102b-f6ea-4622-9202-e60495ffb333",
      "content": "###### 6\n\n(c)\n\n\n_**A**_ =\n\n\n\n0\n\nB\nB\nB\nB\nB\nB\n@\n\n\n\n1 1 0 0 0 0\n\n1 0 1 1 0 0\n\n0 1 0 1 0 0\n\n0 1 1 1 0 0\n\n0 0 0 0 0 1\n\n0 0 0 0 1 0\n\n\n\n_**P**_ =\n\n\n\n1\n\nC\nC\nC\nC\nC\nC\nA\n\n\n\n1\n\nC\nC\nC\nC\nC\nC\nA\n\n\n\n1\n\nC\nC\nC\nC\nC\nC\nA\n\n\n\n0\n\nB\nB\nB\nB\nB\nB\n@\n\n\n\n0 0 0 0 0 1\n\n0 1 1 1 0 0\n\n0 1 0 1 0 0\n\n0 1 1 0 1 0\n\n0 0 0 1 1 0\n\n1 0 0 0 0 0\n\n\n\n0\n\nB\nB\nB\nB\nB\nB\n@\n\n\n\n0 0 0 0 1 0\n\n0 0 0 1 0 0\n\n0 0 1 0 0 0\n\n0 1 0 0 0 0\n\n1 0 0 0 0 0\n\n0 0 0 0 0 1\n\n\n\n_**A**_ _[0]_ =\n\n\n\nFigure 2.1: (a) An example of a graph, (b) the same graph with permutated indices, and (c)\n\n\ncorresponding adjacency and permutation matrices.\n\n\nFor the path graph with five vertices shown in Figure 2.2, the adjacency matrix is expressed\n\n\nas follows:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0 1 0 0 0\n\n\n\n1 0 1 0 0\n\n\n\n_**A**_ =\n\n\nThus, the graph Laplacian matrix is:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0 1 0 1 0\n\n\n\n0 0 1 0 1\n\n\n\n0 0 0 1 0\n\n\n\n(2.19)\n\n\n(2.20)\n\n\n\n_**L**_ =\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 _−_ 1 0 0 0\n\n\n_−_ 1 2 _−_ 1 0 0\n\n\n0 _−_ 1 2 _−_ 1 0\n\n\n\n0 0 _−_ 1 2 _−_ 1\n\n\n\n0 0 0 _−_ 1 1\n\n\n\n",
      "chunk_metadata": {
        "section_title": "6",
        "section_level": 6,
        "page": 35,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 307,
        "char_count": 993,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "43cfba71-33d8-438f-98c6-ab2b09e9ece1",
      "content": "\n\n\n\n\n\n\n\n",
      "chunk_metadata": {
        "section_title": "6",
        "section_level": 6,
        "page": 35,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 9,
        "char_count": 17,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "e06dc5fb-17ed-4f27-a0f4-7d7a756e94fb",
      "content": "**12** 2. Background\n\n\nAs will be discussed in Section 2.2.4, the graph Laplacian matrix is closely related to the\n\n\nLaplacian operator.",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 36,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 21,
        "char_count": 136,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "4ba3ba85-5a0f-4640-90c2-2a91102a086f",
      "content": "## 1 2 3 4 5\n\nFigure 2.2: A path graph with five vertices.\n\n\nA function defined at a set of vertices _**f**_ vertex : _V →_ R _[N]_ is called a vertex signal or\n\n\n_vertex feature_ . Similarly, a function defined at a set of edges _**f**_ edge : _E →_ R _[N]_ is called an\n\n\nedge signal or _edge feature_ . _Graph signal processing_ is a research domain that deals with\n\n\nnode and edge signals on graphs, that is, graph signals. For more details regarding graph\n\n\nsignal processing, refer to, e.g., Ortega et al. (2018); Dong et al. (2020).\n\n\n2.1.2.2 P OINTWISE MLP\n\n\nOne of the most basic neural network models is the _multilayer perceptron_ (MLP). An\n\n\n_L_ -layer MLP _L_ : R _[d]_ [in] _→_ R _[d]_ [out] is defined as a stacking of affine transformations and\n\n\ncomponent-wise functions, called activation functions, as follows:",
      "chunk_metadata": {
        "section_title": "1 2 3 4 5",
        "section_level": 2,
        "page": 36,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 147,
        "char_count": 829,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "84b582a4-f925-4982-a14f-62cf51454902",
      "content": "MLP( _**x**_ ) := _σ_ [(] _[L]_ [)] _◦_ Affine [(] _[L]_ [)] _◦_ _σ_ [(] _[L][−]_ [1)] _◦_ Affine [(] _[L][−]_ [1)] _◦· · · ◦_ _σ_ [(1)] _◦_ Affine [(1)] ( _**x**_ ) (2.21)\n\n\nAffine [(] _[l]_ [)] ( _**h**_ _l_ ) := _**W**_ [(] _[l]_ [)] _**h**_ [(] _[l]_ [)] + _**b**_ [(] _[l]_ [)] _∀l ∈{_ 1 _,_ 2 _, . . ., L}_ (2.22)\n\n\n_**W**_ [(] _[l]_ [)] _∈_ R _[d]_ [(] _[l]_ [+1)] _[×][d]_ [(] _[l]_ [)] _∀l ∈{_ 1 _,_ 2 _, . . ., L}_ (2.23)\n\n\n_**b**_ [(] _[l]_ [)] _∈_ R _[d]_ [(] _[l]_ [+1)] _∀l ∈{_ 1 _,_ 2 _, . . ., L}_ (2.24)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n [=]\n\n\n\n...\n\n\n_σ_ [(] _[l]_ [)] ( _v_ _i_ )\n\n...\n\n\n\n\n\n\n _∀l ∈{_ 1 _,_ 2 _, . . ., L},_ (2.25)\n\n\n\n\n\n_σ_ [(] _[l]_ [)]\n\n\n\n\n\n\n\n\n\n\n\n\n...\n\n\n_v_ _i_\n\n...\n\n\n\nwhere _d_ [(1)] = _d_ in and _d_ [(] _[L]_ [+1)] = _d_ out, and _**W**_ [(] _[l]_ [)], _**b**_ [(] _[l]_ [)], and _σ_ [(] _[l]_ [)] are the _weight matrix_, _bias_, and\n\n\n_activation function_, respectively. An MLP is known as a universal approximator (Hornik,",
      "chunk_metadata": {
        "section_title": "1 2 3 4 5",
        "section_level": 2,
        "page": 36,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 211,
        "char_count": 969,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "286de9d0-4553-4e6d-91d5-f6397c370547",
      "content": "2.1. Machine Learning **13**\n\n\n1991; Cybenko, 1992; Nakkiran et al., 2021) that can approximate any continuous function\n\n\nif the number of hidden features _d_ [(] _[l]_ [)] ( _l ̸_ = 1 _, L_ ) is increased.\n\n\nHowever, an MLP cannot handle an input with an arbitrary length by itself because the\n\n\ndimensions of the input are fixed. Instead, one can use a _pointwise MLP_ to handle inputs\n\n\nwith arbitrary lengths. An _L_ -layer pointwise MLP, PointwiseMLP _L_ : R _[|V|×][d]_ [in] _→_ R _[|V|×][d]_ [out],\n\n\nis constructed by separately applying an _L_ -layer MLP, MLP _L_ : R _[d]_ [in] _→_ R _[d]_ [out], to each point,\n\n\nas follows:\n\n\n\n\n\n\n\n (2.26)\n\n\n\n\n\n\nPointwiseMLP _L_ ( _**H**_ in ) :=\n\n\n_**H**_ in =\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMLP _L_ ( _**h**_ in _,_ 1 )\n\n\nMLP _L_ ( _**h**_ in _,_ 2 )\n\n...\n\n\nMLP _L_ ( _**h**_ in _,N_ )\n\n\n\n_**h**_ in _,_ 1\n\n\n_**h**_ in _,_ 2\n\n...\n\n\n_**h**_ in _,|V|_\n\n\n\n\n\n\n\n _∈_ R _[|V|×][d]_ [in] _,_ (2.27)\n\n",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 37,
        "chunk_index": 0,
        "chunk_type": "table",
        "token_count": 190,
        "char_count": 962,
        "contains_formulas": false,
        "contains_tables": true,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "35a060d4-20bf-4add-9fe7-11a471e55616",
      "content": "where every MLP represents an identical function. Figure 2.3 (a) presents the architecture\n\n\nof a pointwise MLP. It can be seen that an MLP “pointwise” is applied, resulting in the\n\n\ncapability to incorporate an arbitrary input length.\n\n\nAlternatively, a pointwise MLP is expressed as:\n\n\nPointwiseMLP _L_ ( _**H**_ in ) := _σ_ [(] _[L]_ [)] _◦_ PointwiseAffine [(] _[L]_ [)] _◦· · · ◦_ _σ_ [(1)] _◦_ PointwiseAffine [(1)] ( _**H**_ in ) _,_\n\n\n(2.28)\n\n\nwhere\n\n\nPointwiseAffine [(] _[l]_ [)] ( _**H**_ [(] _[l]_ [)] ) := _**H**_ [(] _[l]_ [)] _**W**_ [(] _[l]_ [)] + **1** _|V|_ _**b**_ [(] _[l]_ [)] _∀l ∈{_ 1 _,_ 2 _, . . ., L}_ (2.29)\n\n_**W**_ [(] _[l]_ [)] _∈_ R _[d]_ [(] _[l]_ [+1)] _[×][d]_ [(] _[l]_ [)] _∀l ∈{_ 1 _,_ 2 _, . . ., L}_ (2.30)\n\n\n_**b**_ [(] _[l]_ [)] _∈_ R [1] _[×][d]_ [(] _[l]_ [)] _∀l ∈{_ 1 _,_ 2 _, . . ., L}_ (2.31)",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 37,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 160,
        "char_count": 840,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "507e2e13-f501-4749-903f-6a5fec243c58",
      "content": "**14** 2. Background\n\n\n\n\n\n\n\n _∈_ R _[V×]_ [1] _._ (2.32)\n\n\n\n\n\n\n**1** _|V|_ =\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n\n\n1\n\n...\n\n\n1\n\n\n\nBecause the trainable parameters in the model do not depend on the number of vertices,\n\n\npointwise MLPs can handle arbitrary input lengths, i.e., arbitrary graphs, but they ignore\n\n\nthe edges, that is, the connections between vertices. Nevertheless, pointwise MLPs are\n\n\nwidely used as part of GNNs because of their simplicity.\n\n\n\n(a)\n\n\n(b)\n\n\n\n画像は機械学習のニューラルネットワークの構造を示しています。入力層（h_in,k）から隠れ層（h_out,i）へと信号が伝わっており、各層間で複数のノード（橙色）が相互に接続しています。各ノードは入力値を加算し、活性化関数を通じて出力値（h）を生成しています。\n\n_**h**_ in _,i_\n\n\n\n_**h**_ in _,j_\n\n\n\n_**h**_ out _,j_\n\n\n\n画像は機械学習の更新関数を示すブロック図で、入力値$h_{in,j}$、$h_{in,k}$を含む多層ネットワークの構造を示しています。各層間の値が更新関数$h_{out,j}, h_{out,i}$を通じて変化し、最終的に出力$h$が生成される様子が説明されています。\n\n\n\nMessage function:\n_**f**_ message ( _**h**_ in _,i_ _,_ _**h**_ in _,j_ _,_ _**e**_ in _,ij_ )\n\n\n\n\n\n(c)\n\n\n\n\n\n\n\n\n\n_**h**_ out _,k_\n\n\n_**h**_ out _,k_",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 38,
        "chunk_index": 0,
        "chunk_type": "formula",
        "token_count": 118,
        "char_count": 944,
        "contains_formulas": true,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "578ebe6d-837b-4110-adeb-e629f27307db",
      "content": "画像はニューラルネットワークの層間通信を示した図です。左側の立方体は入力層の活性化関数（\\(\\hat{A}_{ij}h_{in,j}\\)）を表し、各ノード間の入力信号（\\(h_{in,i}\\), \\(h_{in,k}\\)）が伝播します。中央の橙色図形は活性化関節を示し、入力信号が加算され、出力信号\\(h_{out,i}\\)に変換されます。右側の青色立方体は出力層で、\\(n_{out,j}\\)を生成し、最終的な出力\\(h\\)を出力します。各層間の信号は橙色の矢印で伝播し、層間の相互作用を示しています。\n\nUpdate function\n\n\nFigure 2.3: Schematic diagrams of (a) pointwise MLP, (b) MPNN, and (c) GCN.\n\n\n2.1.2.3 M ESSAGE P ASSING N EURAL N ETWORKS (MPNN S )\n\n\nThe term GNN is an umbrella denomination for any neural network that can handle\n\n\ngraph-structured data. Although there are many GNN variants, most are unified under\n\n\nthe concept of _message passing neural networks_ (MPNNs) (Gilmer et al., 2017), which\n\n\ncomprise two main parts: the message function _**f**_ messaga and update function _**f**_ update . One",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 38,
        "chunk_index": 1,
        "chunk_type": "formula",
        "token_count": 84,
        "char_count": 764,
        "contains_formulas": true,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "50591ba7-24fe-4fbf-813f-afa3dc3d25bf",
      "content": "2.1. Machine Learning **15**\n\n\nMPNN operation is defined as:\n\n\nMPNN( _{_ _**h**_ in _,i_ _}_ _i∈V_ _, {_ _**e**_ in _,ij_ _}_ ( _i,j_ ) _∈E_ ) := _**f**_ update ( _**h**_ in _,i_ _,_ _**m**_ _i_ ) (2.33)\n\n_**m**_ _i_ := Y _**f**_ message ( _**h**_ in _,i_ _,_ _**h**_ in _,j_ _,_ _**e**_ in _,ij_ ) _,_ (2.34)\n\n_j∈N_ _i_\n\n\nwhere _{_ _**h**_ in _,i_ _}_ _i∈V_ and _{_ _**e**_ in _,ij_ _}_ ( _i,j_ ) _∈E_ are the vertex and edge features, respectively. Note that\n\n\n_**f**_ message and _**f**_ update are machine learning models usually based on neural networks.\n\n\nFigure 2.3 (b) shows a schematic of an MPNN. The message function models the effect\n\n\nfrom neighboring vertices. A typical example of an update function is a pointwise MLP that\n\n\npredicts the state of vertices using vertex features and aggregated messages. The trainable\n\n\nparameters of the message and update functions independent of the number of vertices",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 39,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 158,
        "char_count": 919,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "54e27d0b-1e98-45a5-9c0e-e7b6af75694b",
      "content": "or edges, which implies that an MPNN can handle a graph with arbitrary dimensions. A\n\n\nsingle MPNN layer considers neighboring vertices as one hop away, hence, the information\n\n\nof vertices _k_ -hops away can be considered by stacking _k_ MPNN layers.\n\n\n2.1.2.4 G RAPH C ONVOLUTIONAL N ETWORK (GCN)\n\n\nGenerally, deep neural networks are used for message passing, which can incur tremen\n\ndous computational cost. In contrast, the _Graph Convolutional Network_ (GCN) developed\n\n\nby Kipf & Welling (2017) is a considerable simplification of an MPNN, that uses a linear\n\n\nmessage-passing scheme expressed as:\n\n\nGCN( _**H**_ in ) := PointwiseMLP _L_ =1 ( _**AH**_ [ˆ] in ) _,_ (2.35)\n\n\nwhere _**A**_ [ˆ] denotes a renormalized adjacency matrix with self-loops and defined as:\n\n\n_**A**_ ˆ := _**D**_ [˜] _[−]_ [1] _[/]_ [2] [ ˜] _**AD**_ [˜] _[−]_ [1] _[/]_ [2] _,_ (2.36)\n\n\nwhere _**A**_ [˜] is an adjacency matrix of the graph with added self-connections and _**D**_ [˜] is the",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 39,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 158,
        "char_count": 973,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "f1f570f2-5a1b-4b51-8a01-2d8115204c08",
      "content": "degree matrix of _**A**_ [˜] .",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 39,
        "chunk_index": 2,
        "chunk_type": "text",
        "token_count": 6,
        "char_count": 30,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "5d0a14bd-6b1f-4cbe-ab8f-553ece53d171",
      "content": "**16** 2. Background\n\n\nThe formulation of a GCN comprehends that of an MPNN, as follows:\n\n\n\n_**m**_ _i_ =\nY\n\n\n\nY _**f**_ message ( _**h**_ in _,_ _**h**_ in _,j_ ) = Y\n\n_j∈N_ _i_ _∪{i}_ _j∈N_ _i_\n\n\n\nY _A_ ˆ _ij_ _**h**_ in _,j_\n\n_j∈N_ _i_ _∪{i}_\n\n\n\nˆ\n= _**AH**_ in\ni j\n\n\n\n(2.37)\n_i_\n\n\n\n_**f**_ update ( _**m**_ _i_ ) = MLP _L_ =1 ( _**m**_ _i_ ) _,_ (2.38)\n\n\nNote that Equation 2.37 is derived based on the fact that _A_ [ˆ] _ij_ = 0 (if _j /∈N_ _i_ _∪{i}_ ).\n\n\nFrom Equation 2.37, one can consider that GCNs use linearized message passing, which\n\ncan accelerate their. Furthermore, if the graph is sparse, i.e., _|E| ≪|V|_ [2], implying that the\n\nnumber of actual edges _|E|_ is significantly smaller than the possible number of edges _|V|_ [2],\n\n\nefficient algorithms can be utilized for sparse matrix operations. Figure 2.3 (c) shows the\n\n\narchitecture of a GCN and Figure 2.4 shows an example of GCN operation. Owing to its",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 40,
        "chunk_index": 0,
        "chunk_type": "table",
        "token_count": 168,
        "char_count": 927,
        "contains_formulas": false,
        "contains_tables": true,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "30123f07-ca03-403b-abe3-08acb222b205",
      "content": "computational efficiency, a GCN is the basis for constructing our proposed IsoGCN, a fast\n\n\nmachine learning model to learn physics, as presented in Chapter 3\n\n\n2\n\n\n1\n\n図は四面体の立体図示で、底面は正方形で各辺の長さを4とし、頂点から底面への高さも4と示されています。各辺に粉ピンクの矢印が描かれており、底面周囲の方向を示しています。\n\n\n3\n\n\n\n_p_\n\n\n\n図は四面体の立体構造を示し、橙色の矢印は頂点方向に作用する力の方向を表しています。四面体は青緑色とオレンジ色で区別され、各面が三角形の形状を有しています。この図は力学的力の分析や運動学の問題を解くための基本的な図示として使用されます。\n\n図は2つの立方体を示しています。左側の立方体の各辺の長さが4、5と示されています。右側の立方体内に橙色の四角形が存在し、その周囲には橙色の線が延びています。立方体と橙色四角形の間には、橙色の矢印が指向しています。\n\n_p_\n\n\n\n5 4\n\n\n\n1\n\nC\nC\nC\nC\nA\n\n\n\n1\n\nC\nC\nC\nC\nA\n\n\n\n20 4\n\n\n\n_p_\n\n_p_\n\n\n\n5 6\n\n\n5 6\n\n\n\n_p_\n\n_p_\n\n\n\n0\n\nB\nB\nB\nB\n@\n\n\n\n20 4 5 4 5 0 0\n\n4 _p_ 5 12 12 6 _p_ 5 6 _p_\n\n\n\n4 5 12 12 6 5 6 5\n\n4 _p_ 5 12 12 6 _p_ 5 6 _p_ 5\n\n\n\n_p_\n\n_p_\n\n\n\n5 12 12 6 5 6 5\n\n0 6 _p_ 5 6 _p_ 5 15 15\n\n\n\n0 6 5 6 5 15 15\n\n0 6 _p_ 5 6 _p_ 5 15 15\n\n\n\n5 12 12 6\n\n\n5 12 12 6\n\n\n\n_p_\n\n_p_\n\n\n\n5 6\n\n\n5 6\n\n\n\n0\n\nB\nB\nB\nB\n@\n\n\n\n3 0 0 0 0\n\n0 5 0 0 0\n\n0 0 5 0 0\n\n0 0 0 4 0\n\n0 0 0 0 4\n\n\n\n_**A**_ ˆ = [1]\n\n60\n\n\n\n_**A**_ =\n\n\n\n0\n\nB\nB\nB\nB\n@\n\n\n\n0 1 1 0 0\n\n1 0 1 1 1",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 40,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 201,
        "char_count": 994,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "ab2f6030-853b-4077-89f9-c620aa78b085",
      "content": "1 1 0 1 1\n\n0 1 1 0 1\n\n0 1 1 1 0\n\n\n\n_**D**_ ˜ =\n\n\n\n1\n\nC\nC\nC\nC\nA\n\n\n\n_p_\n\n_p_\n\n\n\n5 15 15\n\n\n\n\n\n\n\n_**h**_ out _,_ 1 = [GCN( _**H**_ in )] 1 = MLP\n\n\n\n1\n\n_p_\n\n✓ 60 [[20] _**[h]**_ [in] _[,]_ [1] [ + 4]\n\n\n\n5 _**h**_ in _,_ 2 + 4\n\n\n\n_p_ 5 _**h**_ in _,_ 3 ]\n\n\n\nFigure 2.4: An example of a graph and its corresponding adjacency matrix _**A**_, degree\n\nmatrix _**D**_ [˜], renormalized adjacency matrix _**A**_ [ˆ], and resulting output _**h**_ out _,_ 1 . their can be seen\n\n\nthat the GCN model considers information on neighboring vertices through a weighted sum\n\n\ndetermined from the graph structure.\n\n\nIf we consider a graph with no edges, then _A_ _ij_ = 0 ( _∀i, j ∈{_ 1 _,_ 2 _, . . ., |V|}_ ) and _**D**_ [˜] =\n\n\n_**I**_ _|V|_, where _**I**_ _|V|_ denotes an identity matrix of size _|V|_ . In such case, the GCN layer becomes\n\n\na pointwise MLP PointwiseMLP _L_ =1 ( _**I**_ _|V|_ _**H**_ in ) = PointwiseMLP _L_ =1 ( _**H**_ in ). Therefore, the",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 40,
        "chunk_index": 2,
        "chunk_type": "table",
        "token_count": 191,
        "char_count": 943,
        "contains_formulas": false,
        "contains_tables": true,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "6cc13f73-8446-4f2b-be8f-c4e529c297da",
      "content": "2.1. Machine Learning **17**\n\n\nGCN model can be considered a generalization of a pointwise MLP for a model-capturing\n\n\ngraph structure.\n\n\n2.1.3 E QUIVARIANCE\n\n\nEquivariance is an essential concept for characterizing the predictable behavior of a\n\n\nfunction under certain transformations, such as rotation or translation. Because this is\n\n\nclosely related to group theory, we first introduce groups and related concepts.\n\n\n2.1.3.1 G ROUP T HEORY\n\n\nA _group_ is a set _G_ with a binary operation (usually called “multiplication”), _·_ : _G×G →_\n\n\n_G_, that satisfies the following requirements:\n\n\n(Associativity) _∀a, b, c ∈_ _G,_ ( _a · b_ ) _· c_ = _a ·_ ( _b · c_ ) (2.39)\n\n\n(Identity element) _∃e_ s _._ t _. ∀a ∈_ _G,_ _e · a_ = _a · e_ = _a_ (2.40)\n\n\n(Inverse element) _∀a ∈_ _G, ∃b ∈_ _G_ s _._ t _._ _a · b_ = _b · a_ = _e._ (2.41)\n\n\nFor a group _G_ and a set _X_, a (left) _group action_ is a function _·_ : _G × X →_ _X_, which\n\n\nsatisfies the following conditions:",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 41,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 178,
        "char_count": 973,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "4851890d-e892-42ad-9a54-e13bf777fc05",
      "content": "(Identity) _∀x ∈_ _X,_ _e · x_ = _x_ (2.42)\n\n\n(Compatibility) _∀a, b ∈_ _G, ∀x ∈_ _X,_ _a ·_ ( _b · x_ ) = ( _a · b_ ) _· x,_ (2.43)\n\n\nwhere _e_ is the identity element of the group. We denote _α_ ( _a, x_ ) := _a · x_ when we must\n\n\nclarify that the operation is a group action.\n\n\nGroups appear in various fields, such as physics, engineering, and computer science;\n\n\nnext, we provide a few examples of groups and their actions. A first example is the _general_\n\n\n_linear group_ GL( _n_ ), the set of all _n_ -dimensional invertible matrices. One can confirm\n\n\nGL( _n_ ) is a group because the multiplication of matrices is associative, the identity matrix\n\n\nis in GL( _n_ ), and by definition an inverse matrix always exists for a given element in GL( _n_ ).\n\n\nAnother example is the _orthogonal group_ O( _n_ ), the set of _n_ -dimensional orthogonal\n\n\nmatrices representing rotation and reflection, which also satisfies the group requirements.",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 41,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 173,
        "char_count": 947,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "f59e90ba-5e94-4739-968a-40fe4655101a",
      "content": "**18** 2. Background\n\n\nIn addition, one can consider the multiplication between an element of O( _n_ ) and an _n_ \n\ndimensional vector _**x**_ _∈_ R _[n]_ . Such multiplication satisfies the definition of group action as\n\n\nwell. Furthermore, a rank-2 tensor _**T**_ _∈_ R _[n][×][n]_ can be transformed into an orthogonal matrix\n\n\n_**U**_ _∈_ O( _n_ ) By computing _α_ ( _**U**_ _,_ _**T**_ ) = _**UT U**_ _[⊤]_, Which is also a group action. Therefore,\n\n\nthe concrete form of a group action might differ depending on the set on which the group\n\n\nacts.\n\n\nTwo more examples of groups are the _symmetric group S_ _n_, a group of permutations,\n\n\nand the _Euclidean group_ E( _n_ ), which is a group of isometric transformations, namely,\n\n\ntranslation, rotation, and reflection. In particular, E( _n_ ) plays an essential role in developing\n\n\nneural PDE solvers because most physical phenomena occur in the Euclidean space, with\n\n\nits essence remaining the same under E( _n_ ) transformations.",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 42,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 162,
        "char_count": 989,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "5f5ac196-fc42-4727-9241-60e2504ec32a",
      "content": "2.1.3.2 E QUIVARIANT M ODEL\n\n\nA function _f_ : _X →_ _Y_ is said to be _G_  - _equivariant_ when:\n\n\n_∀g ∈_ _G, ∀x ∈_ _X, f_ ( _g · x_ ) = _g · f_ ( _x_ ) _,_ (2.44)\n\n\nassuming that the group _G_ acts on both _X_ and _Y_ . The concept of equivariance is also\n\n\nexplained by the following commutative diagram:\n\n\n_X_ _g·_ _X_\n\n\n_f_ _f_\n\n\n\nIn particular, when:\n\n\n\n_Y_ _g·_ _Y_\n\n\n_∀g ∈_ _G, f_ ( _g · x_ ) = _f_ ( _x_ ) _,_ (2.45)\n\n\n\n_f_ is said to be _G_ - _invariant_, and the corresponding commutative diagram is as follows:\n\n\n_X_ _g·_ _X_\n\n\n_f_\n\n\n_Y_",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 42,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 113,
        "char_count": 549,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "52de3af6-55a2-473f-b812-37f6de285373",
      "content": "2.1. Machine Learning **19**\n\n\nThis invariance is a special case of equivariance because _g · x_ = _x_ qualifies as a group\n\n\naction (trivial group action).\n\n\nMost numerical analysis schemes and models for physical simulations have equivari\n\nance. The principle of material objectivity (Ignatieff, 1996), which is similar to equivari\n\nance, is considered essential for constitutive laws. For instance, the tensor product between\n\n\ntwo rank-1 tensors _**f**_ prod : R _[n]_ _×_ R _[n]_ _∋_ ( _**v**_ _,_ _**u**_ ) _8→_ _**v**_ _⊗_ _**u**_ _∈_ R _[n][×][n]_ is O( _n_ )-equivariant because,\n\n\nfor any orthogonal matrix _**U**_ :\n\n\n[ _**f**_ prod ( _α_ ( _**U**_ _,_ _**v**_ ) _, α_ ( _**U**_ _,_ _**u**_ ))] _ij_ = [ _**Uv**_ _⊗_ _**Uu**_ ] _ij_\n\n\n= [ _**Uv**_ ] _i_ [ _**Uu**_ ] _j_\n\n\n= Y _U_ _ik_ _v_ _k_ _U_ _jl_ _u_ _l_\n\n\n_kl_\n\n= Y _U_ _ik_ _v_ _k_ _u_ _l_ _U_ _lj_ _[⊤]_\n\n\n_kl_\n\n\n= _**Uv**_ _⊗_ _**uU**_ _[⊤]_ [\u0004]\n\u0003 _ij_",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 43,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 163,
        "char_count": 923,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "ab3b297a-ae46-4d70-82a9-8dd39b3226ed",
      "content": "= [ _α_ ( _**U**_ _,_ _**f**_ prod ( _**v**_ _,_ _**u**_ ))] _ij_ _,_ (2.46)\n\n\nsatisfying the definition of equivariance (Equation 2.44). The squared norm operator _f_ norm :\n\nR _[n]_ _∋_ _**v**_ _8→∥_ _**v**_ _∥_ [2] _∈_ R is O( _n_ )-invariant because\n\n\n_f_ norm ( _α_ ( _**U**_ _,_ _**v**_ )) = _∥_ _**Uv**_ _∥_ [2]\n\n\n= ( _**Uv**_ ) _·_ ( _**Uv**_ )\n\n\n= _U_ _ik_ _v_ _k_ _U_ _il_ _v_ _l_\nY\n\n\n_ikl_\n\n= Y _U_ _li_ _[⊤]_ _[U]_ _[ik]_ _[v]_ _[k]_ _[v]_ _[l]_\n\n\n_ikl_\n\n\n= _δ_ _lk_ _v_ _k_ _v_ _l_\nY\n\n\n_kl_\n\n\n= _v_ _k_ _v_ _k_\nY\n\n\n_k_\n\n\n= _∥_ _**v**_ _∥_ [2]\n\n\n= _f_ norm ( _**v**_ ) _,_ (2.47)",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 43,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 119,
        "char_count": 591,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "3024202a-b130-4614-b4d2-5890bdb0704c",
      "content": "**20** 2. Background\n\n\nwhich satisfies the definition of invariance (Equation 2.45). Here, _δ_ _lk_ is the _Kronecker delta_,\n\n\ndefined as:\n\n\n\n(2.48)\n0 otherwise _._\n\n\n\n_δ_ _ij_ =\n\n\n\n\n\n\n\n\n1 if _i_ = _j_\n\n\n\n0\n\n\n\n\n\nIn addition to O( _n_ ), a symmetric group _S_ _n_ is also worth considering because it corre\n\nsponds to the permutation of the vertex indices. In numerical analysis, we choose arbitrary\n\n\nindexing for the nodes and elements in the meshes. Therefore, permutation equivariance is\n\n\nan essential indicator for a preferable numerical analysis scheme [1] . We can demonstrate a\n\n\nGCN layer operation (Equation 2.35) is permutation equivariant for all permutation matri\n\nces as follows: _**P**_,\n\n\nGCN( _α_ ( _**P**_ _,_ _**H**_ )) = PointwiseMLP( _α_ ( _**P**_ _,_ _**AH**_ [ˆ] ))\n\n\n= PointwiseMLP( _**P**_ _**AP**_ [ˆ] _[⊤]_ _**P H**_ ))\n\n\n= PointwiseMLP( _**P**_ _**AH**_ [ˆ] ))\n\n\n= _**P**_ PointwiseMLP( _**AH**_ [ˆ] ))\n\n\n= _α_ ( _**P**_ _,_ GCN( _**H**_ )) _._ (2.49)",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 44,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 160,
        "char_count": 984,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "dcccfa3c-77fe-4e14-a80d-6069bf7f747d",
      "content": "We use the fact that any permutation matrix is orthogonal, i.e., _**P**_ _[⊤]_ = _**P**_ _[−]_ [1], and that all\n\n\npointwise MLP layers are trivially permutation equivariant.\n\n\nGroup equivariant convolutional neural networks (CNNs) were first proposed by Cohen\n\n\n& Welling (2016) for discrete groups. Subsequent studies have categorized such networks\n\n\nas continuous groups (Cohen et al., 2018), three-dimensional data (Weiler et al., 2018),\n\n\nand general manifolds (Cohen et al., 2019). These methods are based on CNNs; thus,\n\n\nthey cannot directly handle mesh or point cloud data structures. Specifically, 3D steerable\n\n\nCNNs (Weiler et al., 2018) which use voxels (regular grids) and are deemed relatively\n\n\neasy to handle, are inefficient because they represent both occupied and empty parts of an\n\n\nobject (Ahmed et al., 2018). In addition, a voxelized object tends to lose smoothness of",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 44,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 134,
        "char_count": 892,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "ae356047-7c76-4566-9599-5c07fb44793f",
      "content": "1 However, several schemes are not permutation equivariant, e.g., one iteration in the successive overrelaxation (SOR) method. Nevertheless, we can assume that the entire SOR process is nearly permutationequivariant if it converges to an accurate solution.",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 44,
        "chunk_index": 2,
        "chunk_type": "text",
        "token_count": 36,
        "char_count": 256,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "d88dbf0e-9d23-47fb-9f1b-12be2dbe7efe",
      "content": "2.2. Numerical Analysis **21**\n\n\nits shape, which can lead to a drastically different behavior in a physical simulation, as is\n\n\ntypically observed in heat analyses and computational fluid dynamics.\n\n\nThomas et al. (2018); Kondor (2018) discussed how to provide rotation equivariance\n\n\nto point clouds. Specifically, Thomas et al. (2018) proposed a tensor field network (TFN),\n\n\nwhich is a point-cloud-based rotation and translation equivariant neural network, whose\n\n\nlayer can be written as:\n\n\n_̸_\n\n\n\n_**H**_ ˜ [(] _[l]_ [)] _**H**_ [(] _[k]_ [)] _**H**_ [(] _[l]_ [)]\nout _,i_ [= TFN] _[l]_ [(] _[{]_ [ ˜] in _,i_ _[}]_ _[k][≥]_ [0] [) =] _[ w]_ _[ll]_ [ ˜] in _,i_ [+] Y\n\n_k≥_ 0 _̸_\n\n\n\nY _**W**_ _[lk]_ ( _**x**_ _j_ _−_ _**x**_ _i_ ) _**H**_ [˜] in [(] _[k]_ _,j_ [)] (2.50)\n\n_j_ = _̸_ _i_\n\n\n\n_̸_\n\n\n_**W**_ _[lk]_ ( _**x**_ ) =\n\n\n\n_̸_\n\n\n_k_ + _l_\nY _φ_ _[lk]_ _J_ [(] _[∥]_ _**[x]**_ _[∥]_ [)]\n\n_J_ = _|k−l|_",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 45,
        "chunk_index": 0,
        "chunk_type": "formula",
        "token_count": 158,
        "char_count": 913,
        "contains_formulas": true,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "31180f66-c06e-4f5f-9ca3-8203259a48c4",
      "content": "Y _**W**_ _[lk]_ ( _**x**_ _j_ _−_ _**x**_ _i_ ) _**H**_ [˜] in [(] _[k]_ _,j_ [)] (2.50)\n\n_j_ = _̸_ _i_\n\n\n_J_\nY _Y_ _Jm_ ( _**x**_ _/∥_ _**x**_ _∥_ ) _**Q**_ _[lk]_ _Jm_ _[,]_ (2.51)\n\n\n_m_ = _−J_\n\n\n\n_̸_\n\n\n_J_\nY\n\n\n\n_̸_\n\n\nwhere _**H**_ [˜] in [(] _[l]_ [)] _,i_ [(] _**H**_ [ ˜] out [(] _[l]_ [)] _,i_ [): is a type-] _[l]_ [ input (output) features at the] _[ i]_ [th vertex,] _[ φ]_ _J_ _[lk]_ [:][ R] _[≥]_ [0] _[ →]_ [R]\n\nis a trainable function, _Y_ _Jm_ is the _m_ th component of the _J_ th spherical harmonic, and _**Q**_ _[lk]_ _Jm_\n\nis the Clebsch-Cordan coefficient. The SE(3)-Transformer (Fuchs et al., 2020) is a TFN\n\n\nvariant with self-attention. Dym & Maron (2020) showed that both the TFN and SE(3)\n\nTransformer are universal in terms of translation, rotation, and permutation equivariance.\n\n\nE( _n_ )-equivariance is essential for solving physical PDEs because it describes rigid\n\nbody motion, i.e., translation, rotation, and reflection. Ling et al. (2016) and Wang et al.",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 45,
        "chunk_index": 1,
        "chunk_type": "formula",
        "token_count": 174,
        "char_count": 989,
        "contains_formulas": true,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "de5920b0-3d24-49ae-90eb-e51199272891",
      "content": "(2021) introduced equivariance into a simple neural network and a CNN to predict flow\n\n\nphenomena. Both studies showed that the predictive and generalization performance im\n\nproved due to equivariance.\n\n\n2.2 N UMERICAL A NALYSIS\n\n\nIn this section, we review the foundations of PDEs to clarify the problems we aim to\n\n\nsolve and introduce related works in which machine learning models are used to solve\n\n\nPDEs.\n\n\n2.2.1 P ARTIAL D IFFERENTIAL E QUATIONS (PDE S ) WITH B OUNDARY C ONDITIONS\n\n\nThe general form of the spatiotemporal PDEs for a field, _**u**_ : (0 _, T_ ) _×_ Ω _→_ R _[d]_, of\n\n\na _d_ -dimensional physical quantity defined in an _n_ -dimensional domain, Ω _⊂_ R _[n]_, can be",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 45,
        "chunk_index": 2,
        "chunk_type": "text",
        "token_count": 119,
        "char_count": 690,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "796ce22d-31e3-479a-b911-310550d4967d",
      "content": "**22** 2. Background\n\n\nexpressed as follows:\n\n\n_∂_ _**u**_\n\n_∂t_ [(] _[t,]_ _**[ x]**_ [) =] _[ D]_ [(] _**[u]**_ [)(] _[t,]_ _**[ x]**_ [)] ( _t,_ _**x**_ ) _∈_ (0 _, T_ ) _×_ Ω (2.52)\n\n\n_**u**_ ( _t_ = 0 _,_ _**x**_ ) = ˆ _**u**_ 0 ( _**x**_ ) _**x**_ _∈_ Ω (2.53)\n\n\n_**u**_ ( _t,_ _**x**_ ) = ˆ _**u**_ ( _t,_ _**x**_ ) ( _t,_ _**x**_ ) _∈_ (0 _, T_ ) _× ∂_ Ω Dirichlet (2.54)\n\n\nˆ\n_**f**_ ( _∇_ _**u**_ ( _t,_ _**x**_ ) _,_ _**n**_ ( _**x**_ )) = **0** ( _t,_ _**x**_ ) _∈_ (0 _, T_ ) _× ∂_ Ω Neumann _,_ (2.55)\n\n\nwhere _∂_ Ω Dirichlet and _∂_ Ω Neumann are mixed _Dirichlet_ and _Neumann_ boundary conditions,\n\n\nrespectively, such that _∂_ Ω Dirichlet _∩_ _∂_ Ω Neumann = _∅_ and _∂_ Ω Dirichlet _∪_ _∂_ Ω Neumann = _∂_ Ω, _∂_ Ω\n\n\nˆ\ndenotes the boundary of Ω, _·_ is a known function, _D_ is a known nonlinear differential oper\n\nator, which can be nonlinear and contains spatial differential operators, and _**n**_ ( _**x**_ ) denotes",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 46,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 186,
        "char_count": 938,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "17fa752a-792b-48db-8565-36def71d3491",
      "content": "the normal vector at _**x**_ _∈_ _∂_ Ω. Equation 2.54 is called the Dirichlet boundary condition,\n\n\nwhere the value of _∂_ Ω Dirichlet is set as a constraint, whereas Equation 2.55 corresponds to\n\n\nthe Neumann boundary condition, where the value of the derivative _**u**_ in the direction of _**n**_\n\n\nis set to _∂_ Ω Neumann rather than _**u**_ . _**u**_ is the solution of the (initial) boundary value problem\n\n\nwhen it satisfies Equations 2.52 – 2.55.\n\n\nEquation 2.52 may represent various types of PDEs. For instance, in the case of the\n\n\nheat equation:\n\n\n_D_ heat ( _u_ ) = _c∇· ∇u,_ (2.56)\n\n\nwhere _u_ is the temperature field ( _d_ = 1) and _c_ is the diffusion coefficient. For an incom\n\npressible Navier–Stokes equations:\n\n\n_D_ NS ( _**u**_ ) = _−_ ( _**u**_ _· ∇_ ) _**u**_ + [1] (2.57)\n\nRe _[∇· ∇]_ _**[u]**_ _[ −∇][p,]_\n\n\nwhere _∇·_ _**u**_ = 0 expresses the incompressible condition, _**u**_ denotes the flow velocity field,",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 46,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 161,
        "char_count": 937,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "783f2363-43ce-4c4b-8ab6-dd18db0dd06f",
      "content": "_p_ is the pressure field, and Re denotesthe Reynolds number.",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 46,
        "chunk_index": 2,
        "chunk_type": "text",
        "token_count": 10,
        "char_count": 61,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "6a7700dd-2d4a-464f-a292-08424e0109f5",
      "content": "2.2. Numerical Analysis **23**\n\n\n2.2.2 D ISCRETIZATION\n\n\nPDEs must be defined in a continuous space for the differentials to be meaningful.\n\n\nDiscretization can be applied to both space and time to enable computers to easily solve\n\n\nthe PDE.\n\n\nIn the numerical analysis of complex-shaped domains, we commonly use _meshes_ (dis\n\ncretized shape data), which can be regarded as a graph, as shown in Figure 2.5. We denote\n\n\nthe position of the _i_ th vertex as _**x**_ _i_ and the value of a function _f_, _g_, _. . ._ at _**x**_ _i_ as _f_ _i_ _g_ _i_, _. . ._ .\n\n\nTherefore, _{f_ _i_ _}_ _i∈V_, _{g_ _i_ _}_ _i∈V_, and _. . ._ are the vertex features [2] . For concrete examples of\n\n\nspatial discretization, see Section 2.2.4.",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 47,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 128,
        "char_count": 724,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "63a23a08-a5ab-4737-a00b-494e434e78de",
      "content": "###### (a)",
      "chunk_metadata": {
        "section_title": "(a)",
        "section_level": 6,
        "page": 47,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 2,
        "char_count": 10,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "5c889fff-d994-44f8-9278-2de4f2d38b6d",
      "content": "###### (b)\n\n主要ノード：頂点A、B、C、D、E、F、G、H、I、J、K、L、M、N、O、P、Q、R、S、T、U、V、W、X、Y、Z。  \nクラスタ：A-B-C-D-E-F-G-H-I-J-K-L-M-N-O-P-Q-R-S-T-U-V-W-X-Y-Z。  \n強い接続：A-F、B-G、C-H、D-I、E-J、F-K、G-L、H-M、I-N、J-O、K-P、L-Q、M-R、N-S、O-T、P-U、Q-V、R-W、S-X、T-Y、U-Z。",
      "chunk_metadata": {
        "section_title": "(b)",
        "section_level": 6,
        "page": 47,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 5,
        "char_count": 224,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "3f80e945-63a1-40ce-b518-b42d39f312ec",
      "content": "###### Spatial discretization (meshing)",
      "chunk_metadata": {
        "section_title": "Spatial discretization (meshing)",
        "section_level": 6,
        "page": 47,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 4,
        "char_count": 39,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "c3ac1ad0-1c74-4de1-a012-2088120ffa1c",
      "content": "##### ⌦\n\nFigure 2.5: Examples of (a) a domain Ω and (b) a mesh representing the corresponding\n\n\ndiscretized domain.\n\n\nOne of the simplest methods to discretize time is the _explicit Euler method_ which is\n\n\nformulated as:\n\n\n_**u**_ ( _t_ + ∆ _t,_ _**x**_ _i_ ) _≈_ _**u**_ ( _t,_ _**x**_ _i_ ) + _D_ ( _**u**_ )( _t,_ _**x**_ _i_ )∆ _t,_ (2.58)\n\n\nwhere _**u**_ ( _t,_ _**x**_ _i_ ) is updated via a small increment _D_ ( _**u**_ )( _t,_ _**x**_ _i_ )∆ _t_ . Another way to discretize\n\n\ntime is the _implicit Euler method_ formulated as:\n\n\n_**u**_ ( _t_ + ∆ _t,_ _**x**_ _i_ ) _≈_ _**u**_ ( _t,_ _**x**_ _i_ ) + _D_ ( _**u**_ )( _t_ + ∆ _t,_ _**x**_ _i_ )∆ _t,_ (2.59)\n\n\n2 Strictly speaking, the components of the PDE, e.g. _D_ and Ω, can be different before and after discretization. However, we use the same notation regardless of discretization to keep the notation simple.",
      "chunk_metadata": {
        "section_title": "⌦",
        "section_level": 5,
        "page": 47,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 161,
        "char_count": 875,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "76d7867e-e56d-44ff-afea-e303a489a5de",
      "content": "**24** 2. Background\n\n\nwhich solves Equation 2.59 rather than simply updating the variables to ensure that the\n\n\noriginal PDE is numerically satisfied. The equation can be viewed as a nonlinear optimiza\n\ntion problem by formulating it as:\n\n\n_**r**_ ( _**v**_ ) := _**v**_ _−_ _**u**_ ( _t, ·_ ) _−D_ ( _**v**_ )∆ _t_ (2.60)\n\n\nSolve _**v**_ _**r**_ ( _**v**_ )( _**x**_ _i_ ) = **0** _, ∀i ∈{_ 1 _, . . ., |V|},_ (2.61)\n\n\nwhere _**r**_ : (Ω _→_ R _[d]_ ) _→_ (Ω _→_ R _[d]_ ) is the operator of the residual vector of the discretized\n\n\nPDE. Since _**r**_ is a map from functions Ω _→_ R _[d]_ to functions Ω _→_ R _[d]_, _**r**_ ( _**v**_ ) : Ω _→_ R _[d]_ is\n\n\nalso a function. Therefore:\n\n\n_**r**_ ( _**v**_ )( _**x**_ _i_ ) = _**v**_ ( _**x**_ _i_ ) _−_ _**u**_ ( _t,_ _**x**_ _i_ ) _−D_ ( _**v**_ )( _**x**_ _i_ )∆ _t ∈_ R _[d]_ (2.62)\n\n\ncorresponds to the error in the current numerical solution _**v**_ at _**x**_ _i_ . If",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 48,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 179,
        "char_count": 927,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "d0558de9-0e44-42ca-b88d-5a3c1189eabf",
      "content": "_**r**_ ( _**v**_ )( _**x**_ _i_ ) = **0**, _**v**_ satisfies the discretized equation at _**x**_ _i_ . Here, by\n\nletting _**V**_ = ( _**v**_ ( _**x**_ 1 ) _[⊤]_ _,_ _**v**_ ( _**x**_ 2 ) _[⊤]_ _, . . .,_ _**v**_ ( _**x**_ _|V|_ ) _[⊤]_ ) _[⊤]_ _∈_ R _[d][|V|]_ and _**U**_ ( _t_ ) =\n\n( _**u**_ ( _t,_ _**x**_ 1 ) _[⊤]_ _,_ _**u**_ ( _t,_ _**x**_ 2 ) _[⊤]_ _, . . .,_ _**u**_ ( _t,_ _**x**_ _|V|_ ) _[⊤]_ ) _[⊤]_ _∈_ R _[d][|V|]_, Equation 2.60 and Equation 2.61\n\n\nbecome:\n\n\n_**R**_ ( _**V**_ ) := _**V**_ _−_ _**U**_ ( _t_ ) _−D_ ( _**V**_ )∆ _t ∈_ R _[d][|V|]_ (2.63)\n\n\nSolve _**V**_ _**R**_ ( _**V**_ ) = **0** _._ (2.64)\n\n\nThe solution to Equation 2.64 corresponds to _**U**_ ( _t_ + ∆ _t_ ) = ( _**u**_ ( _t_ + ∆ _t,_ _**x**_ 1 ) _[⊤]_ _,_ _**u**_ ( _t_ +\n\n∆ _t,_ _**x**_ 2 ) _[⊤]_ _, . . .,_ _**u**_ ( _t_ + ∆ _t,_ _**x**_ _|V|_ ) _[⊤]_ ) _[⊤]_ _∈_ R _[d][|V|]_ .\n\n\n2.2.3 N ONLINEAR S OLVER AND O PTIMIZATION\n\n\n2.2.3.1 B ASIC F ORMULA FOR I TERATIVE M ETHODS",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 48,
        "chunk_index": 1,
        "chunk_type": "table",
        "token_count": 201,
        "char_count": 964,
        "contains_formulas": false,
        "contains_tables": true,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "53602811-a67f-4b6a-9126-bbe95cc136aa",
      "content": "Because Equation 2.64 can be a nonlinear and high-dimensional problem, there is no\n\n\ngeneral formula for solving it. A common method to obtain an approximate solution is to",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 48,
        "chunk_index": 2,
        "chunk_type": "text",
        "token_count": 28,
        "char_count": 172,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "059c1e9b-06ff-454a-9b1e-7a0421d06d3c",
      "content": "2.2. Numerical Analysis **25**\n\n\napply an iterative method to a linearized system, such as:\n\n\n_**V**_ [[0]] = _**U**_ ( _t_ ) (2.65)\n\n\n_**V**_ [[] _[i]_ [+1]] = _**V**_ [[] _[i]_ []] + ∆ _**V**_ [[] _[i]_ []] _,_ (2.66)\n\n\nwhere ∆ _**V**_ [[] _[i]_ []] is an unknown update of the approximate solution. The first-order approxi\n\nmation can be applied to obtain the update, as follows:\n\n\n_**R**_ ( _**V**_ [[] _[i]_ []] + ∆ _**V**_ [[] _[i]_ []] ) _≈_ _**R**_ ( _**V**_ [[] _[i]_ []] ) + _∇_ _**V**_ _⊗_ _**R**_ ( _**V**_ [[] _[i]_ []] )∆ _**V**_ [[] _[i]_ []] = **0** _,_ (2.67)\n\n\nwhere _∇_ _**V**_ _⊗_ _**R**_ _∈_ R _[d][|V|×][d][|V|]_ denotes the Jacobian matrix of _**R**_ with respect to _**V**_ . Instead\n\n\nof using Equation 2.61, we can iteratively solve Equation 2.67.\n\n\nIf a function _φ_ : R _[d][|V|]_ _→_ R satisfying _∇_ _**V**_ _φ_ = _**R**_ exists, solving Equation 2.64 cor\n\nresponds to the optimization of _φ_ in an ( _d|V|_ )-dimensional space, where _|V|_ denotes the",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 49,
        "chunk_index": 0,
        "chunk_type": "table",
        "token_count": 172,
        "char_count": 982,
        "contains_formulas": false,
        "contains_tables": true,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "8f10a79d-49df-4f3b-a827-c0126f4b3dbc",
      "content": "number of vertices in the considered mesh. Therefore, the implicit Euler method is closely\n\n\nrelated to optimization in a high-dimensional space. From this viewpoint, the Jacobian ma\n\ntrix _∇_ _**V**_ _⊗_ _**R**_ corresponds to the Hessian matrix _∇_ _**V**_ _⊗∇_ _**V**_ _φ_ . However, it should be noted\n\n\nthat the Hessian matrix is always symmetric, which is not always the case for the Jacobian\n\n\nmatrix.\n\n\n2.2.3.2 N EWTON –R APHSON M ETHOD AND Q UASI -N EWTON M ETHOD\n\n\nThe _Newton–Raphson method_ solves Equation 2.67 as follows:\n\n\n∆ _**V**_ [[] _[i]_ []] = _−_ \u0003 _∇_ _**V**_ _⊗_ _**R**_ ( _**V**_ [[] _[i]_ []] ) \u0004 _−_ 1 _**R**_ ( _**V**_ [ _i_ ] ) _,_ (2.68)\n\n\nwhich requires solving a linear system with a large number of degrees of freedom, ( _d|V|_ ).\n\n\nSolving such a large system of linear equations occasionally requires considerable compu\n\ntational resources and time. To address this issue, _quasi-Newton methods_ approximate the",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 49,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 158,
        "char_count": 945,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "772ac92b-04f1-4379-9150-3e9906332320",
      "content": "inverse of the Jacobian matrix using a matrix _**H**_ [[] _[i]_ []] to obtain:\n\n\n∆ _**V**_ [[] _[i]_ []] _≈−_ _**H**_ [[] _[i]_ []] _**R**_ ( _**V**_ [[] _[i]_ []] ) _._ (2.69)",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 49,
        "chunk_index": 2,
        "chunk_type": "text",
        "token_count": 33,
        "char_count": 176,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "82c2ddb4-ce39-48cb-bbec-a3f4ded953bb",
      "content": "**26** 2. Background\n\n\nVarious methods can be used to initialize, compute, and update _**H**_ [[] _[i]_ []] . A key concern in\n\n\nquasi-Newton methods is their massive memory consumption because _**H**_ [[] _[i]_ []] could be dense\n\n\neven if _∇_ _**V**_ _⊗_ _**R**_ is sparse. Thus, a lot of effort has been dedicated to reducing the memory\n\n\ndemand of this method, as in Liu & Nocedal (1989).\n\n\n2.2.3.3 G RADIENT D ESCENT M ETHOD\n\n\nThe _gradient descent_ method implements yet another approximation, as follows:\n\n\n∆ _**V**_ [[] _[i]_ []] _≈−α_ [[] _[i]_ []] _**R**_ ( _**V**_ [[] _[i]_ []] ) _,_ (2.70)\n\n\nwhere _α_ [[] _[i]_ []] _∈_ R is a scalar that controls the update magnitude. The approximation has no\n\n\nerror when all eigenvalues _λ_ _i_ ( _i ∈_ 1 _, . . ., d|V|_ ) are the same, i.e., _λ_ _i_ = _λ_, because:\n\n\n\n_∇_ _**V**_ _⊗_ _**R**_ ( _**V**_ [[] _[i]_ []] ) = _**Q**_\n\n\n\n\n\n\n\n\n\n\n\n_λ_\n\n...\n\n\n\n_λ_\n\n\n\n\n\n (2.71)\n _**[Q]**_ _[−]_ [1]",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 50,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 172,
        "char_count": 948,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "20cc1685-bcc0-4b8a-a53f-262719b48df4",
      "content": "= _**Q**_ _λ_ _**I**_ _d|V|_ _**Q**_ _[−]_ [1] (2.72)\n\n\n= _λ_ _**I**_ _d|V|_ _,_ (2.73)\n\n\nwhere _**Q**_ where is the eigenvectors matrix. Thus, by letting _α_ [[] _[i]_ []] = 1 _/λ_, we can show that\n\n\nEquations 2.68 and 2.70 are the same. In contrast, if the eigenvalues are not identical and\n\n\nbroadly distributed, the gradient descent approximation introduces some error. This fact is\n\n\nreasonable because such a situation corresponds to a linear system with a large condition\n\n\nnumber for the matrix _∇_ _**V**_ _**R**_ ( _**V**_ [[] _[i]_ []] ) and, hence, constitutes a challenging problem.\n\n\nThe update using gradient descent is expressed as:\n\n\n_**V**_ [[] _[i]_ [+1]] = _**V**_ [[] _[i]_ []] _−_ _α_ [[] _[i]_ []] _**R**_ ( _**V**_ [[] _[i]_ []] ) _._ (2.74)\n\n\nThis method is termed gradient descent because _**R**_ ( _**V**_ [[] _[i]_ []] ) corresponds to the “gradient”, and\n\n\nthe equation is updated to reduce the error.",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 50,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 155,
        "char_count": 931,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "deb2574b-cf33-4636-8808-36fcc0754d1f",
      "content": "2.2. Numerical Analysis **27**\n\n\n_α_ [[] _[i]_ []] is typically determined using line search. However, owing to the high computational\n\n\ncost of this search, _α_ [[] _[i]_ []] can be fixed to a small value _α_, which corresponds to the explicit\n\n\nEuler method with a time step size _α_ ∆ _t_ because\n\n\n_**V**_ [[] _[i]_ [+1]] = _**V**_ [[] _[i]_ []] _−_ _α_ _**R**_ ( _**V**_ [[] _[i]_ []] ) (2.75)\n\n\n= _**V**_ [[] _[i]_ []] _−_ _α_ \u0003 _**V**_ [[] _[i]_ []] _−_ _**U**_ ( _t_ ) _−D_ ( _**V**_ [[] _[i]_ []] )∆ _t_ \u0004 (2.76)\n\n\n= (1 _−_ _α_ ) _**V**_ [[] _[i]_ []] + _α_ _**U**_ ( _t_ ) + _D_ ( _**V**_ [[] _[i]_ []] ) _α_ ∆ _t._ (2.77)\n\n\nIf we explicitly write the first few steps:\n\n\n_**V**_ [[0]] = _**U**_ ( _t_ ) (2.78)\n\n\n_**V**_ [[1]] = (1 _−_ _α_ ) _**V**_ [[0]] + _α_ _**U**_ ( _t_ ) + _D_ ( _**V**_ [[0]] ) _α_ ∆ _t_ (2.79)\n\n\n= _**U**_ ( _t_ ) + _D_ ( _**U**_ ( _t_ )) _α_ ∆ _t,_ (2.80)\n\n\nobtaining the same update scheme as that in Equation 2.58. For more information regard",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 51,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 196,
        "char_count": 979,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "72832490-0370-41ee-bff8-42c581e3f58c",
      "content": "ing optimization, including quasi-Newton methods and gradient descent, see, e.g., Luen\n\nberger et al. (1984).\n\n\n2.2.3.4 B ARZILAI –B ORWEIN M ETHOD\n\n\nBarzilai & Borwein (1988) suggested another simple, yet effective, way to determine\n\n\nthe step size _α_ [[] _[i]_ []] in the gradient-descent method by using a two-point approximation of the\n\n\nsecant equation underlying the quasi-Newton method. Using this method, we can derive\n\n\nthe step size for the current state as:\n\n\n\n_α_ [[] _[i]_ []] _≈_ _α_ [[] _[i]_ []]\nBB [=]\n\n\n\n\u0003 _**V**_ [[] _[i]_ []] _−_ _**V**_ [[] _[i][−]_ [1]] [\u0004] _·_ \u0003 _**R**_ ( _**V**_ [[] _[i]_ []] ) _−_ _**R**_ ( _**V**_ [[] _[i][−]_ [1]] ) \u0004\n\n(2.81)\n\n[ _**R**_ ( _**V**_ [[] _[i]_ []] ) _−_ _**R**_ ( _**V**_ [[] _[i][−]_ [1]] )] _·_ [ _**R**_ ( _**V**_ [[] _[i]_ []] ) _−_ _**R**_ ( _**V**_ [[] _[i][−]_ [1]] )] _[,]_\n\n\n\nWe now derive Equation 2.81.\n\n\nFirst, to avoid using future information, we assume that",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 51,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 161,
        "char_count": 932,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "c988d201-2261-4af1-b00f-f9d647948c07",
      "content": "∆ _**V**_ [[] _[i][−]_ [1]] = _**V**_ [[] _[i]_ []] _−_ _**V**_ [[] _[i][−]_ [1]] _≈−α_ [[] _[i]_ []] _**R**_ ( _**V**_ [[] _[i][−]_ [1]] ) _,_ (2.82)",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 51,
        "chunk_index": 2,
        "chunk_type": "text",
        "token_count": 28,
        "char_count": 150,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "9a7b80d9-03df-4850-9f51-5d5cfa897105",
      "content": "**28** 2. Background\n\n\ninstead of using Equation 2.70, which contains the state at a future step ( _i_ + 1). Equa\n\ntion 2.82 implies:\n\n\n_∇_ _**V**_ _⊗_ _**R**_ ( _**V**_ [[] _[i][−]_ [1]] ) _≈_ [1] (2.83)\n\n_α_ [[] _[i]_ []] _[.]_\n\n\nBy substituting Equation 2.83 into Equation 2.67 and replacing _i_ with ( _i −_ 1), we obtain:\n\n\n_**R**_ ( _**V**_ [[] _[i]_ []] ) _≈_ _**R**_ ( _**V**_ [[] _[i][−]_ [1]] ) + [1]\n\n_α_ [[] _[i]_ []] [∆] _**[V]**_ [ [] _[i][−]_ [1]]\n\n\n∆ _**V**_ [[] _[i][−]_ [1]] _−_ _α_ [[] _[i]_ []] ∆ _**R**_ [[] _[i][−]_ [1]] _≈_ **0** _,_ (2.84)\n\n\nwhere ∆ _**R**_ [[] _[i][−]_ [1]] = _**R**_ ( _**V**_ [[] _[i]_ []] ) _−_ _**R**_ ( _**V**_ [[] _[i][−]_ [1]] ), We want to find a good _α_ [[] _[i]_ []] that best satisfies\n\nEquation 2.84 in terms of least squares. Thus, we obtain _α_ BB [[] _[i]_ []] [as follows:]\n\n\n_α_ BB [[] _[i]_ []] [:= arg min] _L_ [[] _[i]_ []] ( _α_ ) (2.85)\n\n_α_\n\n\n\nR _∋L_ [[] _[i]_ []] ( _α_ ) := [1]\n\n2",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 52,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 185,
        "char_count": 948,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "9b0ebfa1-585c-487f-9b21-c685f18e58fb",
      "content": "\u000e\u000e ∆ _**V**_ [ _i−_ 1] _−_ _α_ ∆ _**R**_ [ _i−_ 1] \u000e\u000e 2 (2.86)\n\n\n\nBecause of the convexity of the problem, it is sufficient to find an _α_ that satisfies:\n\n\n\n_dL_ [[] _[i]_ []]\n\n\n_dα_\n\n\n\n= ∆ _**v**_ [[] _[i][−]_ [1]] _−_ _α_ BB [[] _[i]_ []] [∆] _**[R]**_ [[] _[i][−]_ [1]] [\u0012] _·_ \u0001 _−_ ∆ _**R**_ [[] _[i][−]_ [1]] [\u0002] = 0 _._ (2.87)\n\u0011\n_α_ [[] BB _[i]_ []]\n\n\n\nUsing the linearity of the inner product, we obtain:\n\n\n_−_ ∆ _**v**_ [[] _[i][−]_ [1]] _·_ ∆ _**R**_ [[] _[i][−]_ [1]] + _α_ BB [[] _[i]_ []] [∆] _**[R]**_ [[] _[i][−]_ [1]] _[ ·]_ [ ∆] _**[R]**_ [[] _[i][−]_ [1]] [ = 0] _[,]_\n\n\ntherefore,\n\n\n_α_ BB [[] _[i]_ []] [=] ∆ [∆] _**R**_ _**[v]**_ [[][[] _[i][i][−][−]_ [1]][1]] _[ ·]_ _·_ [ ∆] ∆ _**[R]**_ _**R**_ [[][[] _[i][i][−][−]_ [1]][1]] _[.]_ (2.88)\n\n\nEquation 2.88 is equivalent to Equation 2.81.\n\n\nAs can be seen, the derivation above aims to establish an _α_ BB [[] _[i]_ []] [that satisfies Equa-]",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 52,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 171,
        "char_count": 914,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "f9457d8c-b4d4-4b84-9175-6333b5159df7",
      "content": "tion 2.84 as closely as possible for all vertices and all feature components. This means\n\nthat _α_ [[] _[i]_ []]\nBB [contains global information because it considers all vertices, making the inclu-]",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 52,
        "chunk_index": 2,
        "chunk_type": "text",
        "token_count": 32,
        "char_count": 198,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "69c20033-144b-4f50-886e-215d398c1468",
      "content": "2.2. Numerical Analysis **29**\n\n\nsion of global interactions possible. Additionally, _α_ BB [[] _[i]_ []] [is][ E(] _[n]_ [)][-invariant because it is scalar]\n\nthat is independent of coordinates. Therefore, _α_ BB [[] _[i]_ []] [is suitable for realizing efficient PDE]\n\n\nsolvers with E( _n_ )-equivariance. Owing to its satisfactory balance between low computa\n\ntional cost and accuracy, the Barzilai–Borwein method is adopted to develop the neural\n\n\nnonlinear solver presented in Chapter 4\n\n\n2.2.4 N UMERICAL A NALYSIS FROM A G RAPH R EPRESENTATION V IEW\n\n\nIn this section, we provide an overview of several numerical analysis methods and dis\n\ncuss how they are related to graphs. In particular, we see that the discretized representation\n\n\nof spatial differentiation is closely related to graphs. For simplicity, we consider the heat\n\n\nequation _D_ = _c∇· ∇_ . However, the same discussion holds for other PDEs.\n\n\n2.2.4.1 F INITE D IFFERENCE M ETHOD",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 53,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 149,
        "char_count": 952,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "b62ddb05-19f4-4b15-924f-1c343290d2c8",
      "content": "The _finite difference method_ (FDM) is one of the most basic numerical analysis\n\n\nschemes. This method is typically applied to structured grids, where the space is discretized\n\n\nusing lines (1D), squares (2D), or cubes (3D), as shown in Figure 2.6.",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 53,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 40,
        "char_count": 249,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "add14f8e-09f0-4110-bfad-fac338e578e0",
      "content": "##### _u_\n\n図は時間tと位置x_i-1、x_i、x_i+1の関数u(t,x_i)を示し、各点間の距離をhとし、左から右に信号が伝わる様子を説明します。右端の点は左端の点よりhだけ右に移動しています。",
      "chunk_metadata": {
        "section_title": "_u_",
        "section_level": 5,
        "page": 53,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 3,
        "char_count": 104,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "17049269-fa8b-4144-8b4d-66aea05e82a1",
      "content": "##### _x_ x i− 1 x i x i +1\n\nFigure 2.6: An example of a 1D _u_ field spatially discretized using FDM.",
      "chunk_metadata": {
        "section_title": "_x_ x i− 1 x i x i +1",
        "section_level": 5,
        "page": 53,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 23,
        "char_count": 102,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "8e6a66c7-37ef-425b-bd17-bb1762877335",
      "content": "**30** 2. Background\n\n\nIn FDM, the gradient operator can be expressed as:\n\n\n_∂_\n_∂x_ _[u]_ [(] _[t, x]_ [)] _[ ≈]_ _[u]_ [(] _[t][,][ x]_ [ +] _[ h]_ _h_ [)] _[ −]_ _[u]_ [(] _[t][,][ x]_ [)] _,_ (2.89)\n\n\nwhere _h_ denotes the step size of the spatial discretization. The Laplacian operator is com\n\nputed as follows:\n\n\n_∇· ∇u_ ( _t, x_ ) _≈∇·_ _[u]_ [(] _[t][,][ x]_ [ +] _[ h]_ [)] _[ −]_ _[u]_ [(] _[t][,][ x]_ [)]\n\n_h_\n\n\n\n\n[1] [ _u_ ( _t, x_ + _h_ ) _−_ _u_ ( _t, x_ )] _−_ [ _u_ ( _t, x_ ) _−_ _u_ ( _t, x −_ _h_ )]\n\n_h_ _h_\n\n\n\n_≈_ [1]\n\n\n\n_h_\n\n\n\n= _h_ [1] [2] [[] _[u]_ [(] _[t, x]_ [ +] _[ h]_ [) +] _[ u]_ [(] _[t, x][ −]_ _[h]_ [)] _[ −]_ [2] _[u]_ [(] _[t, x]_ [)]] _[ .]_ (2.90)\n\n\n\nIf the vertex positions are denoted using indices as follows:\n\n\n_x_ _i_ +1 = _x_ + _h_\n\n\n_x_ _i_ = _x_\n\n\n_x_ _i−_ 1 = _x −_ _h._\n\n\nThe spatially discretized heat equation becomes",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 54,
        "chunk_index": 0,
        "chunk_type": "formula",
        "token_count": 185,
        "char_count": 869,
        "contains_formulas": true,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "ab96fea1-8862-48a4-9f8a-052fba924cba",
      "content": "_∂_\n_∂t_ _[u]_ [(] _[t, x]_ _[i]_ [) =] _h_ _[c]_ [2] [[] _[u]_ [(] _[t, x]_ _[i][−]_ [1] [)] _[ −]_ [2] _[u]_ [(] _[t, x]_ _[i]_ [) +] _[ u]_ [(] _[t, x]_ _[i]_ [+1] [)]] _[ .]_ (2.91)\n\n\nThis expression involves interactions between vertices, that is, edge connectivity, implying\n\n\na graphical structure. By using a matrix form, we can write:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n_u_ ( _t, x_ _i_ +1 )\n\n...\n\n\n\n...\n\n\n\n_∂_\n\n_∂t_\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n_u_ ( _t, x_ _i−_ 1 )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n...\n\n\n_. . ._ _−_ 1 2 _−_ 1 _. . ._\n\n\n...\n\n\n\n_u_ ( _t, x_ _i_ )\n\n\n\n= _−_ _[c]_\n\n_h_ [2]\n\n\n\n_u_ ( _t, x_ _i_ )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n_._ (2.92)\n\n\n\n...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that the matrix appearing on the right-hand side has the same form as the Laplacian\n\n\ngraph matrix, computed in Equation 2.20, meaning that the Laplacian operator corresponds",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 54,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 203,
        "char_count": 886,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "6ef32d0b-79a4-4613-98f2-7df0e0f43d40",
      "content": "2.2. Numerical Analysis **31**\n\n\nto the Laplacian graph matrix in a spatially discretized setting [3] . By denoting Equation 2.92\n\n\nas\n\n\n_∂_\n\n(2.93)\n\n_∂t_ _**[U]**_ [(] _[t]_ [) =] _[ −]_ _h_ _[c]_ [2] _**[L]**_ [FDM] _**[U]**_ [(] _[t]_ [)] _[,]_\n\n\none can see that\n\n\n_D ≈−_ _[c]_ (2.94)\n\n_h_ [2] _**[L]**_ [FDM]\n\n\nin the present case. The temporal discretization methods discussed in Section 2.2.2 can be\n\n\napplied to Equation 2.93. For instance, using the explicit Euler method, we obtain:\n\n\n_**U**_ ( _t_ + ∆ _t_ ) _≈_ _**U**_ ( _t_ ) _−_ _[c]_ (2.95)\n\n_h_ [2] _**[L]**_ [FDM] _**[U]**_ [(] _[t]_ [)∆] _[t.]_\n\n\nwhere the coefficient _c_ ∆ _t/h_ [2], is the diffusion number, which must be less than 1 _/_ 2 for\n\n\nstable computation.\n\n\n\n\n\n|Col1|Col2|(i, j + 1)|Col4|\n|---|---|---|---|\n||-1<br>-1|(_i, j_)<br>-1|(_i, j_)<br>-1|\n||-1|4||\n|||||\n\n\n( _i, j −_ 1)\n\n\n\nFigure 2.7: An example of 2D _u_ field spatially discretized using FDM and its corresponding\n\n\nedge connectivity.",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 55,
        "chunk_index": 0,
        "chunk_type": "formula",
        "token_count": 158,
        "char_count": 977,
        "contains_formulas": true,
        "contains_tables": true,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "90ac514c-679e-435c-904c-d37347c819fe",
      "content": "3 The matrices may differ on the boundary, where some boundary conditions are required.",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 55,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 14,
        "char_count": 87,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "acbbcae4-acd8-4aba-a1d6-d85c0599cda3",
      "content": "**32** 2. Background\n\n\nFor the 2D case, we denote the vertex positions using the following indices:\n\n\n_**x**_ _i,j_ +1 = _**x**_ _i_ + _h_ _**e**_ _y_\n\n\n_**x**_ _i−_ 1 _,j_ = _**x**_ _i_ _−_ _h_ _**e**_ _x_ _**x**_ _i,j_ = _**x**_ _**x**_ _i_ +1 _,j_ = _**x**_ _i_ + _h_ _**e**_ _x_\n\n\n_**x**_ _i,j−_ 1 = _**x**_ _i_ _−_ _h_ _**e**_ _y_ _,_\n\n\nwhere _**e**_ _x_ and _**e**_ _y_ denote the unit vectors in the _X_ and _Y_ directions, respectively. A sim\n\nilar discussion leads to the following spatially discretized representation of the 2D heat\n\n\nequation:\n\n\n_∂_\n\n[[] _[−][u]_ [(] _[t,]_ _**[ x]**_ _[i][−]_ [1] _[,j]_ [)] _[ −]_ _[u]_ [(] _[t,]_ _**[ x]**_ _[i]_ [+1] _[,j]_ [)]\n\n_∂t_ _[u]_ [(] _[t,]_ _**[ x]**_ _[i]_ [) =] _[ −]_ _h_ _[c]_ [2]\n\n\n_−u_ ( _t,_ _**x**_ _i,j−_ 1 ) _−_ _u_ ( _t,_ _**x**_ _i,j_ +1 ) + 4 _u_ ( _t,_ _**x**_ _i,j_ )] _,_ (2.96)\n\n\nwhich also corresponds to the Laplacian matrix of a corresponding graph, as shown in\n\n\nFigure 2.7.\n\n\n2.2.4.2 F INITE E LEMENT M ETHOD (FEM)",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 56,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 181,
        "char_count": 996,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "a09f2333-890e-4f36-a4e9-7fa7c4992b6b",
      "content": "The _finite element method_ (FEM) utilizes a set of functions called shape functions,\n\n\n_N_ : R _[n]_ _→_ R, for the spatial discretization of the weak form of the PDE of interest.\n\n\nFirst, we obtain the weak form by integrating the PDE over the domain Ω And multi\n\nplying by an arbitrary test function _v_, as follows:\n\n\n\n_v_ ( _**x**_ ) _[∂]_\nΩ\n\n\n\n_v_ ( _**x**_ ) _c∇· ∇u_ ( _t,_ _**x**_ ) _d_ Ω( _**x**_ ) _._ (2.97)\nΩ\n\n\n\n\n[\n\n\n\n\n_[∂]_\n\n_∂t_ _[u]_ [(] _[t,]_ _**[ x]**_ [)] _[d]_ [Ω(] _**[x]**_ [) =] [\n\n\n\nUsing\n\n\n\n_∇·_ ( _v_ ( _**x**_ ) _∇u_ ( _t,_ _**x**_ )) _d_ Ω( _**x**_ )\n\n[ Ω\n\n\n\n=\n\n[\n\n\n\n( _∇v_ ( _**x**_ )) _·_ ( _∇u_ ( _t,_ _**x**_ )) _d_ Ω( _**x**_ ) +\nΩ [\n\n\n\n_v_ ( _**x**_ ) _∇· ∇u_ ( _t,_ _**x**_ ) _d_ Ω( _**x**_ ) _,_ (2.98)\nΩ",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 56,
        "chunk_index": 1,
        "chunk_type": "formula",
        "token_count": 151,
        "char_count": 741,
        "contains_formulas": true,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "00709524-e647-477d-900e-ffbd87047745",
      "content": "2.2. Numerical Analysis **33**\n\n\n\nwe obtain:\n\n\n_c_ _v_ ( _**x**_ ) _∇· ∇u_ ( _t,_ _**x**_ ) _d_ Ω( _**x**_ )\n\n[ Ω\n\n\n\n_∇·_ ( _v_ ( _**x**_ ) _∇u_ ( _t,_ _**x**_ )) _d_ Ω( _**x**_ ) _−_ _c_\nΩ [\n\n\n\n= _c_\n\n[\n\n\n\n( _∇v_ ( _**x**_ )) _·_ ( _∇u_ ( _t,_ _**x**_ )) _d_ Ω( _**x**_ ) _._\nΩ\n\n\n\n(2.99)\n\n\n\nUsing Stokes’ theorem, the first term on the right-hand side is transformed into:\n\n\n\n_c_\n\n[\n\n\n\n_∇·_ ( _v_ ( _**x**_ ) _∇u_ ( _t,_ _**x**_ )) _d_ Ω( _**x**_ ) = _c_\nΩ [\n\n\n\n_v_ ( _**x**_ )( _∇u_ ( _t,_ _**x**_ )) _·_ _**n**_ ( _**x**_ ) _d_ Γ( _**x**_ ) _,_ (2.100)\n_∂_ Ω\n\n\n\nwhere _**n**_ ( _**x**_ ) is the normal vector at _**x**_ _∈_ _∂_ Ω. Now, if we assume ( _∇u_ ( _t,_ _**x**_ )) _·_ _**n**_ ( _**x**_ ) = 0\n\n\nfor all _**x**_ _∈_ _∂_ Ω, i.e., the adiabatic condition, Equation 2.100 is equal to zero. Therefore, the\n\n\nequation to solve is:\n\n\n\n_v_ ( _**x**_ ) _[∂]_\nΩ\n\n\n\n( _∇v_ ( _**x**_ )) _·_ ( _∇u_ ( _t,_ _**x**_ )) _d_ Ω( _**x**_ ) _._ (2.101)\nΩ\n\n\n\n\n[\n\n\n\n\n_[∂]_",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 57,
        "chunk_index": 0,
        "chunk_type": "formula",
        "token_count": 200,
        "char_count": 962,
        "contains_formulas": true,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "7ebb71c7-2ca5-43a7-b6f8-c686cbf81cfe",
      "content": "_∂t_ _[u]_ [(] _[t,]_ _**[ x]**_ [)] _[d]_ [Ω(] _**[x]**_ [) =] _[ −][c]_ [\n\n\n\nNext, we consider the spatial discretization using the set of shape functions\n\n\n_{N_ _i_ ( _**x**_ ) _}_ _i∈{_ 1 _,...,|V|}_, which is typically required to satisfy the following properties:\n\n\n_∀_ _**x**_ _∈_ Ω _,_ Y _N_ _i_ ( _**x**_ ) =1 (2.102)\n\n_i∈{_ 1 _,...,|V|}_\n\n\n_∀i ∈{_ 1 _, . . ., |V|},_ supp( _N_ _i_ ) : compact (2.103)\n\n\n_∀i, j ∈{_ 1 _, . . ., |V|}, N_ _i_ ( _**x**_ _j_ ) = _δ_ _ij_ _,_ (2.104)\n\n\nwhere supp( _N_ _i_ ) := _{_ _**x**_ _∈_ Ω _|N_ _i_ ( _**x**_ ) _̸_ = 0 _}_ is the closed support of _N_ _i_ ~~(~~ ~~_·_~~ denotes closure), and\n\n\ncompactness corresponds to the notion of a bounded and closed subset of the Euclidean\n\n\nspace. A typical example of a shape function is the Lagrange interpolating polynomial\n\n\nshown in Figure 2.8. Using a Lagrange interpolating polynomial of degree one, one can\n\n\napproximate the field of _u_ and _v_ as follows:",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 57,
        "chunk_index": 1,
        "chunk_type": "table",
        "token_count": 177,
        "char_count": 949,
        "contains_formulas": false,
        "contains_tables": true,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "90af30d0-96c3-478c-914f-84ccde1f3f81",
      "content": "_u_ ( _t,_ _**x**_ ) _≈_ Y _N_ _i_ ( _**x**_ ) _u_ _i_ ( _t_ ) (2.105)\n\n_i∈{_ 1 _,...,|V|}_",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 57,
        "chunk_index": 2,
        "chunk_type": "text",
        "token_count": 21,
        "char_count": 91,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "70fdea6b-6adb-4b2b-bfac-7951c383ebb8",
      "content": "**34** 2. Background\n\n\n\n_v_ ( _**x**_ ) _≈_ Y _N_ _i_ ( _**x**_ ) _v_ _i_ _,_ (2.106)\n\n_i∈{_ 1 _,...,|V|}_\n\n\n\nwhere _u_ _i_ ( _t_ ) and _v_ _i_ and denotes the value of _u_ ( _t,_ _**x**_ _i_ ) and _v_ ( _**x**_ _i_ ), respectively.",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 58,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 48,
        "char_count": 232,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "3bbdc9d1-2f9a-4a6f-86a3-d6e021df3c31",
      "content": "##### _u_\n\n1\n\n\n\n|X<br>N u(t, x )<br>i i<br>i<br>N5m+Ufrb0q6VL327thW/OvnSdYJXesTj8Fpkw=<laxi>u(t, x )<br>/latexi>u(t, x i−1) i fr/xpskbZ8vdPS5e+XV3n2l06Cc9L7oH1mJBOqgM=<ati>u(t, x )<br>i+1<br>N N N<br>i−1 i i+1|Col2|Col3|Col4|Col5|\n|---|---|---|---|---|\n||_Ni−_1||_Ni_|_Ni_+1|",
      "chunk_metadata": {
        "section_title": "_u_",
        "section_level": 5,
        "page": 58,
        "chunk_index": 0,
        "chunk_type": "table",
        "token_count": 22,
        "char_count": 275,
        "contains_formulas": false,
        "contains_tables": true,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "df2e7538-fa4b-4f4e-bbc3-faee92228d49",
      "content": "##### x i− 1 x i x i +1",
      "chunk_metadata": {
        "section_title": "x i− 1 x i x i +1",
        "section_level": 5,
        "page": 58,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 9,
        "char_count": 23,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "1a745bed-d881-47c8-a6ad-76bbd69bf461",
      "content": "##### _x_\n\nFigure 2.8: An example of a 1D _u_ field spatially discretized using FEM.\n\n\nUsing the shape function, we discretize Equation 2.101 as follows:\n\n\n\n( _∇N_ _i_ ( _**x**_ ) _v_ _i_ ) _·_ ( _∇N_ _j_ ( _**x**_ ) _u_ _j_ ( _t_ )) _d_ Ω( _**x**_ )\n\n[ Ω\n\n\n\n_ij_\n\n\n\nY\n\n_ij_\n\n\n\n\n[\n\n\n\n_∂_\n_N_ _i_ ( _**x**_ ) _v_ _i_ Y\nΩ _∂t_ _[N]_ _[j]_ [(] _**[x]**_ [)] _[u]_ _[j]_ [(] _[t]_ [)] _[d]_ [Ω(] _**[x]**_ [) =] _[ −][c]_\n\n\n\n_∂_\n_∂t_ _[u]_ _[j]_ [(] _[t]_ [)] [\n\n\n\n_u_ _j_ ( _t_ )\n\n[\n_ij_\n\n\n\n( _∇N_ _i_ ( _**x**_ )) _·_ ( _∇N_ _j_ ( _**x**_ )) _d_ Ω( _**x**_ ) _._\nΩ\n\n\n\nY\n\n_ij_\n\n\n\n_N_ _i_ ( _**x**_ ) _N_ _j_ ( _**x**_ ) _d_ Ω( _**x**_ ) = _−c_ Y\nΩ\n\n\n\nBy letting\n\n\n\n(2.107)\n\n\n_M_ _ij_ := _N_ _i_ ( _**x**_ ) _N_ _j_ ( _**x**_ ) _d_ Ω( _**x**_ ) (2.108)\n\n[ Ω\n\n\n\n_K_ _ij_ := _−_ _c_ ( _∇N_ _i_ ( _**x**_ )) _·_ ( _∇N_ _j_ ( _**x**_ )) _d_ Ω( _**x**_ ) _,_ (2.109)\n\n[ Ω",
      "chunk_metadata": {
        "section_title": "_x_",
        "section_level": 5,
        "page": 58,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 187,
        "char_count": 862,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "7fb312ec-20ce-4246-847b-1daa8950c55c",
      "content": "one can obtain:\n\n\nIn particular, for the 1D case:\n\n\n\n2.2. Numerical Analysis **35**\n\n\n_**M**_ _[∂]_ (2.110)\n\n_∂t_ _**[U]**_ [(] _[t]_ [) =] _**[ KU]**_ [(] _[t]_ [)] _[.]_\n\n\n\n_K_ _ij_ =\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n_−_ 2 _c/h_ if _i_ = _j_\n\n\n_c/h_ if _|i −_ _j|_ = 1\n\n\n0 otherwise\n\n\n\n_,_ (2.111)\n\n\n\nthus establishing a relationship to the graph Laplacian matrix in the FEM case. Finally, we\n\n\nobtain:\n\n\n_∂_\n(2.112)\n_∂t_ _**[U]**_ [(] _[t]_ [) =] _**[ M]**_ _[ −]_ [1] _**[KU]**_ [(] _[t]_ [)] _[.]_\n\n\nAgain, we confirm that the spatial discretization introduces interactions between vertices,\n\n\nthat is, graph-like message passing. The connectivity of the graph corresponding to the 2D\n\n\ncase is shown in Figure 2.9. It should be noted that the connectivity of the graph is not\n\n\nnecessarily the same as the edges of the mesh.\n\n中心の黒いノードが主要ノード。周囲のノードをクラスタとし、強い接続を示す。青い矢印が主な接続方向、橙色の回転矢印を含む。\n\n\nFigure 2.9: An example of 2D spatially discretized unstructured grid for FEM (black) and",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 59,
        "chunk_index": 0,
        "chunk_type": "formula",
        "token_count": 157,
        "char_count": 971,
        "contains_formulas": true,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "a146583f-58b0-4884-bac3-9a0343d7ee6a",
      "content": "its corresponding edge connectivity (blue). The connectivity of the graph is not necessarily\n\n\nthe same as the edges of the mesh.",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 59,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 21,
        "char_count": 129,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "b280c255-c2fa-48bf-a1b8-276982913167",
      "content": "**36** 2. Background\n\n\n2.2.4.3 L EAST S QUARES M OVING P ARTICLE S EMI -I MPLICIT (LSMPS) M ETHOD\n\n\nThe _least qquares moving particle semi-implicit_ (LSMPS) method is a mesh-free tech\n\nnique for solving PDEs proposed by Tamai & Koshizuka (2014). Although the scheme\n\n\nproposes a general method to approximate the differential up to an arbitrary order, for sim\n\nplicity, we introduce only the first-order gradient model, which, using the LSMPS method,\n\n\nis expressed as\n\n\n\n_⟨∇u⟩|_ _**x**_ _i_ := _**M**_ _i_ _[−]_ [1] Y\n\n_j∈N_ _i_\n\n\n\n_u_ _j_ _−_ _u_ _i_ _**x**_ _j_ _−_ _**x**_ _i_\n(2.113)\n_∥_ _**x**_ _j_ _−_ _**x**_ _i_ _∥_ _∥_ _**x**_ _j_ _−_ _**x**_ _i_ _∥_ _[w]_ _[ij]_\n\n\n\n_**M**_ _i_ :=\nY\n\n\n_l_\n\n\n\n_**x**_ _l_ _−_ _**x**_ _i_ _**x**_ _l_ _−_ _**x**_ _i_\n(2.114)\n_∥_ _**x**_ _l_ _−_ _**x**_ _i_ _∥_ _[⊗]_ _∥_ _**x**_ _l_ _−_ _**x**_ _i_ _∥_ _[w]_ _[il]_ _[,]_\n\n\n\nwhere _u_ : Ω _→_ R is a scalar field, _u_ _i_ denotes _u_ ( _**x**_ _i_ ), and _w_ _ij_ is a weight determined",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 60,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 172,
        "char_count": 979,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "3d63df74-819e-486a-a592-8ba7caff8afb",
      "content": "depending on the distance between _**x**_ _i_ and _**x**_ _j_ . Because this method does not require a\n\n\nmesh, _N_ _i_ is determined using the effective radius set by the users.\n\n\nThe first-order model is derived using the first-order Taylor expansion as follows:\n\n\n_u_ _j_ _≈_ _u_ _i_ + ( _**x**_ _j_ _−_ _**x**_ _i_ ) _·_ ( _∇u_ ) _|_ _**x**_ _i_ _, ∀j ∈N_ _i_ _._ (2.115)\n\n\nSince _∇u|_ _**x**_ _i_ is what we want to obtain, and we let _∇u|_ _**x**_ _i_ = _**X**_ _i_, then we rewrite the\n\n\nequation as:\n\n\n_**x**_ _j_ _−_ _**x**_ _i_ _u_ _j_ _−_ _u_ _i_\n(2.116)\n_∥_ _**x**_ _j_ _−_ _**x**_ _i_ _∥_ _[·]_ _**[ X]**_ _[i]_ _[ −]_ _∥_ _**x**_ _j_ _−_ _**x**_ _i_ _∥_ _[≈]_ [0] _[,][ ∀][j][ ∈N]_ _[i]_ _[.]_\n\n\nWe consider a situation in which _|N_ _i_ _| ≥_ _n_ ( _n_ denotes the spatial dimension), then one can\n\n\nobtain _**X**_ _i_ in terms of least squares, by defining a weighted evaluation function _J_ ( _**X**_ _i_ ) as\n\n\nfollows:\n\n\n\n2\n\n_._ (2.117)\n\u0016\n\n\n\n_J_ ( _**X**_ _i_ ) := [1]\n\n2",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 60,
        "chunk_index": 1,
        "chunk_type": "formula",
        "token_count": 185,
        "char_count": 989,
        "contains_formulas": true,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "e5823486-d61f-47ca-a526-b56e62c0a002",
      "content": "Y _w_ _ij_\n\n_j∈N_ _i_\n\n\n\nY\n\n\n\n_**x**_ _j_ _−_ _**x**_ _i_ _u_ _j_ _−_ _u_ _i_\n\u0015 _∥_ _**x**_ _j_ _−_ _**x**_ _i_ _∥_ _[·]_ _**[ X]**_ _[i]_ _[ −]_ _∥_ _**x**_ _j_ _−_ _**x**_ _i_ _∥_\n\n\n\n_**x**_ _j_ _−_ _**x**_ _i_\n\u0015 _∥_ _**x**_ _j_ _−_ _**x**_ _i_",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 60,
        "chunk_index": 2,
        "chunk_type": "text",
        "token_count": 49,
        "char_count": 246,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "c472dfc1-2965-4557-987b-c815bf322fd6",
      "content": "2.2. Numerical Analysis **37**\n\n\nThe best approximation to the gradient _**X**_ _i_ _[∗]_ _[≈∇][u][|]_ _**[x]**_ _i_ [in terms of least squares can be]\n\n\nobtained when _∇_ _**X**_ _i_ _J_ ( _**X**_ _i_ _[∗]_ [) =] **[ 0]** [, therefore, we solve:]\n\n\n\n_**x**_ _j_ _−_ _**x**_ _i_\n\n(2.118)\n\n\u0016 _∥_ _**x**_ _j_ _−_ _**x**_ _i_ _∥_ [=] **[ 0]** _[.]_\n\n\n\n_∇_ _**X**_ _i_ _J_ ( _**X**_ _i_ _[∗]_ [) =] Y _w_ _ij_\n\n_j∈N_ _i_\n\n\n\n_**x**_ _j_ _−_ _**x**_ _i_ _u_ _j_ _−_ _u_ _i_\n\n_i_ _[−]_\n\n\u0015 _∥_ _**x**_ _j_ _−_ _**x**_ _i_ _∥_ _[·]_ _**[ X]**_ _[∗]_ _∥_ _**x**_ _j_ _−_ _**x**_ _i_ _∥_\n\n\n\nFor any vectors _**v**_ _,_ _**w**_ _∈_ R _[n]_, the following condition holds:\n\n\n( _**v**_ _·_ _**w**_ ) _**v**_ = ( _**v**_ _⊗_ _**v**_ ) _**w**_ _,_ (2.119)\n\n\nbecause\n\n\n[( _**v**_ _·_ _**w**_ ) _**v**_ ] _i_ = Y _v_ _k_ _w_ _k_ _v_ _i_ (2.120)\n\n\n_k_\n\n= Y ( _v_ _i_ _v_ _k_ ) _w_ _k_ (2.121)\n\n\n_k_\n\n\n= [( _**v**_ _⊗_ _**v**_ ) _**w**_ ] _i_ _._ (2.122)\n\n\nTherefore, by substituting",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 61,
        "chunk_index": 0,
        "chunk_type": "formula",
        "token_count": 178,
        "char_count": 963,
        "contains_formulas": true,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "5352bf4b-f9ad-4a40-9342-97eb54a7cec3",
      "content": "_j_ _−_ _**x**_ _i_ _**x**_ _j_ _−_ _**x**_ _i_\n\n_∥_ _**x**_ _j_ _−_ _**x**_ _i_ _∥_ _[⊗]_ _∥_ _**x**_ _j_ _−_ _**x**_ _i_ _∥_\n\n\n\n_**x**_ _j_ _−_ _**x**_ _i_\n\n_i_\n\n\u0015 _∥_ _**x**_ _j_ _−_ _**x**_ _i_ _∥_ _[·]_ _**[ X]**_ _[∗]_\n\n\n\n_**x**_ _j_ _−_ _**x**_ _i_\n\u0016 _∥_ _**x**_ _j_ _−_ _**x**_ _i_\n\n\n\n_**x**_ _j_ _−_ _**x**_ _i_ _**x**_ _j_ _−_ _**x**_ _i_\n\n_∥_ _**x**_ _j_ _−_ _**x**_ _i_ _∥_ [=] \u0015 _∥_ _**x**_ _j_ _−_ _**x**_ _i_\n\n\n\n_**X**_ _i_ _[∗]_ (2.123)\n\u0016\n\n\n\ninto Equation 2.118, we get:\n\n\n\n_u_ _j_ _−_ _u_ _i_ _**x**_ _j_ _−_ _**x**_ _i_\n(2.124)\n_∥_ _**x**_ _j_ _−_ _**x**_ _i_ _∥_ _∥_ _**x**_ _j_ _−_ _**x**_ _i_ _∥_ _[.]_\n\n\n\nY\n\n_l∈N_ _i_\n\n\n\n_**x**_ _l_ _−_ _**x**_ _i_ _**x**_ _l_ _−_ _**x**_ _i_\n\u0015 _∥_ _**x**_ _l_ _−_ _**x**_ _i_ _∥_ _[⊗]_ _∥_ _**x**_ _l_ _−_ _**x**_ _i_ _∥_\n\n\n\n_**X**_ _i_ _[∗]_ [=] Y\n\u0016\n\n_j∈N_ _i_\n\n\n\n_**X**_ _i_ _[∗]_ [=] Y\n\u0016\n\n\n\nBy solving this, we finally obtain:\n\n\n\n_−_ 1\nY\n$ _j∈N_ _i_",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 61,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 170,
        "char_count": 909,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "bf8dd742-53e8-4ea4-8bc7-5ff08eac118a",
      "content": "_u_ _j_ _−_ _u_ _i_ _**x**_ _j_ _−_ _**x**_ _i_\n(2.125)\n_∥_ _**x**_ _j_ _−_ _**x**_ _i_ _∥_ _∥_ _**x**_ _j_ _−_ _**x**_ _i_ _∥_ _[,]_\n\n\n\n_**X**_ _i_ _[∗]_ [=]",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 61,
        "chunk_index": 2,
        "chunk_type": "text",
        "token_count": 30,
        "char_count": 158,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "1a790023-d725-41df-bed0-3576764aeb22",
      "content": "# Y _l∈N_ _i_\n\n_**x**_ _l_ _−_ _**x**_ _i_ _**x**_ _l_ _−_ _**x**_ _i_\n_∥_ _**x**_ _l_ _−_ _**x**_ _i_ _∥_ _[⊗]_ _∥_ _**x**_ _l_ _−_ _**x**_ _i_ _∥_\n\n\n\nwhich is equivalent to Equation 2.113.\n\n\nAlthough the Laplacian model can be derived in a different manner, one can apply\n\n\nthe gradient operator twice, and then compute the trace to obtain a representation of the",
      "chunk_metadata": {
        "section_title": "Y _l∈N_ _i_",
        "section_level": 1,
        "page": 61,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 64,
        "char_count": 365,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "36fa7e0a-d5e3-4a4c-83e7-b41ea078d1b3",
      "content": "**38** 2. Background\n\n\nLaplacian operator. Nevertheless, we again confirm that Equation 2.113 introduces edge\n\n\nconnectivity through the spatial differentiation. The LSMPS model is also important be\n\ncause it is used as the foundation of our IsoGCN model, owing to its high generalizability,\n\n\nas seen in Chapter 3.",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 62,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 47,
        "char_count": 315,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "5d594ec3-de79-4c89-a60b-0b4d9ed04105",
      "content": "# **Chapter 3** **_n_** **IsoGCN: E( )-Equivariant Graph** **Convolutional Network**\n\n3.1 I NTRODUCTION\n\n\nGraph-structured data embedded in Euclidean spaces can be utilized in many differ\n\nent fields such as object detection, structural chemistry analysis, and physical simulations.\n\n\nGraph neural networks (GNNs) have been introduced to deal with such data. The cru\n\ncial properties of GNNs include permutation invariance and equivariance, as seen in Sec\n\ntion 2.1.3.2. Besides permutations, E( _n_ )-invariance and equivariance must be addressed\n\n\nwhen considering graphs in Euclidean spaces because many properties of objects in the\n\n\nEuclidean space do not change under translation and rotation. Due to such invariance and\n\n\nequivariance, we can expect:\n\n\n1. the interpretation of the model is facilitated;\n\n\n2. the output of the model is stabilized and predictable; and\n\n\n3. the training is rendered efficient by eliminating the necessity of data augmentation,",
      "chunk_metadata": {
        "section_title": "**Chapter 3** **_n_** **IsoGCN: E( )-Equivariant Graph** **Convolutional Network**",
        "section_level": 1,
        "page": 63,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 139,
        "char_count": 965,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "aac7aabd-547a-451c-be13-236036b7a98e",
      "content": "as discussed in the literature (Thomas et al., 2018; Weiler et al., 2018; Fuchs et al., 2020).\n\n\nE( _n_ )-invariance and equivariance are inevitable, especially when applied to physical\n\n\nsimulations, because every physical quantity and physical law is either invariant or equiv\n\n39",
      "chunk_metadata": {
        "section_title": "**Chapter 3** **_n_** **IsoGCN: E( )-Equivariant Graph** **Convolutional Network**",
        "section_level": 1,
        "page": 63,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 43,
        "char_count": 282,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "ba80424f-ee92-4762-9248-71149e042c71",
      "content": "**40** 3. IsoGCN: E( _n_ )-Equivariant Graph Convolutional Network\n\n\nariant to such a transformation. Another essential requirement for such applications is\n\n\ncomputational efficiency because the primary objective of learning a physical simulation is\n\n\nto replace a computationally expensive simulation method with a faster machine learning\n\n\nmodel.\n\n\nIn this chapter, we present _IsoGCNs_, a set of simple yet powerful models that pro\n\nvide computationally-efficient E( _n_ )- invariance and equivariance based on GCNs (Kipf &\n\n\nWelling, 2017). Specifically, by simply tweaking the definition of an adjacency matrix,\n\n\nthe proposed model can realize E( _n_ )-invariance. Because the proposed approach relies on\n\n\ngraphs, it can deal with the complex shapes that are usually presented using mesh or point\n\n\ncloud data structures. Besides, a specific form of the IsoGCN layer can be regarded as a\n\n\nspatial differential operator that is essential for describing physical laws. In addition, we",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 64,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 145,
        "char_count": 991,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "5eb29d79-a366-421c-bfe6-4f55495d0cae",
      "content": "have shown that the proposed approach is computationally efficient in terms of process\n\ning graphs with up to 1M vertices that are often presented in real physical simulations.\n\n\nMoreover, the proposed model exhibited faster inference compared to a conventional fi\n\nnite element analysis approach at the same level of accuracy. Therefore, IsoGCN models\n\n\ncan suitably replace physical simulations regarding its power to express physical laws and\n\n\nfaster, scalable computation. The corresponding implementation and the dataset are avail\n\nable online [1] .\n\n\nThe main contributions of the present study can be summarized as follows:\n\n\n   - We construct E( _n_ )- invariant and equivariant GCNs, called IsoGCNs for the speci\n\nfied input and output tensor ranks.\n\n\n   - We demonstrate that an IsoGCN model enjoys competitive performance against\n\n\nstate-of-the-art baseline models on the considered tasks related to physical simula\n\ntions.",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 64,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 137,
        "char_count": 935,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "cd283d42-6e2b-4559-bd25-31feddbc83f6",
      "content": "- We confirm that IsoGCNs can be scalable to graphs with 1M vertices and achieve\n\n\ninference considerably faster than conventional finite element analysis, while exist\n\ning state-of-the-art baseline machine learning models cannot.\n\n\n1 [https://github.com/yellowshippo/isogcn-iclr2021](https://github.com/yellowshippo/isogcn-iclr2021)",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 64,
        "chunk_index": 2,
        "chunk_type": "text",
        "token_count": 34,
        "char_count": 333,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "badecb87-2302-4b3e-8236-0321eb13ae0c",
      "content": "3.2. Related Prior Work **41**\n\n\n3.2 R ELATED P RIOR W ORK\n\n\n3.2.1 GCN\n\n\nOur IsoGCN models are based on GCN (Kipf & Welling, 2017), a lightweight GNN\n\n\nmodel, because GCN shows computational efficiency compared to other GNNs, where\n\n\nmessage functions are constructed using deep neural networks (Equations 2.35 and 2.37).\n\n\nIn addition, GCN models can be E( _n_ )-invariant if all input features are E( _n_ )-equivariant\n\n\nbecause the renormalized adjacency matrix is also invariant.\n\n\nHowever, since the message function in the GCN models is determined only by in\n\nformation on edge connectivities in the graphs, there have been difficulties in capturing\n\n\ngeometrical information of meshes, e.g., the distance between vertices and angles between\n\n\nedges. GCN models can consider geometrical information, e.g., by feeding vertex posi\n\ntions to the model; however, this kind of ad-hoc solution will destroy the E( _n_ )-invariance,",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 65,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 143,
        "char_count": 931,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "b5b93ab4-f508-4fce-b3c6-165a77030e33",
      "content": "resulting in unstable prediction for geometrical data. The IsoGCN model successfully in\n\ncorporates geometrical data through IsoAM (Equation 3.8), a set of adjacency matrices\n\n\nreflecting the geometry of meshes while retaining the computational efficiency of GCNs.\n\n\n3.2.2 TFN\n\n\nAnother essential basis of our model is TFN (Thomas et al., 2018) (Equation 2.50).\n\n\nTheir model incorporates SE(3)- invariance and equivariance, where SE(3) is a subgroup\n\n\nof E(3) without reflection. The idea of TFN is to guarantee SE(3)-equivariance using spher\n\nical harmonics, which are SE(3)-equivariant functions, and nonlinear neural networks are\n\n\napplied to the norm of relative positions of vertices so that equivariance is not destroyed\n\n\ndue to nonlinearity.\n\n\nThe TFN model achieves high expressibility based on spherical harmonics and message\n\n\npassing with nonlinear neural networks. However, for this reason, considerable computa",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 65,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 130,
        "char_count": 925,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "7c35bd87-efe8-490e-8f24-1d02074599f7",
      "content": "tional resources are required. In contrast, the present study allows a significant reduction in\n\n\nthe computational costs because it eliminates spherical harmonics and nonlinear message\n\n\npassing. From this perspective, IsoGCNs are also regarded as a simplification of the TFN,\n\n\nas seen in equation 3.45.",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 65,
        "chunk_index": 2,
        "chunk_type": "text",
        "token_count": 44,
        "char_count": 305,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "fcfbaa5a-6889-465c-8e29-85c71039c83b",
      "content": "**42** 3. IsoGCN: E( _n_ )-Equivariant Graph Convolutional Network\n\n\n3.2.3 GNN M ODEL FOR P HYSICAL S IMULATION\n\n\nSeveral related studies, including those by Sanchez-Gonzalez et al. (2018; 2019); Alet\n\n\net al. (2019); Chang & Cheng (2020) focused on applying GNNs to learn physical sim\n\nulations. These approaches allowed the physical information to be introduced to GNNs;\n\n\nhowever, addressing E( _n_ )-equivariance was out of the scope of their research.\n\n\nIn the present study, we incorporate E( _n_ )-invariance and equivariance into GCNs,\n\n\nthereby, ensuring the stability of the training and inference under E( _n_ ) transformation.\n\n\nMoreover, the proposed approach is efficient in processing large graphs with up to 1M\n\n\nvertices that have a sufficient number of degrees of freedom to express complex shapes.\n\n\n3.3 M ETHOD\n\n\nIn this section, we discuss how to construct IsoGCN layers that correspond to the E( _n_ )\n\ninvariant and equivariant GCN layers. To formulate a model, we assume that:",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 66,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 157,
        "char_count": 1000,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "6125d3db-89f3-400d-bfde-67d108211b3b",
      "content": "1. only attributes associated with vertices and not edges; and\n\n\n2. graphs do not contain self-loops.\n\n\nHere, _n_ denotes the dimension of the Euclidean space we are working on.\n\n\n3.3.1 D ISCRETE T ENSOR F IELD",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 66,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 36,
        "char_count": 210,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "46dcf83c-9cde-44b8-99e8-824a8ba67554",
      "content": "### ( )\n\n中心ノード「i」を主軸とし、強い接続を示す赤色の「H^(1)_{i;1};;」と緑色の接続が主な特徴です。周囲のノード間には、赤緑の矢印で示された相互作用が存在します。",
      "chunk_metadata": {
        "section_title": "( )",
        "section_level": 3,
        "page": 66,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 4,
        "char_count": 93,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "9e175c74-f1be-4aba-ba64-f1e366666854",
      "content": "### ( b )\n\n中心ノード「i」を主軸とし、その周囲のノード間で強い接続が示された図。各ノードは「i」と「j」を表し、強い相互接続を示す青色の矢印が存在。\n\n\n\n\n\nFigure 3.1: Schematic diagrams of (a) rank-1 tensor field _**H**_ [(1)] with the number of features\n\n\nequaling 2 and (b) the simplest case of _**G**_ _ij_ ;;: = _δ_ _il_ _δ_ _jk_ _A_ _ij_ _**I**_ ( _**x**_ _k_ _−_ _**x**_ _l_ ) = _A_ _ij_ ( _**x**_ _j_ _−_ _**x**_ _i_ ).",
      "chunk_metadata": {
        "section_title": "( b )",
        "section_level": 3,
        "page": 66,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 57,
        "char_count": 357,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "96bc37bc-61e1-4fa9-a0a0-dedf0011a0fb",
      "content": "3.3. Method **43**\n\n\nFirst, we introduce the concept of _discrete tensor fields_, which play an essential role to\n\n\nconstract E( _n_ )-equivariant models. In the present study, we refer to tensor as geometric\n\n\ntensors, i.e., a rank- _p_ tensor field _**u**_ : Ω _→_ [(] _[p]_ [)] _∈_ R _[n]_ _[p]_ is equivariant with regard to the\n\n\northogonal transformation using _**U**_ expressed as:\n\n\n_**U**_ : _u_ [(] _k_ _[p]_ 1 [)] _k_ 2 _...k_ _p_ _[8→]_ _[U]_ _[k]_ 1 _[l]_ 1 _[U]_ _[k]_ 2 _[l]_ 2 _[. . . U]_ _[k]_ _p_ _[l]_ _p_ _[u]_ _l_ [(] 1 _[p]_ _l_ [)] 2 _...l_ _p_ _[.]_ (3.1)\n\n\nTo exploit the expressive power of neural networks, we consider a collection of _d_ f tensors\n\n\nwith rank- _p_ such as:\n\n\n_**h**_ [(] _[p]_ [)] := ( _**u**_ [(] _[p]_ [)] _,_ _**v**_ [(] _[p]_ [)] _,_ _**w**_ [(] _[p]_ [)] _, . . ._ ) : Ω _→_ R _[d]_ [f] _[×][n]_ _[p]_ _._ (3.2)\n} |{ ~~~~~\n_d_ f items\n\n\nThis is a collection of rank- _p_ tensor field, so note that",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 67,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 180,
        "char_count": 947,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "eb3bc7b0-82bd-4dc4-a0a3-cc641a35d15a",
      "content": "_**h**_ [(] _[p]_ [)] ( _**x**_ ) _∈_ R _[d]_ [f] _[×][n]_ _[p]_ (3.3)\n\n\nand\n\n\n_**U**_ : _h_ [(] _g_ _[p]_ ; _k_ [)] 1 _k_ 2 _...k_ _p_ _[8→]_ _[U]_ _[k]_ 1 _[l]_ 1 _[U]_ _[k]_ 2 _[l]_ 2 _[. . . U]_ _[k]_ _p_ _[l]_ _p_ _[h]_ [(] _g_ _[p]_ ; _l_ [)] 1 _l_ 2 _...l_ _p_ (3.4)\n\n\nhold.\n\n\nNow, we consider a _discrete rank-p tensor field_ _**H**_ [(] _[p]_ [)] _∈_ R _[|V|×][d]_ [f] _[×][n]_ _[p]_, as follows:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n_**H**_ [(] _[p]_ [)] :=\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n_**h**_ [(] _[p]_ [)] ( _**x**_ 1 )\n\n\n_**h**_ [(] _[p]_ [)] ( _**x**_ 2 )\n\n\n...\n\n\n\n_,_ (3.5)\n\n\n\n_**h**_ [(] _[p]_ [)] ( _**x**_ _|V|_ )\n\n\n\nwhere _d_ f denotes the number of features (channels) of _**H**_ [(] _[p]_ [)], and _**x**_ _i_ _∈_ Ω _⊂_ R _[n]_ is the\n\n\nposition of the _i_ th vertex. An example of the discrete tensor field is shown in Figure 3.1",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 67,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 182,
        "char_count": 852,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "92aa725f-031c-4948-b17d-bbe7d790f12d",
      "content": "**44** 3. IsoGCN: E( _n_ )-Equivariant Graph Convolutional Network\n\n\n(a). With the indices, we denote _H_ _i_ [(] ; _[p]_ _g_ [)] ; _k_ 1 _k_ 2 _...k_ _p_ [, where] _[ i]_ [ permutes under the permutation of]\n\n\nvertices and _k_ 1 _, . . ., k_ _p_ refers to the Euclidean representation. _g_ is the index of features, so\n\n\ninvariant with regard to permutation and E( _n_ ) transformation. Thus, under the permutation\n\n\n_π_, _**H**_ [(] _[p]_ [)] is equivariant with regard to the vertex indices:\n\n\n_π_ : _H_ _i_ [(] ; _[p]_ _g_ [)] ; _k_ 1 _k_ 2 _...k_ _p_ _[8→]_ _[H]_ _π_ [(] _[p]_ ( _i_ [)] ); _g_ ; _k_ 1 _k_ 2 _...k_ _p_ _[,]_ (3.6)\n\n\nand under orthogonal transformation _**U**_, _**H**_ [(] _[p]_ [)] is equivariant with regard to the dimensional\n\n\nindices:\n\n\n_**U**_ : _H_ _i_ [(] ; _[p]_ _g_ [)] ; _k_ 1 _k_ 2 _...k_ _p_ _[8→]_ Y _U_ _k_ 1 _l_ 1 _U_ _k_ 2 _l_ 2 _. . . U_ _k_ _p_ _l_ _p_ _H_ _i_ [(] ; _[p]_ _g_ [)] ; _l_ 1 _l_ 2 _...l_ _p_ _[.]_ (3.7)\n\n_l_ 1 _,l_ 2 _,...,l_ _p_",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 68,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 198,
        "char_count": 986,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "717fce05-4575-4e8f-a4c1-4844b6b03556",
      "content": "We use discrete tensor fields for inputs, hidden state, and outputs of our IsoGCN models.\n\n\n3.3.2 I SOMETRIC A DJACENCY M ATRIX (I SO AM)\n\n\nBefore constructing an IsoGCN, an _isometric adjacency matrix_ (IsoAM), which is at\n\n\nthe core of the IsoGCN concept must be defined.\n\n\n3.3.2.1 D EFINITION OF I SO AM\n\n\nAn IsoAM _**G**_ _∈_ R _[|V|]_ [2] _[×]_ [1] _[×][n]_ is defined as:\n\n\nR _[d]_ _∋_ _**G**_ _ij_ ;;: := _**g**_ _ij_ := Y _**T**_ _ijkl_ ( _**x**_ _k_ _−_ _**x**_ _l_ ) _,_ (3.8)\n\n_k,l∈V,k_ = _̸_ _l_\n\n\nwhere _**G**_ _ij_ ;;: is a slice in the spatial index of _**G**_, and _**T**_ _ijkl_ _∈_ R _[n][×][n]_ is an untrainable trans\n\nformation invariant and orthogonal transformation equivariant rank-2 tensor defined de\n\npending on the problem of interest. Note that we denote _G_ _ij_ ;; _k_ to be consistent with the\n\nnotation of the discrete tensor field _H_ _i_ [(] ; _[p]_ _g_ [)] ; _k_ 1 _k_ 2 _...k_ _p_ [because] _[ i]_ [ and] _[ j]_ [ permutes under the vertex]",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 68,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 176,
        "char_count": 976,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "4582f2db-0712-471e-ab98-b0335aa9b930",
      "content": "permutation and _k_ represents the spatial index while the number of features is always 1.\n\n\nThe IsoAM can be viewed as a weighted adjacency matrix for each direction and reflects\n\n\nspatial information while the usual weighted adjacency matrix cannot because a graph has\n\n\nonly one adjacency matrix. Also, IsoAM can be viewed as a rank-1-tensor-valued matrix",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 68,
        "chunk_index": 2,
        "chunk_type": "text",
        "token_count": 56,
        "char_count": 358,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "eb307fab-dc33-482f-b62c-2dc66c340b13",
      "content": "expressed as:\n\n\n\n3.3. Method **45**\n\n\n_._ (3.9)\n\n\n\n_**G**_ =\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n_**g**_ 11 _**g**_ 12 _. . ._ _**g**_ 1 _|V|_\n\n\n_**g**_ 21 _**g**_ 22 _. . ._ _**g**_ 2 _|V|_\n\n\n... ... ... ...\n\n\n\n_**g**_ _|V|_ 1 _**g**_ _|V|_ 2 _. . ._ _**g**_ _|V||V|_\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor the simplest case, one can define _**T**_ _ijkl_ = _δ_ _il_ _δ_ _jk_ _A_ _ij_ _**I**_ _n_ (Figure 3.1 (b)), where _δ_ _ij_ is\n\n\nthe Kronecker delta, _**A**_ is the adjacency matrix of the graph, and _**I**_ _n_ is the _n_ -dimensional\n\n\nidentity matrix that is the simplest rank-2 tensor. With the simplification, the definition of\n\n\nIsoAM (Equation 3.8) is expressed as:\n\n\n_**g**_ _ij_ = _A_ _ij_ ( _**x**_ _j_ _−_ _**x**_ _i_ ) _._ (3.10)\n\n\nIn the case of the path graph with five vertices (Figure 2.2), it can be expressed as:\n\n\n\n**0** _**x**_ 2 _−_ _**x**_ 1 **0** **0** **0**\n\n\n\n_**x**_ 1 _−_ _**x**_ 2 **0** _**x**_ 3 _−_ _**x**_ 2 **0** **0**\n\n\n\n_**G**_ =\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 69,
        "chunk_index": 0,
        "chunk_type": "table",
        "token_count": 198,
        "char_count": 989,
        "contains_formulas": false,
        "contains_tables": true,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "0df356ef-47fc-46b9-9d87-dc0dd2edf53e",
      "content": "**0** _**x**_ 2 _−_ _**x**_ 3 **0** _**x**_ 4 _−_ _**x**_ 3 **0**\n\n\n**0** **0** _**x**_ 3 _−_ _**x**_ 4 **0** _**x**_ 5 _−_ _**x**_ 4\n\n\n**0** **0** **0** _**x**_ 4 _−_ _**x**_ 5 **0**\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n_._ (3.11)\n\n\n\nTherefore, one can see the IsoAMs are based on relative positions of vertices, which are\n\n\ntranslation invariant and orthogonal transformation equivariant.\n\n\nIn another case, _**T**_ _ijkl_ can be determined from the geometry of a graph, as defined\n\n\nin Equation 3.47. Nevertheless, in the bulk of this section, we retain _**T**_ _ijkl_ abstract to cover\n\n\nvarious forms of interaction, such as position-aware GNNs (You et al., 2019). Here, _**G**_ is\n\n\ncomposed of only untrainable parameters and thus can be determined before training.",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 69,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 128,
        "char_count": 762,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "e9b74833-0024-4c9c-9016-7644c85eb7c7",
      "content": "**46** 3. IsoGCN: E( _n_ )-Equivariant Graph Convolutional Network\n\n\n3.3.2.2 P ROPERTY OF I SO AM\n\n\nHere, we present the properties of the IsoAM defined by Equation 3.8. We let R [3] _∋_\n\n\n_**d**_ ( _**x**_ _l_ _,_ _**x**_ _k_ ) = ( _**x**_ _k_ _−_ _**x**_ _l_ ) for the proofs. Note that _**G**_ is expressed using _**d**_ ( _**x**_ _i_ _,_ _**x**_ _j_ ) as\n\n\n_**G**_ _ij_ ;;: = _**g**_ _ij_ = Y _**T**_ _ijkl_ _**d**_ ( _**x**_ _l_ _,_ _**x**_ _k_ ) _._ (3.12)\n\n_k,l∈V,k_ = _̸_ _l_\n\n\n**Proposition 3.3.1.** _IsoAM defined in Equation 3.8 is translation invariant and orthogonal_\n\n\n_transformation equivariant, i.e., for any_ E( _n_ ) _transformation ∀_ _**t**_ _∈_ R _[n]_ _,_ _**U**_ _∈_ O( _n_ ) _, T_ :\n\n\n_**x**_ _8→_ _**Ux**_ + _**t**_ _,_\n\n\n_T_ : _G_ _ij_ ;; _k_ _8→_ Y _U_ _kl_ _G_ _ij_ ;; _l_ _._ (3.13)\n\n\n_l_\n\n\n_Proof._ First, we demonstrate the invariance with respect to the translation with _∀_ _**t**_ _∈_ R _[d]_ .",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 70,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 167,
        "char_count": 929,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "394b0015-eebc-4f76-9f14-d8c9495f239c",
      "content": "_**d**_ ( _**x**_ _i_ _,_ _**x**_ _j_ ) is transformed invariantly as follows under translation:\n\n\n_**d**_ ( _**x**_ _i_ + _**t**_ _,_ _**x**_ _j_ + _**t**_ ) = [ _**x**_ _j_ + _**t**_ _−_ ( _**x**_ _i_ + _**t**_ )]\n\n\n= ( _**x**_ _j_ _−_ _**x**_ _i_ )\n\n\n= _**d**_ ( _**x**_ _i_ _,_ _**x**_ _j_ ) _._ (3.14)\n\n\nBy definition, _**T**_ _ijkl_ is also translation invariant. Thus,\n\n\n_̸_ _̸_\n\n\n\n_̸_\n\n\nY _**T**_ _ijkl_ _**d**_ ( _**x**_ _l_ + _**t**_ _,_ _**x**_ _k_ + _**t**_ ) = Y\n\n_k,l∈V,k_ = _̸_ _l_ _k,l∈V,k̸_\n\n\n\n_̸_\n\n\nY\n\n_̸_ _̸_\n\n\n\n_̸_\n\n\nY _**T**_ _ijkl_ _**d**_ ( _**x**_ _l_ _,_ _**x**_ _k_ )\n_̸_ _k,l∈V,k_ = _̸_ _l_\n\n\n\n_̸_\n\n\n_̸_ _̸_\n\n\n= _**G**_ _ij_ ;;: _._ (3.15)\n\n\nWe then show an equivariance regarding the orthogonal transformation with _∀_ _**U**_ _∈_ O( _d_ ).\n\n\n_**d**_ ( _**x**_ _i_ _,_ _**x**_ _j_ ) is transformed as follows by orthogonal transformation:\n\n\n_**d**_ ( _**Ux**_ _i_ _,_ _**Ux**_ _j_ ) = _**Ux**_ _j_ _−_ _**Ux**_ _i_\n\n\n= _**U**_ ( _**x**_ _j_ _−_ _**x**_ _i_ )",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 70,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 177,
        "char_count": 986,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "def028e7-0acd-4d1a-8c98-19ce082e8416",
      "content": "= _**Ud**_ ( _**x**_ _i_ _,_ _**x**_ _j_ ) _._ (3.16)",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 70,
        "chunk_index": 2,
        "chunk_type": "text",
        "token_count": 11,
        "char_count": 53,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "d4364a2e-ba79-48d3-ad09-2cc16d2524a2",
      "content": "3.3. Method **47**\n\n\nBy definition, _**T**_ _ijkl_ is transformed to _**UT**_ _ijkl_ _**U**_ _[−]_ [1] by orthogonal transformation. Thus,\n\n\n_̸_ _̸_\n\n\n\nY _**UT**_ _ijkl_ _**U**_ _[−]_ [1] _**d**_ ( _**Ux**_ _l_ _,_ _**Ux**_ _k_ ) = Y\n\n_k,l∈V,k_ = _̸_ _l_ _k,l∈V,k̸_\n\n\n\nY\n\n_̸_ _̸_\n\n\n\nY _**UT**_ _ijkl_ _**U**_ _[−]_ [1] _**Ud**_ ( _**x**_ _l_ _,_ _**x**_ _k_ )\n_̸_ _k,l∈V,k_ = _̸_ _l_\n\n\n\n_̸_ _̸_\n\n\n= _**UG**_ _ij_ ;;: _._ (3.17)\n\n\nTherefore, _**G**_ is translation invariant and an orthogonal transformation equivariant.\n\n\nHere, we define essential operations between IsoAMs and discrete tensor fields. Based\n\n\non the definition of the GCN layer in the equation 2.35, let _**G**_ _∗_ _**H**_ [(0)] _∈_ R _[|V|×][f]_ _[×][d]_ denote\n\n\nthe _convolution_ between _**G**_ and the rank-0 tensor field _**H**_ [(0)] _∈_ R _[|V|×][d]_ [f] as follows:\n\n\n( _**G**_ _∗_ _**H**_ [(0)] ) _i_ ; _g_ ; _k_ := Y _**G**_ _ij_ ;; _k_ _H_ _j_ [(0)] ; _g_ ; _[.]_ (3.18)\n\n_j_",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 71,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 158,
        "char_count": 955,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "f6e48787-a4e4-447f-956a-888924234e76",
      "content": "With a rank-1 tensor field _**H**_ [(1)] _∈_ R _[|V|×][f]_ _[×][d]_, let _**G**_ _⊙_ _**H**_ [(1)] _∈_ R _[|V|×][f]_ and _**G**_ _⊙_ _**G**_ _∈_\n\n\nR _[|V|×|V|]_ denote the _contractions_ which are defined as follows:\n\n\n( _**G**_ _⊙_ _**H**_ [(1)] ) _i_ ; _g_ ; := Y _G_ _ij_ ;; _k_ _H_ _j_ [(1)] ; _g_ ; _k_ (3.19)\n\n_j,k_\n\n( _**G**_ _⊙_ _**G**_ ) _il_ ;; := Y _G_ _ij_ ;; _k_ _G_ _jl_ ; _k_ _._ (3.20)\n\n_j,k_\n\n\nThe contraction of IsoAMs _**G**_ _⊙_ _**G**_ can be interpreted as the inner product of each compo\n\nnent in the IsoAMs. Thus, the subsequent proposition follows.\n\n\n**Proposition 3.3.2.** _The contraction of IsoAMs_ _**G**_ _⊙_ _**G**_ _is_ E( _n_ ) _-invariant, i.e., for any_ E( _n_ )\n\n\n_transformation ∀_ _**t**_ _∈_ R [3] _,_ _**U**_ _∈_ O( _d_ ) _, T_ : _**x**_ _8→_ _**Ux**_ + _**t**_ _,_ _**G**_ _⊙_ _**G**_ _8→_ _**G**_ _⊙_ _**G**_ _._\n\n\n_Proof._ Here, _**G**_ _⊙_ _**G**_ is translation invariant because _**G**_ is translation invariant. We prove",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 71,
        "chunk_index": 1,
        "chunk_type": "table",
        "token_count": 169,
        "char_count": 967,
        "contains_formulas": false,
        "contains_tables": true,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "9e927c7e-9ecf-445b-a181-d6a38dba4a62",
      "content": "rotation invariance under an orthogonal transformation _∀_ _**U**_ _∈_ O( _n_ ). In addition, _**G**_ _⊙_ _**G**_ is",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 71,
        "chunk_index": 2,
        "chunk_type": "text",
        "token_count": 18,
        "char_count": 116,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "b2e25a9c-01e6-4f6d-8f9a-df2c4a1c4136",
      "content": "**48** 3. IsoGCN: E( _n_ )-Equivariant Graph Convolutional Network\n\n\ntransformed under _**U**_ as follows:\n\n\n\nY\n\n\n\nY _G_ _ij_ ;; _k_ _G_ _jl_ ;; _k_ _8→_ Y\n\n_j,k_ _j,k,m,n_\n\n\n\nY _U_ _km_ _G_ _ij_ ;; _m_ _U_ _kn_ _G_ _jl_ ;; _n_\n\n_j,k,m,n_\n\n\n\n= Y _U_ _km_ _U_ _kn_ _G_ _ij_ ;; _m_ _G_ _jl_ ;; _n_\n\n_j,k,m,n_\n\n\n\n= Y _U_ _mk_ _[T]_ _[U]_ _[kn]_ _[G]_ _[ij]_ [;;] _[m]_ _[G]_ _[jl]_ [;;] _[n]_\n\n_j,k,m,n_\n\n\n\n= Y _δ_ _mn_ _G_ _ij_ ;; _m_ _G_ _jl_ ;; _n_ (∵ property of the orthogonal matrix)\n\n_j,m,n_\n\n\n\n= Y _G_ _ij_ ;; _m_ _G_ _jl_ ;; _m_\n\n_j_\n\n\n\n= Y _G_ _ij_ ;; _k_ _G_ _jl_ ;; _k_ _._ (∵ Change the dummy index _m →_ _k_ )\n\n_j,k_\n\n\n\n(3.21)\n\n\nTherefore, _**G**_ _⊙_ _**G**_ is E( _n_ )-invariant.\n\n\nWith a rank- _p_ tensor field _**H**_ [(] _[p]_ [)] _∈_ R _[|V|×][f]_ _[×][d]_ _[p]_, let _**G**_ _⊗_ _**H**_ [(] _[p]_ [)] _∈_ R _[|V|×][f]_ _[×][d]_ [1+] _[p]_ . and _**G**_ _⊗_ _**G**_ _∈_\n\nR _[|V|×|V|×][d]_ [2] denote the **tensor products** defined as follows:",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 72,
        "chunk_index": 0,
        "chunk_type": "table",
        "token_count": 177,
        "char_count": 961,
        "contains_formulas": false,
        "contains_tables": true,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "ef2ad8bb-91ba-426c-87b1-188c42bf251b",
      "content": "( _**G**_ _⊗_ _**H**_ [(] _[p]_ [)] ) _i_ ; _g_ ; _km_ 1 _m_ 2 _...m_ _p_ := Y _**G**_ _ij_ ;; _k_ _H_ _j_ [(] ; _[p]_ _g_ [)] ; _m_ 1 _m_ 2 _...m_ _p_ _[,]_ (3.22)\n\n_j_\n\n( _**G**_ _⊗_ _**G**_ ) _il_ ;; _k_ 1 _k_ 2 := Y _**G**_ _ij_ ;; _k_ 1 _**G**_ _jl_ ;; _k_ 2 _._ (3.23)\n\n_j_\n\n\nThe tensor product of IsoAMs _**G**_ _⊗_ _**G**_ can be interpreted as the tensor product of each of\n\n\nthe IsoAMs components. Thus, the subsequent proposition follows:\n\n\n**Proposition 3.3.3.** _The tensor product of the IsoAMs_ _**G**_ _⊗_ _**G**_ _is_ E( _n_ ) _-equivariant in terms of_\n\n\n_the rank-2 tensor, i.e., for any_ E( _n_ ) _transformation ∀_ _**t**_ _∈_ R [3] _,_ _**U**_ _∈_ O( _d_ ) _, T_ : _**x**_ _8→_ _**Ux**_ + _**t**_ _,_\n\n\n_and ∀i, j ∈_ 1 _, . . ., |V|,_ ( _**G**_ _⊗_ _**G**_ ) _ij_ ;; _k_ 1 _k_ 2 _8→_ _**U**_ _k_ 1 _l_ 1 _**U**_ _k_ 2 _l_ 2 ( _**G**_ _⊗_ _**G**_ ) _ij_ ;; _l_ 1 _l_ 2 _._",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 72,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 186,
        "char_count": 893,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "bbade173-8e81-45b1-ae75-2afc8502fc58",
      "content": "3.3. Method **49**\n\n\n\n_Proof._ _**G**_ _⊗_ _**G**_ is transformed under _∀_ _**U**_ _∈_ O( _n_ ) as follows:\n\n\n\nY\n\n\n\n_U_ _kn_ _G_ _ij_ ;; _n_ _U_ _mo_ _G_ _jl_ ;; _o_\n\n_n,o_\n\n\n\n_G_ _ij_ ;; _k_ _G_ _jl_ ;; _m_ _8→_ Y\n_j_ _n,o_\n\n\n\n= Y _U_ _kn_ _G_ _ij_ ;; _n_ _G_ _jl_ ;; _o_ _U_ _om_ _[T]_ _[.]_ (3.24)\n\n\n_n,o_\n\n\n\nBy regarding _G_ _ij_ ;; _n_ _G_ _jl_ ;; _o_ as one matrix _H_ _no_, it follows the coordinate transformation of\n\n\nrank-2 tensor _**UHU**_ _[T]_ for each _i_, _j_, and _l_ .\n\n\nThis proposition is easily generalized to the tensors of higher ranks by defining the _p_ th\n\n\ntensor power of _**G**_ as follows:\n\n\n0\n\n_**G**_ = 1 (3.25)\nP\n\n\n1\n\n_**G**_ = _**G**_ (3.26)\nP\n\n\n\n_p_\nP _**G**_ =\n\n\n\n_p−_ 1\nP _**G**_ _⊗_ _**G**_ ( _p >_ 1) _._ (3.27)\n\n\n\nNamely, [O] _[p]_ _**G**_ is E( _n_ )-equivariant in terms of rank- _p_ tensor. Also, one can compute the\n\n\ntensor product between the rank- _p_ IsoAM and rank- _q_ discrete tensor field as follows:\n\n\n\n_p_\nP _**G**_\n! \"",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 73,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 183,
        "char_count": 973,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "81ff38fd-cef1-4529-9195-c78312522f7a",
      "content": "_⊗_ _**H**_ [(] _[q]_ [)] =\n\n\n=\n\n\n\n_p−_ 1\nP _**G**_\n! \"\n\n\n\n_p−_ 2\nP _**G**_\n! \"\n\n\n\n_⊗_ ( _**G**_ _⊗_ _**H**_ [(] _[q]_ [)] )\n} ~~|~~ { ~~~~~\n\nLet _**H**_ [(] _[q]_ [+1)]\n\n\n_⊗_ ( _**G**_ _⊗_ _**H**_ [(] _[q]_ [+1)] )\n} ~~|{~~ ~~~~~\n\nLet _**H**_ [(] _[q]_ [+2)]\n\n\n\n= _. . ._\n\n\n= _**H**_ [(] _[q]_ [+] _[p]_ [)] (3.28)\n\n\nSimilarly, the convolution can be generalized for [O] _[p]_ _**G**_ and the rank-0 tensor field _**H**_ [(0)] _∈_\n\n\nR _[|V|×][f]_ as follows:\n\n\n\n_p_\nP _**G**_",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 73,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 91,
        "char_count": 476,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "3f1e4eba-5534-4bbc-b8ee-13a23531fb39",
      "content": "# ! \"\n\n_∗_ _**H**_ [(0)]\n$\n\n\n\n=\nY\n_i_ ; _g_ ; _k_ 1 _k_ 2 _...k_ _p_ _j_\n\n\n\n_H_ _j_ [(0)] ; _g_ ; _[.]_ (3.29)\n\n_ij_ ;; _k_ 1 _k_ 2 _...k_ _p_\n\n\n\n_p_\nP _**G**_\n!\n\n\n\n\"",
      "chunk_metadata": {
        "section_title": "! \"",
        "section_level": 1,
        "page": 73,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 41,
        "char_count": 166,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "d5d34a92-399d-414a-a508-b986ae64c6b1",
      "content": "**50** 3. IsoGCN: E( _n_ )-Equivariant Graph Convolutional Network\n\n\nThe contraction can be generalized for [O] _[p]_ _**G**_ and the rank- _q_ tensor field _**H**_ [(] _[q]_ [)] _∈_ R _[|V|×][f]_ _[×][d]_ _[q]_\n\n\n( _p ≥_ _q_ ) as specified below:\n\n\n\n_p_\nP _**G**_",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 74,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 44,
        "char_count": 264,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "904a8c7a-2680-4dfe-8c8e-aa8bed06bd23",
      "content": "# ! \"\n\n_⊙_ _**H**_ [(] _[q]_ [)]\n$\n\n\n\n=\nY\n_i_ ; _g_ ; _k_ 1 _k_ 2 _...k_ _p−q_ _j,m_ 1 _,m_ 2 _,...,m_ _q_\n\n\n\n_H_ [(] _[q]_ [)]\n_j_ ; _g_ ; _m_ 1 _m_ 2 _...m_ _q_ _[.]_\n\n_ij_ ;; _k_ 1 _k_ 2 _...k_ _p−q_ _m_ 1 _m_ 2 _...m_ _q_\n\n\n\n_p_\nP _**G**_\n!\n\n\n\n\"\n\n\n\n(3.30)\n\n\nFor the case _p < q_, the contraction can be defined similarly.\n\n\nBy construction, one can see the the IsoAM is permutation equivariant as:\n\n\n_π_ : _**G**_ _8→_ _**P GP**_ _[⊤]_ _,_ (3.31)\n\n\nwhere _**P**_ is the corresponding permutation matrix, as discussed in Maron et al. (2018).\n\n\nThis property is the same as that of ordinary adjacency matrices. The contraction and\n\n\ntensor product of IsoAMs are also permutation equivariant because:\n\n\n_π_ : _**G**_ _⊙_ _**G**_ _8→_ _**P GP**_ _[⊤]_ _⊙_ _**P GP**_ _[⊤]_ (3.32)\n\n\n= _**P G**_ _⊙_ _**GP**_ _[⊤]_ (3.33)\n\n\n_π_ : _**G**_ _⊗_ _**G**_ _8→_ _**P GP**_ _[⊤]_ _⊗_ _**P GP**_ _[⊤]_ (3.34)\n\n\n= _**P G**_ _⊗_ _**GP**_ _[⊤]_ _._ (3.35)",
      "chunk_metadata": {
        "section_title": "! \"",
        "section_level": 1,
        "page": 74,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 175,
        "char_count": 941,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "373e820a-f47d-4dce-bc4c-b8e571ea0025",
      "content": "This discussion is also easily generalized for the higher order tensor cases.\n\n\nFinally, we can conclude that convolution, contraction, and tensor product between\n\n\nrank-p IsoAM and discrete tensor field are permutation and E( _n_ )-equivariant because each\n\n\ncomponent has such equivariance. Therefore, these operations are essential to construct\n\n\nE( _n_ )-equivariant GCN layers, IsoGCNs.\n\n\n3.3.3 C ONSTRUCTION OF I SO GCN\n\n\nUsing the operations defined above, we can construct IsoGCN layers, which take the\n\n\ndiscrete tensor field of any rank as input, and output the tensor field of any rank, which can\n\n\ndiffer from those of the input.",
      "chunk_metadata": {
        "section_title": "! \"",
        "section_level": 1,
        "page": 74,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 98,
        "char_count": 641,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "25b6b8cc-4777-413f-b3a6-942c72637f8a",
      "content": "3.3. Method **51**\n\n\n3.3.3.1 E( _n_ )-I NVARIANT L AYER\n\n\nAs can be seen in Proposition 3.3.1, the contraction of IsoAMs is E( _n_ )-invariant. There\n\nfore, an E( _n_ )-invariant layer with a rank-0 input discrete tensor field and rank-0 output dis\ncrete tensor field, IsoGCN 0 _→_ 0 : R _[|V|×][d]_ [in] _∋_ _**H**_ in [(0)] _[8→]_ _**[H]**_ out [(0)] _[∈]_ [R] _[|V|×][d]_ [out] [, can be constructed]\n\n\nas\n\n\n_**H**_ out [(0)] [= IsoGCN] [0] _[→]_ [0] [(] _**[H]**_ in [(0)] [) = PointwiseMLP] ( _**G**_ _⊙_ _**G**_ ) _**H**_ in [(0)] _,_ (3.36)\n\u0011 \u0012\n\n\nwhere PointwiseMLP : R _[|V|×][d]_ [in] _→_ R _[|V|×][d]_ [out] is the pointwise MLP defined in Equa\n\ntion 2.26. By defining _**L**_ := _**G**_ _⊙_ _**G**_ _∈_ R _[|V|×|V|]_, it can be simplified as\n\n\n_**H**_ out [(0)] [= PointwiseMLP] _**LH**_ in [(0)] _,_ (3.37)\n\u0011 \u0012\n\n\nwhich has the same form as a GCN (equation 2.35), with the exception that _**A**_ [ˆ] is replaced",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 75,
        "chunk_index": 0,
        "chunk_type": "table",
        "token_count": 162,
        "char_count": 922,
        "contains_formulas": false,
        "contains_tables": true,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "0246eb1c-9a5c-4e37-aba4-a62063fc09f2",
      "content": "with _**L**_ . It is noteworthy that _**L**_ incorporates geometry information, which was missing\n\n\nin the GCN formulation, even though the equations are similar. Therefore, we see that\n\n\nIsoGCN successfully leverages geometry information in addition to graph topology.\n\n\nAn E( _n_ )-invariant layer with the rank- _p_ input tensor field and rank-0 output tensor field,\n\nIsoGCN _p→_ 0 : R _[|V|×][d]_ [in] _[×][n]_ _[p]_ _∋_ _**H**_ in [(] _[p]_ [)] _[8→]_ _**[H]**_ out [(0)] _[∈]_ [R] _[|V|×][d]_ [out] [, can be formulated as]\n\n\n\n_**H**_ out [(0)] [= IsoGCN] _[p][→]_ [0] [(] _**[H]**_ in [(] _[p]_ [)] [) = PointwiseMLP]\n\n\n\n_p_\nP _**G**_\n!# $\n\n\n\n_⊙_ _**H**_ in [(] _[p]_ [)]\n\n\n\n\"\n\n\n\n_._ (3.38)\n\n\n\nIf _p_ = 1, such approaches utilize the inner products of the vectors in R _[d]_, these operations\n\n\ncorrespond to the extractions of a relative distance and an angle of each pair of vertices,\n\n\nwhich are employed in Klicpera et al. (2020).\n\n\n3.3.3.2 E( _n_ )-E QUIVARIANT L AYER",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 75,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 162,
        "char_count": 980,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "b3902caa-c860-40b7-841f-ec58d1f52f53",
      "content": "To construct an E( _n_ )-equivariant layer, one can use linear transformation, convolution\n\n\nand tensor product to the input tensors. If both the input and the output tensor ranks are\n\n\ngreater than 0, one can apply neither nonlinear activation nor bias addition because these\n\n\noperations will cause an inappropriate distortion of the isometry because E( _n_ ) transforma",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 75,
        "chunk_index": 2,
        "chunk_type": "text",
        "token_count": 58,
        "char_count": 372,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "c47e679d-2f39-4754-93cf-c0d96688c6e2",
      "content": "**52** 3. IsoGCN: E( _n_ )-Equivariant Graph Convolutional Network\n\n\ntion does not commute with them in general. However, a conversion that uses only a linear\n\n\ntransformation, convolution, and tensor product does not have nonlinearity, which limits\n\n\nthe predictive performance of the model. To add nonlinearity to such a conversion, we can\n\n\nfirst convert the input tensors to rank-0 ones, apply nonlinear activations, and then multiply\n\n\nthem to the higher rank tensors, as done in TFN model (Equation 2.50).\n\n\nTo achieve nonlinearity, first we define the E( _n_ ) _-equivariant pointwise MLP_ layer,\n\n\nEquivariantPointwiseMLP : R _[|V|×][d]_ [in] _[×][n]_ _[p]_ _→_ R _[|V|×][d]_ [out] _[×][n]_ _[p]_, as follows:\n\n\n2 [\u0014]\n( _p_ )\nEquivariantPointwiseMLP( _**H**_ in [(] _[p]_ [)] [) := PointwiseMLP] _**H**_ in _∗⃝_ feat _**H**_ in [(] _[p]_ [)] _[∗⃝]_ feat _**[W]**_ _[,]_\n\u000e\u000e\u000e\n\u0013\u000e\u000e\u000e\n\n\n(3.39)",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 76,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 137,
        "char_count": 895,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "ccddc027-defb-4f79-959f-46b002fc309e",
      "content": "where _∗⃝_ feat is the multiplication in the feature direction and _**W**_ _∈_ R _[d]_ [in] _[×][d]_ [out] is a trainable\n\n\nweight matrix. The pointwise MLP, PointwiseMLP : R _[|V|×][d]_ [in] _[×][n]_ _[p]_ _→_ R _[|V|×][d]_ [out] _[×][n]_ _[p]_, is cho\n\nsen to have the consistent output dimension. Using the index notation, Equation 3.39 turns\n\n\ninto:\n\n\n\n_**H**_ [(] _[p]_ [)]\nEquivariantPointwiseMLP( in [)]\ni j\n\n\n\n_i_ ; _g_ ; _k_ 1 _k_ 2 _...k_ _p_\n\n\n\n_i_ ; _h_ ; _k_ 1 _k_ 2 _...k_ _p_ _[W]_ _[hg]_ _[.]_\n\n\n(3.40)\n\n\n\n=\nY\n\n\n_h_\n\n\n\n2 [\u0014\u0016]\n( _p_ )\nPointwiseMLP _**H**_\nin\n\u000e\u000e\u000e\n\u0015 \u0013\u000e\u000e\u000e _i_ ; _h_ ;\n\n\n\n_**H**_ [(] _[p]_ [)]\nin\ni j\n\n\n\nOne can easily see that EquivariantPointwiseMLP defined in Equation 3.39 is translation\n\n\ninvariant and orthogonal transformation equivariant because\n\n\n\n2\n( _p_ )\n_**H**_\nin\n\u000e\u000e\u000e \u000e\u000e\u000e _i_\n\n\n\n_i_ ; _g_ ; _k_ 1 _k_ 2 _...k_ _p_\n\n\n\n(3.41)\n_i_ ; _g_ ; _k_ 1 _k_ 2 _...k_ _p_\n\n\n\n_i_ ; _g_ ; [=] Y\n\n\n\n_**H**_ [(] _[p]_ [)]\nin\ni j\n\n\n\n_**H**_ [(] _[p]_ [)]\nin\ni j",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 76,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 182,
        "char_count": 985,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "7deab287-83f5-4ab6-a03b-4c52e5455010",
      "content": "_k_ 1 _k_ 2 _...k_ _p_\n\n\n\nis E( _n_ )-invariant. We use _∥·∥_ [2] instead of _∥·∥_ in the function because computation of _∥·∥_\n\n\nrequires computation of the square root, which leads extreme gradient around zero. One\n\n\ncan regard EquivariantPointwiseMLP as an equivariant function that does not change the\n\n\ninput tensor rank and may change the number of features.",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 76,
        "chunk_index": 2,
        "chunk_type": "text",
        "token_count": 59,
        "char_count": 364,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "1759fac2-40bd-4308-8b35-f5dcec383921",
      "content": "3.3. Method **53**\n\n\nThe nonlinear E( _n_ )-equivariant layer with the rank- _p_ input discrete tensor field and the\n\nrank- _q_ ( _p ≤_ _q_ ) output discrete tensor field, IsoGCN _p→q_ : R _[|V|×][d]_ [in] _[×][n]_ _[p]_ _∋_ _**H**_ in [(] _[p]_ [)] _[8→]_ _**[H]**_ out [(] _[q]_ [)] _[∈]_\n\n\nR _[|V|×][d]_ [out] _[×][n]_ _[q]_, can be defined as:\n\n\n\n_**H**_ out [(] _[q]_ [)] [= IsoGCN] _[p][→][q]_ [(] _**[H]**_ in [(] _[p]_ [)] [) := EquivariantPointwiseMLP]\n\n\n\n_q−p_\nP _**G**_\n!# $\n\n\n\n_⊗_ _**H**_ in [(] _[p]_ [)]\n\n\n\n\"\n\n\n\n_._\n\n\n\n(3.42)\n\n\nIf _p_ = 0, we regard _**G**_ _⊗_ _**H**_ [(0)] as _**G**_ _∗_ _**H**_ [(0)] . If _p_ = _q_, one can add the residual\n\n\nconnection (He et al., 2016) in Equation 3.42. If _p > q_,\n\n\n\n_**H**_ out [(] _[q]_ [)] [= IsoGCN] _[p][→][q]_ [(] _**[H]**_ in [(] _[p]_ [)] [) := EquivariantPointwiseMLP]\n\n\n\n_p−q_\nP _**G**_\n!# $\n\n\n\n_⊙_ _**H**_ in [(] _[p]_ [)]\n\n\n\n\"\n\n\n\n_._\n\n\n\n(3.43)",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 77,
        "chunk_index": 0,
        "chunk_type": "formula",
        "token_count": 159,
        "char_count": 912,
        "contains_formulas": true,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "ea6976db-6938-42ad-978a-52bbec96ab84",
      "content": "In general, the nonlinear E( _n_ )-equivariant IsoGCN layer with the rank- _P_ min to rank\n_P_ max\n_P_ max input tensor field _**H**_ in [(] _[p]_ [)] out [can be defined]\no p _p_ = _P_ min [and the rank-] _[q]_ [ output tensor field] _**[ H]**_ [(] _[q]_ [)]\n\n\nas:\n\n\n\n(3.44)\n\u0014\n\n\n\n_**H**_ out [(] _[q]_ [)] [=IsoGCN] _[·→][q]_\n\n\n\n_P_ max\n_**H**_ [(] _[p]_ [)]\nin\n\u0013o p _p_ = _P_ min\n\n\n\n_,_\n\u0014\n\n\n\n_**H**_ [(] _[q]_ [)]\n:=EquivariantPointwiseMLP( in [) +] _**[ F]**_ [gather]\n\n\n\n_P_ max\nIsoGCN _p→q_ ( _**H**_ in [(] _[p]_ [)] [)]\n\u0013o p _p_ = _P_ min\n\n\n\n(3.45)\n\n\nwhere _**F**_ gather denotes a function such as summation, product and concatenation in the\n\n\nfeature direction. One can see that this layer is similar to that in the TFN (Equation 2.50),\n\n\nwhile there are no spherical harmonics and trainable message passing in the IsoGCN model.\n\n\nTo be exact, the output of the layer defined above is translation invariant. To out",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 77,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 165,
        "char_count": 923,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "5c25e3be-6d84-4bae-9003-ca33157f9260",
      "content": "put translation equivariant variables such as the vertex positions after deformation (which\n\n\nchange accordingly with the translation of the input graph), one can first define the ref\n\nerence vertex position _**x**_ ref for each graph, then compute the translation invariant output\n\n\nusing equation 3.45, and finally, add _**x**_ ref to the output. For more detailed information on\n\n\nIsoGCN modeling, see Section 3.3.5.",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 77,
        "chunk_index": 2,
        "chunk_type": "text",
        "token_count": 62,
        "char_count": 419,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "5b57a43d-dffd-48cc-8eef-b90ca0f4623b",
      "content": "**54** 3. IsoGCN: E( _n_ )-Equivariant Graph Convolutional Network\n\n\n3.3.4 I SO AM R EFINED FOR N UMERICAL A NALYSIS\n\n\nThe IsoAM _**G**_ is defined in a general form for the propositions to work with various\n\n\nclasses of graph. In this section, we concretize the concept of the IsoAM to apply an\n\n\nIsoGCN to mesh-structured numerical analysis data. Here, a mesh is regarded as a graph\n\n\nregarding the points in the mesh as vertices of the graph and assuming two vertices are\n\n\nconnected when they share the same element (cell), as seen in Figure 2.9.\n\n\n3.3.4.1 D EFINITION OF D IFFERENTIAL I SO AM\n\n\nAs seesn in Section 2.2.4, the graph connectivities are closely related to spatial dif\n\nferentiation. Therefore, it is natural to construct a graph reflecting the structure of spatial\n\n\ndifferentiation. Here, we define the _differential IsoAM_, a concrete instance of IsoAMs re\n\nfined for numerical analysis.",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 78,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 149,
        "char_count": 908,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "dc90b9dd-f462-4550-8538-3b5365ece836",
      "content": "The differential IsoAM _**G**_ [˜] _,_ _**G**_ [ˆ] _∈_ R _[|V|×|V|×][d]_ is defined as follows:\n\n\n_G_ ˜ _ij_ ;; _k_ = ˆ _G_ _ij_ ;; _k_ _−_ _δ_ _ij_ Y _G_ ˆ _il_ ;; _k_ (3.46)\n\n\n_l_\n\n\nˆ _**x**_ _j_ _−_ _**x**_ _i_\n_G_ _ij_ ;;: = _**M**_ _i_ _[−]_ [1] (3.47)\n_∥_ _**x**_ _j_ _−_ _**x**_ _i_ _∥_ [2] _[w]_ _[ij]_ _[A]_ _[ij]_ [(] _[m]_ [)]\n\n\n\n_**M**_ _i_ =\nY\n\n\n_l_\n\n\n\n_**x**_ _l_ _−_ _**x**_ _i_ _**x**_ _l_ _−_ _**x**_ _i_\n(3.48)\n_∥_ _**x**_ _l_ _−_ _**x**_ _i_ _∥_ _[⊗]_ _∥_ _**x**_ _l_ _−_ _**x**_ _i_ _∥_ _[w]_ _[il]_ _[A]_ _[il]_ [(] _[m]_ [)] _[,]_\n\n\n\nwhere R _[|V|×|V|]_ _∋_ _**A**_ ( _m_ ) := min ( [Q] _[m]_ _k_ =1 _**[A]**_ _[k]_ _[,]_ [ 1)][ is an adjacency matrix up to] _[ m]_ [ hops and]\n\n\n_w_ _ij_ _∈_ R is an untrainable weight between the _i_ th and _j_ th vertices that is determined depend\n\ning on the tasks [2] . Although one could define _w_ _ij_ as a function of the distance _∥_ _**x**_ _j_ _−_ _**x**_ _i_ _∥_,",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 78,
        "chunk_index": 1,
        "chunk_type": "table",
        "token_count": 182,
        "char_count": 932,
        "contains_formulas": false,
        "contains_tables": true,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "c4ba5f86-a8d3-43bc-aa9d-3d574e198dc8",
      "content": "_w_ _ij_ was kept constant with respect to the distance required to maintain the simplicity of the\n\n\nmodel with fewer hyperparameters.\n\n\nBy regarding\n\n\n_**T**_ _ijkl_ = _δ_ _il_ _δ_ _jk_ _**M**_ _i_ _[−]_ [1] _w_ _ij_ _**A**_ _ij_ ( _m_ ) _/∥_ _**x**_ _j_ _−_ _**x**_ _i_ _∥_ [2] (3.49)\n\n\n2 _**M**_ _i_ is invertible when the number of independent vectors in _{_ _**x**_ _l_ _−_ _**x**_ _i_ _}_ _l_ is greater than or equal to the\nspace dimension _n_, which is true for common meshes, e.g., a solid mesh in 3D Euclidean space.",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 78,
        "chunk_index": 2,
        "chunk_type": "text",
        "token_count": 94,
        "char_count": 526,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "d2f75c90-b2a0-44f1-a7f3-55913db35617",
      "content": "3.3. Method **55**\n\n\nin equation 3.8, one can see that _**G**_ [ˆ] is qualified as an IsoAM. Because a linear combination\n\nof IsoAMs is also an IsoAM, _**G**_ [˜] is an IsoAM. Thus, they provide translation invariance and\n\northogonal transformation equivariance. _**G**_ [˜] can be obtained only from the mesh geometry\n\n\ninformation, thus can be computed in the preprocessing step.\n\n\nHere, _**G**_ [˜] is designed such that it corresponds to the gradient operator model used in the\n\n\nLSMPS method(Tamai & Koshizuka, 2014) (Equation 2.113, while we added _A_ _ij_ ( _m_ ) fac\ntor to work on graphs. As presented in Table 3.1, _**G**_ [˜] is closely related to many differential\n\n\noperators, such as the gradient, divergence, Laplacian, Jacobian, and Hessian. Therefore,\n\n\nthe considered IsoAM plays an essential role in constructing neural network models that",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 79,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 135,
        "char_count": 858,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "f9dc7071-64d3-46d8-a3b7-feddc2993276",
      "content": "are capable of learning differential equations. In the following sections, we discuss the con\nnection between the IsoAM for numerical analysis _**G**_ [˜] and the differential operators such as\n\n\nthe gradient, divergence, the Laplacian, the Jacobian, and the Hessian operators.\n\n\nTable 3.1: Correspondence between the differential operators and the expressions using the\n\n\nIsoAM _**G**_ [˜] .\n\n\n**Differential operator** **Expression**\n\n\n˜\nGradient _**G**_ _∗_ _**H**_ [(0)]\n\n\nDivergence _**G**_ ˜ _⊙_ _**H**_ [(1)]\n\n\nLaplacian _**G**_ ˜ _⊙_ _**GH**_ ˜ [(0)]\n\n\nJacobian _**G**_ ˜ _⊗_ _**H**_ [(1)]\n\n\n˜ ˜\nHessian _**G**_ _⊗_ _**G**_ _∗_ _**H**_ [(0)]\n\n\n3.3.4.2 P ARTIAL D ERIVATIVE\n\n\nFirst let us consider a partial derivative model of a rank-0 discrete tensor field _**H**_ [(0)] at\n\n\n\nthe _i_ th vertex and _g_ th feature regarding the _k_ th axis _∂_ _**H**_ [(0)] _/∂x_ _k_\n\n\n\n_i_ ; _g_ ; _[∈]_ [R][ (] _[k][ ∈{]_ [1] _[, . . ., n][}]_ [)][.]",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 79,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 149,
        "char_count": 945,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "84efd0da-2c20-46d3-bd9f-aa0be98e2e43",
      "content": "Recalling the gradient model of the LSMPS method (Equation 2.113),\n\n\n\n_∂_ _**H**_ (0)\n _∂x_ _k_\n\n\n\n\n\n\n\n:= _**M**_ _[−]_ [1]\n_i_ Y\n_i_ ; _g_ ;\n\n\n\n_j_\n\n\n\n_H_ _j_ [(0)] ; _g_ ; _[−]_ _[H]_ _i_ [(0)] ; _g_ ; _x_ _jk_ _−_ _x_ _ik_\n_∥_ _**x**_ _j_ _−_ _**x**_ _i_ _∥_ _∥_ _**x**_ _j_ _−_ _**x**_ _i_ _∥_ _[w]_ _[ij]_ _[A]_ _[ij]_ [(] _[m]_ [)]\n\n\n\nˆ\n\n= Y _G_ _ijk_ ( _H_ _j_ [(0)] ; _g_ ; _[−]_ _[H]_ _i_ [(0)] ; _g_ ; [)] _[.]_ (3.50)\n\n_j_",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 79,
        "chunk_index": 2,
        "chunk_type": "text",
        "token_count": 88,
        "char_count": 433,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "f791c25e-9df8-4f07-a9d1-0e246ca7fb5f",
      "content": "**56** 3. IsoGCN: E( _n_ )-Equivariant Graph Convolutional Network\n\n\n3.3.4.3 G RADIENT\n\n\n_**G**_ ˜ is similar to a graph Laplacian matrix based on ˆ _**G**_ ; however, surprizingly, ˜ _**G**_ _∗_ _**H**_ [(0)]\n\n\ncan be interpreted as the gradient within the Euclidean space. Let _∇_ _**H**_ [(0)] _∈_ R _[|V|×][f]_ _[×][d]_ be\n\n\nan approximation of the gradient of _**H**_ [(0)] . Using Equation 3.50, the gradient model can be\n\n\nexpressed as follows:\n\n\n\n_∇_ _**H**_ [(0)]\n\n\n\n_i_ ; _g_ ; _k_ [=] _[ ∂H]_ _∂x_ _i_ [(0)] _k_ ; _g_ ; (3.51)\n\n= _G_ [ˆ] _ijk_ ( _H_ _j_ [(0)] ; _g_ ; _[−]_ _[H]_ _i_ [(0)] ; _g_ ; [)] _[.]_ (3.52)\n\n\n\nUsing this gradient model, we can confirm that ( _**G**_ [˜] _∗_ _**H**_ [(0)] ) _i_ ; _g_ ; _k_ = ( _∇_ _**H**_ [(0)] ) _i_ ; _glk_ because\n\n\n\n˜\n_**G**_ _∗_ _**H**_ [(0)] [F]\nE\n\n\n\n_i_ ; _g_ ; _k_ [=] Y _G_ ˜ _ij_ ;; _k_ _H_ _j_ [(0)] ; _g_ ; (3.53)\n\n_j_\n\n\n\n=\nY\n\n\n\n( _G_ [ˆ] _ij_ ;; _k_ _−_ _δ_ _ij_ Y\n_j_ _l_\n\n\n\nˆ\n_G_ _il_ ;; _k_ ) _H_ _j_ [(0)] ; _g_ ;\n\n_l_",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 80,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 196,
        "char_count": 988,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "bd9b538c-7138-49a5-ac42-86ea0930463a",
      "content": "ˆ\n_G_ _ij_ ;; _k_ _H_ _j_ [(0)] ; _g_ ; _[−]_ Y\n_j_ _j,l_\n\n\nˆ\n_G_ _ij_ ;; _k_ _H_ _j_ [(0)] ; _g_ ; _[−]_ Y\n_j_ _l_\n\n\nˆ\n_G_ _ij_ ;; _k_ _H_ _j_ [(0)] ; _g_ ; _[−]_ Y\n_j_ _j_\n\n\n\n=\nY\n\n\n\n_δ_ _ij_ _G_ [ˆ] _il_ ;; _k_ _H_ _j_ [(0)] ; _g_ ;\n\n_j,l_\n\n\n\n=\nY\n\n\n\nˆ\n_G_ _il_ ;; _k_ _H_ _i_ [(0)] ; _g_ ;\n\n_l_\n\n\n\n=\nY\n\n\n\nˆ\n_G_ _ij_ ;; _k_ _H_ _i_ [(0)] ; _g_ ; (∵ Change the dummy index _l →_ _j_ )\n\n_j_\n\n\n\nˆ\n\n= Y _G_ _ij_ ;; _k_ ( _H_ _j_ [(0)] ; _g_ ; _[−]_ _[H]_ _i_ [(0)] ; _g_ ; [)]\n\n_j_\n\n\n\n= _∇_ _**H**_ [(0)]\n\n\n\n_i_ ; _g_ ; _k_ _[.]_ (3.54)\n\n\n\nTherefore, _**G**_ [˜] _∗_ can be interpreted as the gradient operator within a Euclidean space.\n\n\n3.3.4.4 D IVERGENCE\n\n\nWe show that _**G**_ [˜] _⊙_ _**H**_ [(1)] corresponds to the divergence. Using Equation 3.50, the\n\n\ndivergence model _∇·_ _**H**_ [(1)] _∈_ R _[|V|×][f]_ is expressed as follows:\n\n\n\n(3.55)\n\n, _i_ ; _g_ ;\n\n\n\n_∇·_ _**H**_ [(1)]\n\n\n\n_i_ ; _g_ ; [=]\n\n\n\n+Y _k_\n\n\n\n_∂_ _**H**_ [(1)]\n\n\n_∂x_ _k_",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 80,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 199,
        "char_count": 945,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "65bd2b40-bfe5-47b2-9eb6-5481b7b2a1e9",
      "content": "3.3. Method **57**\n\n\nˆ\n\n= Y _G_ _ij_ ;; _k_ ( _H_ _j_ [(1)] ; _g_ ; _k_ _[−]_ _[H]_ _i_ [(1)] ; _g_ ; _k_ [)] _[.]_ (3.56)\n\n_j,k_\n\n\n\nThen, _**G**_ [˜] _⊙_ _**H**_ [(1)] is\n\n\n\n_**G**_ ˜ _⊙_ _**H**_ [(1)] [F]\nE\n\n\n\n_i_ ; _g_ ; [=] Y _G_ ˜ _ij_ ;; _k_ _H_ _j_ [(1)] ; _g_ ; _k_\n\n_j,k_\n\n\n\n\"\n\n\n\n=\nY\n\n_j,k_\n\n\n=\nY\n\n\n\n_G_ ˆ _ij_ ;; _k_ _−_ _δ_ _ij_ Y _G_ ˆ _il_ ;; _k_\n! _l_\n\n\n\nˆ\n\nY _G_ _il_ ;; _k_ _H_ _i_ [(1)] ; _g_ ; _k_\n\n_l,k_\n\n\n\n_H_ [(1)]\n_j_ ; _g_ ; _k_\n\n\n\nˆ\n\nY _G_ _ij_ ;; _k_ _H_ _j_ [(1)] ; _g_ ; _k_ _[−]_ Y\n\n_j,k_ _l,k_\n\n\n\nˆ\n\n= Y _G_ _ij_ ;; _k_ ( _H_ _j_ [(1)] ; _g_ ; _k_ _[−]_ _[H]_ _i_ [(1)] ; _g_ ; _k_ [)] (∵ Change the dummy index _l →_ _j_ )\n\n_j,k_\n\n\n\n= _∇·_ _**H**_ [(1)]\n\n\n\n_i_ ; _g_ ; _[.]_ (3.57)\n\n\n\n3.3.4.5 L APLACIAN O PERATOR\n\n\nWe prove that _**G**_ [˜] _⊙_ _**G**_ [˜] corresponds to the Laplacian operator within a Euclidean space.\n\n\nUsing Equation 3.50, the Laplacian model _∇· ∇_ _**H**_ [(0)] _∈_ R _[|V|×][f]_ can be expressed as\n\n\nfollows:\n\n\n\n _i_\n\n\n\n _i_ ; _g_ ;",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 81,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 212,
        "char_count": 988,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "e39b577a-22bc-4cf3-94f1-ad7bc40e9f8a",
      "content": "_∂_ _∂_ _**H**_\n _∂x_ _k_ _∂x_ _k_\n\n\n\n_∇· ∇_ _**H**_ [(0)]\n\n\n\n_i_ ; _g_ ; [:=] Y\n\n_k_\n\n\n\n_∂_ _**H**_\n_j_ ; _g_ ; _−_ _∂x_ _k_\n\n\n\n _i_ ; _g_ ;\n\n\n\n,\n\n\n\nˆ\n\n= Y _G_ _ij_ ;; _k_\n\n_j,k_\n\n\nˆ\n\n= Y _G_ _ij_ ;; _k_\n\n_j,k_\n\n\n\n_∂_ _**H**_\n\n\n_∂x_ _k_\n\n,",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 81,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 53,
        "char_count": 240,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "256290f4-3044-4cea-8ac2-0a13f2aa05b1",
      "content": "# Y _l_\n\nˆ\n_G_ _jl_ ;; _k_ ( _H_ _l_ [(0)] ; _g_ ; _[−]_ _[H]_ _j_ [(0)] ; _g_ ; [)] _[ −]_ Y\n\n_l_ _l_\n\n\n\nˆ\n_G_ _il_ ;; _k_ ( _H_ _l_ [(0)] ; _g_ ; _[−]_ _[H]_ _i_ [(0)] ; _g_ ; [)]\n\n_l_\n\n\n\n$\n\n\n\nˆ ˆ\n\n= Y _G_ _ij_ ;; _k_ ( ˆ _G_ _jl_ ;; _k_ _−_ _G_ _il_ ;; _k_ )( _H_ _l_ [(0)] ; _g_ ; _[−]_ _[H]_ _j_ [(0)] ; _g_ ; [)] _[.]_ (3.58)\n\n_j,k,l_\n\n\n\nThen, ( _**G**_ [˜] _⊙_ _**G**_ [˜] ) _**H**_ [(0)] is\n\n\n(( _**G**_ [˜] _⊙_ _**G**_ [˜] ) _**H**_ [(0)] ) _i_ ; _g_ ; = Y _G_ ˜ _ij_ ;; _k_ ˜ _G_ _jl_ ;; _k_ _H_ _l_ [(0)] ; _g_ ;\n\n_j,k,l_",
      "chunk_metadata": {
        "section_title": "Y _l_",
        "section_level": 1,
        "page": 81,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 131,
        "char_count": 532,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "8b69def9-6016-4d6f-8139-3a8133192c8e",
      "content": "**58** 3. IsoGCN: E( _n_ )-Equivariant Graph Convolutional Network\n\n\n\n_G_ ˆ _jl_ ;; _k_ _−_ _δ_ _jl_ Y _G_ ˆ _jn_ ;; _k_\n\" ! _n_\n\n\n\n\"\n\n\n\n=\nY\n\n_j,k,l_\n\n\n=\nY\n\n\n\n_G_ ˆ _ij_ ;; _k_ _−_ _δ_ _ij_ Y _G_ ˆ _im_ ;; _k_\n! _m_\n\n\n\nˆ\n\nY _G_ _ij_ ;; _k_ ˆ _G_ _jn_ ;; _k_ _H_ _j_ [(0)] ; _g_ ;\n\n_j,k,n_\n\n\n\n_H_ [(0)]\n_l_ ; _g_ ;\n\n\n\nˆ\n\nY _G_ _ij_ ;; _k_ ˆ _G_ _jl_ ;; _k_ _H_ _l_ [(0)] ; _g_ ; _[−]_ Y\n\n_j,k,l_ _j,k,n_\n\n\n\nˆ\n\nY _G_ _im_ ;; _k_ ˆ _G_ _il_ ;; _k_ _H_ _l_ [(0)] ; _g_ ; [+] Y\n\n_k,l,m_ _k,m,n_\n\n\n\n_−_\nY\n\n\n\nˆ\n\nY _G_ _im_ ;; _k_ ˆ _G_ _in_ ;; _k_ _H_ _i_ [(0)] ; _g_ ;\n\n_k,m,n_\n\n\n\nˆ\n\nY _G_ _ij_ ;; _k_ ˆ _G_ _jl_ ;; _k_ _H_ _l_ [(0)] ; _g_ ; _[−]_ Y\n\n_j,k,l_ _j,k,n_\n\n\n\n=\nY\n\n\n\nˆ\n\nY _G_ _ij_ ;; _k_ ˆ _G_ _jn_ ;; _k_ _H_ _j_ [(0)] ; _g_ ;\n\n_j,k,n_\n\n\n\nˆ\n\nY _G_ _ij_ ;; _k_ ˆ _G_ _il_ ;; _k_ _H_ _l_ [(0)] ; _g_ ; [+] Y\n\n_k,l,j_ _k,j,n_\n\n\n\n_−_\nY\n\n\n\nˆ\n\nY _G_ _ij_ ;; _k_ ˆ _G_ _in_ ;; _k_ _H_ _i_ [(0)] ; _g_ ;\n\n_k,j,n_\n\n\n\n(∵ Change the dummy index _m →_ _j_ for the third and fourth terms)\n\n\nˆ ˆ",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 82,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 232,
        "char_count": 986,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "4585e5cc-e2b9-4772-815b-05c239ce054e",
      "content": "= Y _G_ _ij_ ;; _k_ ( ˆ _G_ _jl_ ;; _k_ _−_ _G_ _il_ ;; _k_ )( _H_ _l_ [(0)] ; _g_ ; _[−]_ _[H]_ _j_ [(0)] ; _g_ ; [)]\n\n_j,k,l_\n\n\n(∵ Change the dummy index _n →_ _l_ for the second and fourth terms)\n\n\n\n= _∇_ [2] _**H**_ [(0)]\n\n\n\n_i_ ; _g_ ; _[.]_ (3.59)\n\n\n\n3.3.4.6 J ACOBIAN AND H ESSIAN O PERATORS\n\n\nConsidering a similar discussion, we can show the following dependencies. For the\n\n\nJacobian model, _∇⊗_ _**H**_ [(1)] _∈_ R _[|V|×][f]_ _[×][d][×][d]_,\n\n\n\n(3.60)\n\n_i_ ; _g_ ; _k_\n\n\n\n_∇⊗_ _**H**_ [(1)]\n\n\n\n_∂_ _**H**_ (1)\n_i_ ; _g_ ; _kl_ [=] _∂x_ _l_\n\n\n\n\n\n\n\nˆ\n\n= Y _G_ _ij_ ;; _l_ ( _H_ _j_ [(1)] ; _g_ ; _k_ _[−]_ _[H]_ _i_ [(1)] ; _g_ ; _k_ [)] (3.61)\n\n_j_\n\n\n\n= ( _**G**_ [˜] _⊗_ _**H**_ [(1)] ) _i_ ; _g_ ; _lk_ _._ (3.62)\n\n\nFor the Hessian model, _∇⊗∇_ _**H**_ [(0)] _∈_ R _[|V|×][f]_ _[×][d][×][d]_,\n\n\n\n_∂_\n_**H**_ [(0)]\n_∂x_ _l_ \n\n\n\n(3.63)\n\n_i_ ; _g_ ;\n\n\n\n_∇⊗∇_ _**H**_ [(0)]\n\n\n\n_∂_\n_i_ ; _g_ ; _kl_ [=] _∂x_ _k_",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 82,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 181,
        "char_count": 919,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "6720d160-cf74-4f0e-ad7f-a4e36a0931be",
      "content": "3.3. Method **59**\n\n\nˆ\n\n= Y _G_ _ij_ ;; _k_ [ ˆ _G_ _jm_ ;; _l_ ( _H_ _m_ [(0)] ; _g_ ; _[−]_ _[H]_ _l_ [(0)] ; _g_ ; [)] _[ −]_ _[G]_ [ˆ] _[im]_ [;;] _[l]_ [(] _[H]_ _m_ [(0)] ; _g_ ; _[−]_ _[H]_ _i_ [(0)] ; _g_ ; [)]] (3.64)\n\n_j,m_\n\n\n\n= ( _**G**_ [˜] _⊗_ _**G**_ [˜] ) _∗_ _**H**_ [(0)] [j]\ni\n\n\n3.3.5 I SO GCN M ODELING D ETAILS\n\n\n\n(3.65)\n_i_ ; _g_ ; _kl_ _[.]_\n\n\n\nTo achieve E( _n_ )- invariance and equivariance, there are several rules to follow. Here,\n\n\nwe describe the desired focus when constructing an IsoGCN model. In this section, a rank\n\n_p_ tensor denotes a tensor the rank of which is _p ≥_ 1 and _σ_ denotes a nonlinear activation\n\n\nfunction. _**W**_ is a trainable weight matrix and _**b**_ is a trainable bias.\n\n\n3.3.5.1 A CTIVATION AND B IAS\n\n\nAs the nonlinear activation function is not E( _n_ )-equivariant, nonlinear activation to\n\n\nrank- _p_ tensors cannot be applied, while one can apply any activation to rank-0 tensors. In",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 83,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 180,
        "char_count": 947,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "39d82c26-8a96-4cbb-b0ed-f935a7edfcf0",
      "content": "addition, adding bias is also not E( _n_ )-equivariant, so one cannot add bias when performing\n\n\nan affine transformation to rank- _p_ tensors. Again, one can add bias to rank-0 tensors.\n\n\nThus, for instance, if one converts from rank-0 tensors _**H**_ [(0)] to rank-1 tensors using\n\n\nIsoAM _**G**_, _**G**_ _∗_ _σ_ ( _**H**_ [(0)] _**W**_ + _**b**_ ) and ( _**G**_ _∗_ _σ_ ( _**H**_ [(0)] )) _**W**_ are E( _n_ )-equivariant functions,\n\nhowever ( _**G**_ _∗_ _**H**_ [(0)] ) _**W**_ + _**b**_ and _σ_ \u0001 ( _**G**_ _∗_ _σ_ ( _**H**_ [(0)] )) _**W**_ \u0002 are not due to the bias and the\n\n\nnonlinear activation, respectively. Likewise, regarding a conversion from rank-1 tensors\n\n\n_**H**_ [(1)] to rank-0 tensors, _σ_ \u0001 ( _**G**_ _⊙_ _**H**_ [(1)] ) _**W**_ + _**b**_ \u0002 and _σ_ \u0001 _**G**_ _⊙_ ( _**H**_ [(1)] _**W**_ ) \u0002 are E( _n_ )-invariant\n\nfunctions; however, _**G**_ _⊙_ ( _**H**_ [(1)] _**W**_ + _**b**_ ) and ( _**G**_ _⊙_ _σ_ ( _**H**_ [(1)] )) _**W**_ + _**b**_ are not.",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 83,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 171,
        "char_count": 974,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "46e319db-2786-4bd6-b911-ef1bb09d77ab",
      "content": "To convert rank- _p_ tensors to rank- _q_ tensors ( _q ≥_ 1), one can apply neither bias nor non\n\nlinear activation. To add nonlinearity to such a conversion, we can multiply the converted\n\n\nrank-0 tensors _σ_ (( [O] _[p]_ _**G**_ _⊙_ _**H**_ [(] _[p]_ [)] ) _**W**_ + _**b**_ ) with the input tensors _**H**_ [(] _[p]_ [)] or the output tensors\n\n\n_**H**_ [(] _[q]_ [)] .\n\n\n3.3.5.2 P REPROCESSING OF I NPUT F EATURE\n\n\nSimilarly to the discussion regarding the biases, we have to take care of the prepro\n\ncessing of rank- _p_ tensors to retain E( _n_ )-invariance because adding a constant array and\n\n\ncomponent-wise scaling could distort the tensors, resulting in broken E( _n_ )-equivariance.",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 83,
        "chunk_index": 2,
        "chunk_type": "text",
        "token_count": 119,
        "char_count": 693,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "de67d12a-8736-49f1-8683-0947ad9d0a1e",
      "content": "**60** 3. IsoGCN: E( _n_ )-Equivariant Graph Convolutional Network\n\n\nFor instance, _**H**_ [(] _[p]_ [)] _/_ Std all \u0003 _**H**_ [(] _[p]_ [)] [\u0004] is a valid transformation to retain E( _n_ )-equivariance,\n\nassuming Std all \u0003 _**H**_ [(] _[p]_ [)] [\u0004] _∈_ R is a standard deviation of all components of _**H**_ [(] _[p]_ [)] . However,\n\nconversions such as _**H**_ [(] _[p]_ [)] _/_ Std component \u0003 _**H**_ [(] _[p]_ [)] [\u0004] and _**H**_ [(] _[p]_ [)] _−_ Mean \u0003 _**H**_ [(] _[p]_ [)] [\u0004] are not E( _n_ )\nequivariant, assuming that Std component \u0003 _**H**_ [(] _[p]_ [)] [\u0004] _∈_ R _[d]_ _[p]_ is a component-wise standard de\n\nviation.\n\n\n3.3.5.3 S CALING\n\n\nBecause the differential IsoAM _**G**_ [˜] corresponds to the differential operator, the scale of\n\nthe output after operations regarding _D_ [˜] can be huge. Thus, we rescale _**G**_ [˜] using the scaling",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 84,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 148,
        "char_count": 857,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "116aea31-f724-46c0-a8f6-5e79b11f5200",
      "content": "1 _/_ 2\nfactor Mean sample _,i_ ( _G_ [˜] [2] _ii_ ;;1 [+ ˜] _[G]_ [2] _ii_ ;;2 [+ ˜] _[G]_ [2] _ii_ ;;3 [)], where Mean sample _,i_ denotes the mean over\ni j\n\nthe samples and vertices.\n\n\n3.3.5.4 T ENSOR R ANK\n\n\nAlthough we defined IsoGCN _p→q_ in Equations 3.42 and 3.43, there are other ways to\n\n\nmodel function converting from rank- _p_ discrete tensor field to rank- _q_ discrete tensor field.\n\n\nFor instance, in the case of _p_ = 2 and _q_ = 3, one may also define as:\n\n\n_**H**_ out [(3)] [= EquivariantPointwiseMLP] _**G**_ _⊗_ _**G**_ _⊙_ _**G**_ _⊗_ _**H**_ in [(2)] _,_ (3.66)\n\u0011 \u0012\n\n\nor\n\n\n_**H**_ out [(3)] [= IsoGCN] [4] _[→]_ [3] _[◦]_ [IsoGCN] [3] _[→]_ [4] _[◦]_ [IsoGCN] [2] _[→]_ [3] [(] _**[H]**_ in [(2)] [)] _[.]_ (3.67)\n\n\nOne guideline is to consider PDEs of interest when using the differential IsoAM, as done in\n\n\nthe numerical experiments (Section 3.4). In the other case, generally, tensor rank should not",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 84,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 167,
        "char_count": 927,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "a6074b53-274a-4524-b6d4-baa7c0ff7f97",
      "content": "be dropped unless necessary. Namely, transformation of the tensor rank 2 _→_ 3 _→_ 4 _→_ 3\n\n\nis more preferable compared to that of 2 _→_ 1 _→_ 0 _→_ 1 _→_ 2 _→_ 3 to constract the IsoGCN\n\n\nmodel which converts a rank-2 discrete tensor field to a rank-3 discrete tensor field.\n\n\n3.3.5.5 I MPLEMENTATION\n\n\nBecause an adjacency matrix _**A**_ is usually a sparse matrix for a regular mesh, _**A**_ ( _m_ )\n\n\nin equation 3.47 is also a sparse matrix for a sufficiently small _m_ . Thus, we can leverage",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 84,
        "chunk_index": 2,
        "chunk_type": "text",
        "token_count": 92,
        "char_count": 499,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "7b59722c-7f26-45b0-a4d4-c866e3322d9a",
      "content": "3.4. Numerical Experiments **61**\n\n\nsparse matrix multiplication in the IsoGCN computation. This is one major reason why\n\n\nIsoGCNs can compute rapidly. If the multiplication (tensor product or contraction) of\n\n\nIsoAMs must be computed multiple times the associative property of the IsoAM can be\n\n\nutilized.\n\n\nFor instance, it is apparent that\n\n\n\n_k_\nP _**G**_",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 85,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 53,
        "char_count": 359,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "4c35463b-ac15-41d7-af22-4431183df4c9",
      "content": "# $\n\n_∗_ _**H**_ [(0)] = _**G**_ _⊗_ ( _**G**_ _⊗_ _. . ._ ( _**G**_ _∗_ _**H**_ [(0)] )) _._ (3.68)\n\n\n\nAssuming that the number of nonzero elements in _**A**_ ( _m_ ) equals _n_ and _**H**_ [(0)] _∈_ R _[|V|×][f]_, then\n\n\nthe computational complexity of the right-hand side is _O_ ( _n|V|fn_ _[k]_ ). This is an exponential\n\n\norder regarding the spatial dimension _n_ . However, _n_ and _k_ are usually small numbers (typi\n\ncally _n_ = 3 and _k ≤_ 4). Therefore one can compute an IsoGCN layer with a realistic spatial\n\n\ndimension _n_ and tensor rank _k_ fast and memory efficiently. In our implementation, both a\n\n\nsparse matrix operation and associative property are utilized to realize fast computation.\n\n\n3.4 N UMERICAL E XPERIMENTS\n\n\nTo test the applicability of the proposed model, we composed the following two\n\n\ndatasets:\n\n\n1. a differential operator dataset of grid meshes; and\n\n\n2. an anisotropic nonlinear heat equation dataset of meshes generated from CAD data.",
      "chunk_metadata": {
        "section_title": "$",
        "section_level": 1,
        "page": 85,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 163,
        "char_count": 974,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "3094a078-edf1-41c0-bf29-5cf05f9189e0",
      "content": "In this section, we discuss our machine learning model, the definition of the problem, and\n\n\nthe results for each dataset.\n\n\nUsing _**G**_ [˜] defined in Section 3.3.4, we constructed a neural network model considering\n\n\nan encode-process-decode configuration (Battaglia et al., 2018). The encoder and decoder\n\n\nwere comprised of component-wise MLPs and tensor operations. For each task, we tested\n\n\n_m_ = 2 _,_ 5 in Equation 3.47 to investigate the effect of the number of hops considered.",
      "chunk_metadata": {
        "section_title": "$",
        "section_level": 1,
        "page": 85,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 76,
        "char_count": 490,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "ebfbf910-ae8f-4c4a-b204-55d4f962164d",
      "content": "**62** 3. IsoGCN: E( _n_ )-Equivariant Graph Convolutional Network\n\n\nIn addition to the GCN (Kipf & Welling, 2017), we chose GIN (Xu et al., 2018),\n\n\nSGCN (Wu et al., 2019), Cluster-GCN (Chiang et al., 2019), and GCNII (Chen et al.,\n\n\n2020) as GCN variant baseline models.\n\n\nFor the equivariant models, we chose the TFN (Thomas et al., 2018) and SE(3)\n\nTransformer (Fuchs et al., 2020) as the baseline. We implemented these models using\n\n\nPyTorch 1.6.0 (Paszke et al., 2019) and PyTorch Geometric 1.6.1 (Fey & Lenssen, 2019).\n\n\nFor both the TFN and SE(3)-Transformer, we used implementation of Fuchs et al. (2020) [3]\n\n\nbecause the computation of the TFN is considerably faster than the original implemen\n\ntation, as claimed in Fuchs et al. (2020). For each experiment, we minimized the mean\n\n\nsquared loss using the Adam optimizer (Kingma & Ba, 2014). The corresponding imple\n\nmentation and the dataset will be made available online.\n\n\n3.4.1 D IFFERENTIAL O PERATOR D ATASET",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 86,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 159,
        "char_count": 975,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "2f0bffff-4e39-490c-ab87-8ea392a13aa8",
      "content": "3.4.1.1 T ASK D EFINITION\n\n\nTo demonstrate the expressive power of IsoGCNs, we created a dataset to learn the\n\n\ndifferential operators. We first generated a pseudo-2D grid mesh randomly with only one\n\n\ncell in the _Z_ direction and 10 to 100 cells in the _X_ and _Y_ directions. We then generated\n\n\nscalar fields on the grid meshes and analytically calculated the gradient, Laplacian, and\n\n\nHessian fields. We generated 100 samples for each train, validation, and test dataset.\n\n\nFor simplicity, we set _w_ _ij_ = 1 in Equation 3.47 for all ( _i, j_ ) _∈E_ . To compare the\n\n\nperformance with the GCN models, we simply replaced an IsoGCN layer with a GCN or\n\n\nits variant layers while keeping the number of hops _m_ the same to enable a fair comparison.\n\n\nWe adjusted the hyperparameters for the equivariant models to ensure that the number of\n\n\nparameters in each was almost the same as that in the IsoGCN model.\n\n\nWe conducted the experiments using the following settings:",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 86,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 166,
        "char_count": 974,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "8b387ef4-23ad-4a55-a00e-eeb3dcd4bce0",
      "content": "1. inputting the scalar field _φ_ and predicting the gradient field _∇φ_ (rank-0 _→_ rank-1\n\n\ntensor);\n\n\n3 [https://github.com/FabianFuchsML/se3-transformer-public](https://github.com/FabianFuchsML/se3-transformer-public)",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 86,
        "chunk_index": 2,
        "chunk_type": "text",
        "token_count": 18,
        "char_count": 221,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "2711fd1a-518b-4a90-a2ad-e06db0680a8a",
      "content": "3.4. Numerical Experiments **63**\n\n\n2. inputting the scalar field _φ_ and predicting the Hessian field _∇⊗∇φ_ (rank-0 _→_\n\n\nrank-2 tensor);\n\n\n3. inputting the gradient field _∇φ_ and predicting the Laplacian field _∇· ∇φ_ (rank-1\n\n\n_→_ rank-0 tensor); and\n\n\n4. inputting the gradient field _∇φ_ and predicting the Hessian field _∇⊗∇φ_ (rank-1 _→_\n\n\nrank-2 tensor).\n\n\n3.4.1.2 M ODEL A RCHITECTURES\n\n\n\n( _a_ )\n\n\n( _b_ )\n\n\n( _c_ )\n\n\n( _d_ )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nブロック図の主要コンポーネントは「∇φ」「IsoGCN」「MLP」「∇²φ」です。信号の入出力関係は、∇φからLinear([1, 64], [Identity])を出力し、その出力からMLP([64,64],[tanh,tanh,Identity])に送り、MLPの出力はIsoGCn([6,6],[Identity])に入力され、再びMLPを経過した信号から∇₂φに送られ、最終的に∇φに戻る仕組みです。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nフローチャートの開始は「∇φ」から始まり、主な分岐は「MLP」と「IsoGCN G⊙−」で、処理の流れは「Linear [1,64] [Identity] → IsoGCN/G⊙− → MLP/tanh/tanh/Identity → Linear [64,1] → ∇φ ∘ ∘」と進みます。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEncoder\n\n\n\nProcess\n\n\n\nDecoder",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 87,
        "chunk_index": 0,
        "chunk_type": "formula",
        "token_count": 92,
        "char_count": 919,
        "contains_formulas": true,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "5b7eec11-ea5d-4ef5-98d3-d0fb15f3b8eb",
      "content": "Figure 3.2: The IsoGCN models used for (a) the scalar field to the gradient field, (b) the\n\n\nscalar field to the Hessian field, (c) the gradient field to the Laplacian field, (d) the gradient\n\n\nfield to the Hessian field of the gradient operator dataset. Gray boxes are trainable com\n\nponents. In each trainable cell, we put the number of units in each layer along with the\n\n\nactivation functions used. _∗⃝_ denotes the multiplication in the feature direction.",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 87,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 77,
        "char_count": 460,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "acf251c3-ae75-4b19-ac7a-9d4b1e9d0e1e",
      "content": "**64** 3. IsoGCN: E( _n_ )-Equivariant Graph Convolutional Network\n\n\nFigure 3.2 represents the IsoGCN model used for the differential operator dataset. We\n\n\nused the tanh activation function as a nonlinear activation function because we expect\n\n\nthe target temperature field to be smooth. Therefore, we avoid using non-differentiable\n\n\nactivation functions such as the rectified linear unit (ReLU) (Nair & Hinton, 2010). For\n\n\nGCN and its variants, we simply replaced the IsoGCN layers with the corresponding ones.\n\n\nWe stacked _m_ (= 2 _,_ 5) layers for GCN, GIN, GCNII, and Cluster-GCN. We used an _m_\n\n\nhop adjacency matrix for SGCN.\n\n\nFor the TFN and SE(3)-Transformer, we set the hyperparameters to have almost the\n\n\nsame number of parameters as in the IsoGCN model. The settings of the hyperparameters\n\n\nare shown in Table 3.2.\n\n\nTable 3.2: Summary of the hyperparameter setting for both the TFN and\n\n\nSE(3)-Transformer. For the parameters not in the table, we used the de",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 88,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 154,
        "char_count": 978,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "228f9a25-5919-41d8-a0cc-0fa7c804c4b3",
      "content": "[fault setting in the implementation of https://github.com/FabianFuchsML/](https://github.com/FabianFuchsML/se3-transformer-public)\n\n\n[se3-transformer-public.](https://github.com/FabianFuchsML/se3-transformer-public)\n\n\n**0** _**→**_ **1** **0** _**→**_ **2** **1** _**→**_ **0** **1** _**→**_ **2**",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 88,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 20,
        "char_count": 298,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "fb8c3e15-a536-4fe7-931e-3c4450cb336a",
      "content": "# hidden layers 1 1 1 1",
      "chunk_metadata": {
        "section_title": "hidden layers 1 1 1 1",
        "section_level": 1,
        "page": 88,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 7,
        "char_count": 23,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "e146522b-d0d3-4559-92dd-28643e86ec76",
      "content": "# NL layers in the self-interaction 1 1 1 1",
      "chunk_metadata": {
        "section_title": "NL layers in the self-interaction 1 1 1 1",
        "section_level": 1,
        "page": 88,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 10,
        "char_count": 43,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "7d44a2af-d406-4bf8-8e30-6b9fdafd3d95",
      "content": "# channels 24 20 24 24",
      "chunk_metadata": {
        "section_title": "channels 24 20 24 24",
        "section_level": 1,
        "page": 88,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 6,
        "char_count": 22,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "05cda6ad-f26c-45fb-967c-7caf3a531f35",
      "content": "# maximum rank of the hidden layers 1 2 1 2",
      "chunk_metadata": {
        "section_title": "maximum rank of the hidden layers 1 2 1 2",
        "section_level": 1,
        "page": 88,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 11,
        "char_count": 43,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "02f97339-eb47-4e90-ab5f-78ed2fe0e832",
      "content": "# nodes in the radial function 16 8 16 22\n\n3.4.1.3 R ESULTS\n\n\nFigure 3.3 and Table 3.3 present a visualization and comparison of predictive perfor\n\nmance, respectively. The results show that an IsoGCN outperforms other GCN models for\n\n\nall settings. This is because the IsoGCN model has information on the relative position of\n\n\nthe adjacency vertices, and thus understands the direction of the gradient, whereas the other\n\n\nGCN models cannot distinguish where the adjacencies are, making it nearly impossible to\n\n\npredict the gradient directions.",
      "chunk_metadata": {
        "section_title": "nodes in the radial function 16 8 16 22",
        "section_level": 1,
        "page": 88,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 85,
        "char_count": 547,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "b75ed76e-f92d-4de8-be9c-1174977d918d",
      "content": "ヒートマップの軸は縦横の格子線で構成され、緑色から黄色、橙色、赤色に色が変化しています。高域集中領域は左上隅に黄色と橙色が混在する部分、右下隅に赤色が集中的に分布しています。目立つパターンは、左下隅の黄色と赤色の混合区域が特に目立っています。\n\nヒートマップの軸は横軸と縦軸で、データの分布を示します。高濃度の領域は黄色・橙色に、低濃度は青色・緑色に表示されます。目立つパターンは、色の変化や形状の一致が特徴的です。\n\nヒートマップの軸は横軸と縦軸です。高域集中領域は緑・黄・橙色で、低域は青色です。目立つパターンは、左上角の緑黄橙色域と右下角の紫白色域が特徴的です。\n\n主要ノード：Cluster-GCN\nクラスタ：SE(3)-Tr\n強い接続：網膜状接続（網膜様接続）、星状接通（星様接通）、放射状接線（放射様接線）、網状接合（網様接合）\n\n\n\n3.4. Numerical Experiments **65**\n\n\n\nヒートマップの軸は「gradient Magnitude（梯度の大きさ）」で、0.0e+00から1.5e-01の範囲を示しています。色は赤から緑に変化し、高値域（0.1程度）は赤色で、低値領域は青色です。右上角の赤色部分が最大値の集中領域で、左下角の青色部分は最小値区域です。\n\nヒートマップの軸は横軸と縦軸です。高域集中領域は黄色・橙色、低域は緑・青です。目立つパターンは、左の図では不規則な緑色斑点、中央と右の団円状の色調変化が見られます。\n\n図はベクトル場の分布を示しています。矢印の大きさは場の強度を表し、右上に斜めの矢印が特徴的で、左下の領域では矢量が逆方向に方向づけられ、強度が弱いことがわかります。\n\n縦軸は「difference-gradient Magnitude（差分勾配の大きさ）」を示し、横軸には「gr（不明）」が記載されています。色条は正の値（赤色）から負の值（青色）へと変化し、最大値5.0e-02（0.05）を示しています。全体的に正の傾向が見られ、最大差分は右端の赤色部分に位置しています。\n\n左の図は「r-GCN」で生成されたベクトル場を示しています。矢印の大きさは場の強さを表し、右上に指向する傾向が特徴的で、中央に集中的な強度が見られ、周囲には弱い矢量が分布しています。",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 89,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 17,
        "char_count": 965,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "0b9b8af9-1db6-42d8-9a2e-673528aaec2e",
      "content": "ヒートマップの横軸は「difference-gradient Magnitude（差分勾配の大きさ）」を示し、右側の色条はこの値の範囲を示しています。上部のヒートmapでは、緑色と黄色の領域が目立っており、これらの色域内では勾配が大きいため、高集中領域に該当します。下部のIsoGCN（Ours）ヒートmapsでは、赤色と橙色の領域も目立っていますが、勾配はそれほど大きくないため、低集中領域と見なされています。\n\nFigure 3.3: (Top) the gradient field and (bottom) the error vector between the prediction\n\n\nand the ground truth of a test data sample. The error vectors are exaggerated by a factor of\n\n\n2 for clear visualization.\n\n\nAdding the vertex positions to the input feature to other GCN models exhibited a perfor\n\nmance improvement, however as the vertex position is not a translation invariant feature, it\n\n\ncould degrade the predictive performance of the models. Thus, we did not input _**x**_ as a ver\n\ntex feature to the IsoGCN model or other equivariant models to retain their E( _n_ )- invariant\n\n\nand equivariant natures.",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 89,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 104,
        "char_count": 827,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "61d5e321-ec74-4c55-8d44-6b2c9f63d6c9",
      "content": "IsoGCNs perform competitively against other equivariant models with shorter predic\ntion time as shown in Table 3.4. As mentioned in Section 3.3.4, _**G**_ [˜] corresponds to the\n\n\ngradient operator, which is now confirmed in practice. Therefore, it can be found out the\n\n\nproposed model has a strong expressive power to express differential regarding space with\n\n\nless computation resources compared to the TFN and SE(3)-Transformer.",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 89,
        "chunk_index": 2,
        "chunk_type": "text",
        "token_count": 64,
        "char_count": 433,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "789b0994-7c9d-44fd-a7c9-60beb404d45d",
      "content": "**66** 3. IsoGCN: E( _n_ )-Equivariant Graph Convolutional Network\n\n\nTable 3.3: Summary of the test losses (mean squared error _±_ the standard error of the\n\n\nmean in the original scale) of the differential operator dataset: 0 _→_ 1 (the scalar field to the\n\n\ngradient field), 0 _→_ 2 (the scalar field to the Hessian field), 1 _→_ 0 (the gradient field to the\n\n\nLaplacian field), and 1 _→_ 2 (the gradient field to the Hessian field). Here, if “ _**x**_ ” is “Yes”,\n\n\n_**x**_ is also in the input feature.\n\n\n**Loss of 0** _**→**_ **1** **Loss of 0** _**→**_ **2** **Loss of 1** _**→**_ **0** **Loss of 1** _**→**_ **2**\n**Method** **# hops** _**x**_\n_×_ 10 _[−]_ [5] _×_ 10 _[−]_ [6] _×_ 10 _[−]_ [6] _×_ 10 _[−]_ [6]\n\n\n2 No 151.19 _±_ 0.53 49.10 _±_ 0.36 542.52 _±_ 2.14 59.65 _±_ 0.46\n\n\n2 Yes 147.10 _±_ 0.51 47.56 _±_ 0.35 463.79 _±_ 2.08 50.73 _±_ 0.40\n\nGIN\n\n5 No 151.18 _±_ 0.53 48.99 _±_ 0.36 542.54 _±_ 2.14 59.64 _±_ 0.46\n\n\n5 Yes 147.07 _±_ 0.51 47.35 _±_ 0.35 404.92 _±_ 1.74 46.18 _±_ 0.39",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 90,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 187,
        "char_count": 1000,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "04172cde-608d-4087-8e62-fc9a27541768",
      "content": "2 No 151.18 _±_ 0.53 43.08 _±_ 0.31 542.74 _±_ 2.14 59.65 _±_ 0.46\n\n\n2 Yes 151.14 _±_ 0.53 40.72 _±_ 0.29 194.65 _±_ 1.00 45.43 _±_ 0.36\n\nGCNII\n\n5 No 151.11 _±_ 0.53 32.85 _±_ 0.23 542.65 _±_ 2.14 59.66 _±_ 0.46\n\n\n5 Yes 151.13 _±_ 0.53 31.87 _±_ 0.22 280.61 _±_ 1.30 39.38 _±_ 0.34\n\n\n2 No 151.17 _±_ 0.53 50.26 _±_ 0.38 542.90 _±_ 2.14 59.65 _±_ 0.46\n\n\n2 Yes 151.12 _±_ 0.53 49.96 _±_ 0.37 353.29 _±_ 1.49 59.61 _±_ 0.46\n\nSGCN\n\n5 No 151.12 _±_ 0.53 55.02 _±_ 0.42 542.73 _±_ 2.14 59.64 _±_ 0.46\n\n\n5 Yes 151.16 _±_ 0.53 55.08 _±_ 0.42 127.21 _±_ 0.63 56.97 _±_ 0.44\n\n\n2 No 151.23 _±_ 0.53 49.59 _±_ 0.37 542.54 _±_ 2.14 59.64 _±_ 0.46\n\n\n2 Yes 151.14 _±_ 0.53 47.91 _±_ 0.35 542.68 _±_ 2.14 59.60 _±_ 0.46\n\nGCN\n\n5 No 151.18 _±_ 0.53 50.58 _±_ 0.38 542.53 _±_ 2.14 59.64 _±_ 0.46\n\n\n5 Yes 151.14 _±_ 0.53 48.50 _±_ 0.35 542.30 _±_ 2.14 25.37 _±_ 0.28\n\n\n2 No 151.19 _±_ 0.53 33.39 _±_ 0.24 542.54 _±_ 2.14 59.66 _±_ 0.46\n\n\n2 Yes 147.23 _±_ 0.51 32.29 _±_ 0.24 167.73 _±_ 0.83 17.72 _±_ 0.17\n\nCluster-GCN",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 90,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 200,
        "char_count": 998,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "44b21e26-6a28-46b4-98fc-dc0629f773da",
      "content": "5 No 151.15 _±_ 0.53 28.79 _±_ 0.21 542.51 _±_ 2.14 59.66 _±_ 0.46\n\n\n5 Yes 146.91 _±_ 0.51 26.60 _±_ 0.19 185.21 _±_ 0.99 18.18 _±_ 0.20\n\n\n2 No 2.47 _±_ 0.02 OOM 26.69 _±_ 0.24 OOM\nTFN\n\n5 No OOM OOM OOM OOM\n\n\n2 No **1.79** _±_ 0.02 **3.50** _±_ 0.04 **2.52** _±_ 0.02 OOM\nSE(3)-Trans.\n\n5 No 2.12 _±_ 0.02 OOM 7.66 _±_ 0.05 OOM\n\n\n2 No 2.67 _±_ 0.02 6.37 _±_ 0.07 7.18 _±_ 0.06 **1.44** _±_ 0.02\n**IsoGCN** (Ours)\n\n5 No 14.19 _±_ 0.10 21.72 _±_ 0.25 34.09 _±_ 0.19 8.32 _±_ 0.09",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 90,
        "chunk_index": 2,
        "chunk_type": "text",
        "token_count": 98,
        "char_count": 476,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "55b7299b-1969-4f91-8479-1255a60d0603",
      "content": "3.4. Numerical Experiments **67**\n\n\nTable 3.4: Summary of the prediction time on the test dataset. 0 _→_ 1 corresponds to the\n\n\nscalar field to the gradient field, and 0 _→_ 2 corresponds to the scalar field to the Hessian\n\n\nfield. Each computation was run on the same GPU (NVIDIA Tesla V100 with 32 GiB\n\n\nmemory). OOM denotes the out-of-memory of the GPU.\n\n\n**0** _**→**_ **1** **0** _**→**_ **2**\n\n\n**Method** **# parameters** **Inference time [s]** **# parameters** **Inference time [s]**\n\n\nTFN 5264 3.8 5220 OOM\n\n\nSE(3)-Trans. 5392 4.0 5265 9.2\n\n\n**IsoGCN** (Ours) 4816 0.4 4816 0.7",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 91,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 95,
        "char_count": 586,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "1be8fbaa-3340-4ce7-b379-45f6027d47d8",
      "content": "**68** 3. IsoGCN: E( _n_ )-Equivariant Graph Convolutional Network\n\n\n3.4.2 A NISOTROPIC N ONLINEAR H EAT E QUATION D ATASET\n\n\n3.4.2.1 T ASK D EFINITION\n\n\nTo apply the proposed model to a real problem, we adopted the anisotropic nonlinear\n\n\nheat equation. We considered the task of predicting the time evolution of the temperature\n\n\nfield based on the initial temperature field, material property, and mesh geometry infor\n\nmation as inputs. We randomly selected 82 CAD shapes from the first 200 shapes of the\n\n\nABC dataset (Koch et al., 2019), generate first-order tetrahedral meshes using a mesh gen\n\nerator program, Gmsh (Geuzaine & Remacle, 2009), randomly set the initial temperature\n\n\nand anisotropic thermal conductivity, and finally conducted a finite element analysis (FEA)\n\n\nusing the FEA program FrontISTR [4] (Morita et al., 2016; Ihara et al., 2017).\n\n\nFor this task, we set\n\n\n_w_ _ij_ = _V_ _j_ [effective] _/V_ _i_ [effective] _,_ (3.69)",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 92,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 150,
        "char_count": 950,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "0a044ce5-d6fd-4079-8599-745da80c3b3d",
      "content": "where _V_ _i_ [effective] denotes the effective volume of the _i_ th vertex (Equation 3.74.) Similarly to\n\n\nthe differential operator dataset, we tested the number of hops _m_ = 2 _,_ 5. However because\n\n\nwe put four IsoAM operations in one model, the number of hops visible from the model is\n\n\n8 ( _m_ = 2) or 20 ( _m_ = 5). As is the case with the differential operator dataset, we replaced\n\n\nan IsoGCN layer accordingly for GCN or its variant models.\n\n\nIn the case of _k_ = 2, we reduced the number of parameters for each of the baseline\n\n\nequivariant models to fewer than the IsoGCN model because they exceeded the memory of\n\n\nthe GPU (NVIDIA Tesla V100 with 32 GiB memory) with the same number of parameters.\n\n\nIn the case of _k_ = 5, neither the TFN nor the SE(3)-Transformer fits into the memory of the\n\n\nGPU even with the number of parameters equal to 10. For more details about the dataset\n\n\nand the model, see Section 3.4.2.",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 92,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 171,
        "char_count": 934,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "e16280c0-e555-407e-9549-05d62ce421a4",
      "content": "4 [https://github.com/FrontISTR/FrontISTR. We applied a private update to FrontISTR to](https://github.com/FrontISTR/FrontISTR)\ndeal with the anisotropic heat problem, which will be also made available online.",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 92,
        "chunk_index": 2,
        "chunk_type": "text",
        "token_count": 23,
        "char_count": 209,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "032c7058-365f-44ba-8e9f-21cd585dc61b",
      "content": "3.4. Numerical Experiments **69**\n\n\n3.4.2.2 D ATASET\n\n\nThe purpose of the experiment was to solve the anisotropic nonlinear heat diffusion\n\n\nunder an adiabatic boundary condition. The governing equation is defined as follows:\n\n\nΩ _⊂_ R [3] (3.70)\n\n\n_∂T_ ( _t,_ _**x**_ )\n\n= _∇·_ _**C**_ ( _T_ ( _t,_ _**x**_ )) _∇T_ ( _t,_ _**x**_ ) in Ω (3.71)\n_∂t_\n\n\n_T_ ( _t_ = 0 _,_ _**x**_ ) = _T_ 0 _._ 0 ( _**x**_ ) in Ω (3.72)\n\n\n_∇T_ ( _t,_ _**x**_ ) _|_ _**x**_ = _**x**_ _b_ _·_ _**n**_ ( _**x**_ _b_ ) = 0 on _∂_ Ω _,_ (3.73)\n\n\nwhere _T_ is the temperature field, _T_ 0 _._ 0 is the initial temperature field, _**C**_ _∈_ R _[d][×][d]_ is an\n\n\nanisotropic diffusion tensor and _**n**_ ( _**x**_ _b_ ) is the normal vector at _**x**_ _b_ _∈_ _∂_ Ω. The Neumann\n\n\nboundary condition expressed in Equation 3.73 corresponds to the adiabatic condition.\n\n\nHere, _**C**_ depends on temperature thus the equation is nonlinear. We randomly generate",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 93,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 168,
        "char_count": 933,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "e6067054-39a8-4898-aa07-91f1a5a96ab5",
      "content": "_**C**_ ( _T_ = _−_ 1) for it to be a positive semidefinite symmetric tensor with eigenvalues varying\n\n\nfrom 0.0 to 0.02. Then, we defined the linear temperature dependency the slope of which\n\n\nis _−_ _**C**_ ( _T_ = _−_ 1) _/_ 4. The function of the anisotropic diffusion tensor is uniform for each\n\n\nsample.\n\n\nThe task is defined to predict the temperature field at _t_ = 0 _._ 2 _,_ 0 _._ 4 _,_ 0 _._ 6 _,_ 1 _._ 0\n\n\n( _T_ 0 _._ 2 _, T_ 0 _._ 4 _, T_ 0 _._ 6 _, T_ 0 _._ 8 _, T_ 1 _._ 0 ) from the given initial temperature field _T_ 0 _._ 9, material property,\n\n\nand mesh geometry. However, the performance is evaluated only with _T_ 1 _._ 0 to focus on\n\n\nthe predictive performance. We inserted other output features to stabilize the trainings.\n\n\nAccordingly, the diffusion number of this problem is _**C**_ ∆ _t/_ (∆ _x_ ) [2] _≃_ 10 _._ 0 [4] assuming\n\n\n∆ _x ≃_ 10 _._ 0 _[−]_ [3] .\n\n\nFigure 3.4 represents the process of generating the dataset. We generated up to 9 FEA",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 93,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 194,
        "char_count": 977,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "4bbf7770-5ca2-4e04-9afc-fa0fe51d7eb0",
      "content": "results for each CAD shape. To avoid data leakage in terms of the CAD shapes, we first\n\n\nsplit them into training, validation, and test datasets, and then applied the following process.\n\n\nUsing one CAD shape, we generated up to three meshes using clscale (a control pa\n\nrameter of the mesh characteristic lengths) = 0.20, 0.25, and 0.30. To facilitate the training\n\n\nprocess, we scaled the meshes to fit into a cube with an edge length equal to 1.",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 93,
        "chunk_index": 2,
        "chunk_type": "text",
        "token_count": 78,
        "char_count": 447,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "6f82a1b0-9a99-4dfe-a0d1-6271081be5a9",
      "content": "**70** 3. IsoGCN: E( _n_ )-Equivariant Graph Convolutional Network\n\n\nUsing one mesh, we generated three initial conditions randomly using a Fourier series\n\n\nof the 2nd to 10th orders. We then applied an FEA to each initial condition and material\n\n\nproperty determined randomly as described above. We applied an implicit method to solve\n\n\ntime evolutions and a direct method to solve the linear equations. The FEA time step ∆ _t_\n\n\nwas set to 0.01.\n\n\nDuring this process, some of the meshes or FEA results may not have been available\n\n\ndue to excessive computation time or non-convergence. Therefore, the size of the dataset\n\n\nwas not exactly equal to the number multiplied by 9. Finally, we obtained 439 FEA results\n\n\nfor the training dataset, 143 FEA results for the validation dataset, and 140 FEA results for\n\n\n\n図はCAD（Computer-Aided Design）で作成された試験データセットの機械図を示し、上部の半円形と下部の矩形状が互いに接合する構造を主な要素とし、灰色と白色の塗り分けで内部構造を示しています。",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 94,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 135,
        "char_count": 919,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "019f32e3-f7e5-4354-9ae9-6a90fcbff102",
      "content": "この図は、CAD（Computer-Aided Design，コンピュータアシスタント設計）からFEA（Finite Element Analysis，フinit element解析）へのプロセスを示しています。左側はCADで作成された曲面形状の図示と、その図示を細かく分割した「mesh（マッシュ）」を示します。右側は、初期条件と材料特性を設定した後、FEAによる温度分布の結果を示した図です。図の下に温度の範囲（-1.0e+00 ～ 1.0E+00）を示し、色の変化は温度の高低を表しています。左から右へと進むと、meshの粒度（clscale）が0.30から0.20に減少し、FEA結果の温度分布の精度が向上する様子が示されています。\n\n\n\n\n\n画像は機械構造の変化を示す図です。左から右に進むと、構造の形状が徐々に変化し、中央の図では「clscale = 0.30」という値が示されています。右端の図は表面の温度分布を表す彩色図で、緑色は高温、黄色は中温、灰色は低温を示しています。\n\n画像は物理的性質の変化を示す3D図示で、左側の図形から右側に移動する方向に性質が変化していることを示しています。背景色の変化は性質の高低を表し、右側の形状は左側の状態から変化した状態を示しています。\n\n画像は材料特性の変化を示すブロック図で、灰色の立方体から彩色表面が変化し、上向きの矢印で「Material property（材料特性）」を示す。右側の彩色表面は材料特性が変化した状態を表しています。\n\n画像は物体の形状変化を示すプロセスを表しています。最初に灰色の立方体状物体が示され、次にその物体の表面に彩色マップが追加され、最後に物体の形状が変化した彩色表面が示されています。彩色マップは物体表面の特性を表し、色の変化は表面の特性の変化を反映しています。\n\n図は物体の形状を示す3D図と、同一物体の表面温度分布を示す彩色曲面図を示し、左図の「clscale = 0.25」が右図の温度分布に影響する関係を示しています。左図から右図への矢印は、物体の形状から表面温度分布への変換を表しています。",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 94,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 17,
        "char_count": 897,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "4b43846a-6b67-4a44-87da-c0d2a2d29a58",
      "content": "画像は流体シミュレーションの結果を示す3D図で、左から右に進むと流体の温度分布が変化する様子を示しています。色の変化は温度の高低を表し、右側の図では上部が高温（赤色）に、下部が低温（青色）に分布しています。\n\n図は機械構造の変形過程を示しています。左側の灰色構造体から右側の彩色曲面構造体へと変形が進行し、色の変化は応力の分布を表しています。右側の構造体は左側の状態に比べて形状が変化しています。\n\nこの画像は、物体の形状変形を示す技術的な図です。左の図は原始の形状を表し、右の図ではその形状が変形した状態を示しています。色の変化は変形の程度を示すもので、左から右に進むにつれて変形が進行しています。\n\n図は機械構造のスケーリング関係を示す図で、左側の図は「clscale = 0.20」の状態を表し、右側の彩色曲面図は同一スケーラビリティ条件下での形状変化を示しています。\n\nFigure 3.4: The process of generating the dataset. A smaller clscale parameter generates\n\n\nsmaller meshes.",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 94,
        "chunk_index": 2,
        "chunk_type": "text",
        "token_count": 21,
        "char_count": 495,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "32f56367-7e3b-4791-a575-09ddf0ba4451",
      "content": "3.4. Numerical Experiments **71**\n\n\n3.4.2.3 I NPUT AND O UTPUT F EATURES\n\n\nTo express the geometry information, we extracted the effective volume of the _i_ th vertex\n\n\n_V_ _i_ [effective] and the mean volume of the _i_ th vertex _V_ _i_ [mean], which are defined as follows:\n\n\n\n_V_ _i_ [effective] = Y\n\n_e∈N_ _i_ _[e]_\n\n\n\n1\n(3.74)\n4 _[V]_ _[e]_\n\n\n\n_V_ _i_ [mean] =\n\n\n\nQ _e_ _|N_ _∈Ni_ _[e]_ _i_ _[e]_ _[|]_ _[V]_ _[e]_ _,_ (3.75)\n\n\n\nwhere _N_ _i_ _[e]_ [is the set of elements, including the] _[ i]_ [th vertex.]\n\n\nFor GCN or its variant models, we tested several combinations of input vertex features\n\n\n_T_ 0 _._ 0, _**C**_, _V_ [effective], _V_ [mean], and _**x**_ (Table 3.6). For the IsoGCN model, inputs were _T_ 0 _._ 0,\n\n\n_V_ [effective], _V_ [mean], and _**C**_ . Since we construct define the discrete tensor field for each tensor\n\n\nrank, we have\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n_**H**_ [(0)]\nin [=]\n\n\n_**H**_ [(2)]\nin [=]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 95,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 196,
        "char_count": 991,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "0a8d21df-e254-48dc-a4b0-b85d66c72e63",
      "content": "_**C**_\n\n\n_**C**_\n\n\n...\n\n\n_**C**_\n\n\n\n_T_ 0 _._ 0 ( _**x**_ 1 ) _V_ [effective] ( _**x**_ 1 ) _V_ mean ( _**x**_ 1 )\n\n\n_T_ 0 _._ 0 ( _**x**_ 2 ) _V_ [effective] ( _**x**_ 2 ) _V_ mean ( _**x**_ 2 )\n\n\n... ... ...\n\n\n\n_∈_ R _[|V|×]_ [3] _[×][n]_ [0] (3.76)\n\n\n\n_T_ 0 _._ 0 ( _**x**_ _|V|_ ) _V_ [effective] ( _**x**_ _|V|_ ) _V_ mean ( _**x**_ _|V|_ )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n_|V|_ rows _∈_ R _[|V|×]_ [1] _[×][n]_ [2] _,_ (3.77)",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 95,
        "chunk_index": 1,
        "chunk_type": "table",
        "token_count": 123,
        "char_count": 492,
        "contains_formulas": false,
        "contains_tables": true,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "a0da266b-57c6-4742-b4d2-48cab72a2182",
      "content": "**72** 3. IsoGCN: E( _n_ )-Equivariant Graph Convolutional Network\n\n\nfor input discrete tensor fields and\n\n\n\n_∈_ R _[|V|×]_ [5] _[×][n]_ [0]\n\n\n(3.78)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n_**H**_ out [(0)] [=]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n_T_ 0 _._ 2 ( _**x**_ 1 ) _T_ 0 _._ 4 ( _**x**_ 1 ) _T_ 0 _._ 6 ( _**x**_ 1 ) _T_ 0 _._ 8 ( _**x**_ 1 ) _T_ 1 _._ 0 ( _**x**_ 1 )\n\n\n_T_ 0 _._ 2 ( _**x**_ 2 ) _T_ 0 _._ 4 ( _**x**_ 2 ) _T_ 0 _._ 6 ( _**x**_ 2 ) _T_ 0 _._ 8 ( _**x**_ 2 ) _T_ 1 _._ 0 ( _**x**_ 2 )\n\n\n... ... ... ... ...\n\n\n\n_T_ 0 _._ 2 ( _**x**_ _|V|_ ) _T_ 0 _._ 4 ( _**x**_ _|V|_ ) _T_ 0 _._ 6 ( _**x**_ _|V|_ ) _T_ 0 _._ 8 ( _**x**_ _|V|_ ) _T_ 1 _._ 0 ( _**x**_ _|V|_ )\n\n\n\nfor the output discrete tensor field in the present task.\n\n\n3.4.2.4 M ODEL A RCHITECTURES\n\n\n\nフローチャートの開始は「T₀.₀」から始まり、終了は「C」へと進みます。主な分岐は「MLP」と「IsoGCN」で、MLPは「Identity」「tanh」を含む多層パーソナルニューラルネットワークを経て「Linear」に到達し、「Identity」を出力する処理が主な流れです。",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 96,
        "chunk_index": 0,
        "chunk_type": "table",
        "token_count": 193,
        "char_count": 910,
        "contains_formulas": false,
        "contains_tables": true,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "f099f0ee-794d-433a-a671-2fe507b30251",
      "content": "図はMLP（多層パーソナルニューラルネットワーク）のブロック図を示しています。MLPの入力層は512個、隠れ層は2層で、各層の活性化関数はtanhで、最後の層はIdentity（線形）を含んでいます。出力はT₀.₂からT₁.₀までの5個の値になります。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPropagation 1 Propagation 2\n\n\nEncoder Process Decoder\n\n\n\nFigure 3.5: The IsoGCN model used for the anisotropic nonlinear heat equation dataset.\n\n\nGray boxes are trainable components. In each trainable cell, we put the number of units in\n\n\neach layer along with the activation functions used. Below the unit numbers, the activation\n\n\nfunction used for each layer is also shown. _∗⃝_ denotes the multiplication in the feature\n\n\ndirection, _⊙_ denotes the contraction, and _⊕_ denotes the addition in the feature direction.\n\n\nFigure 3.5 represents the IsoGCN model used for the anisotropic nonlinear heat equa\n\ntion dataset. We adopted the encode-process-decode configuration (Battaglia et al., 2018)\n\n\nto leverage the expressive power of neural networks. The encoder embeds the input fea",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 96,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 118,
        "char_count": 953,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "8c55d634-0e6f-4921-9303-0915a7a27497",
      "content": "tures to a higher dimensional space, 512 dimension in the present case. By increasing the",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 96,
        "chunk_index": 2,
        "chunk_type": "text",
        "token_count": 15,
        "char_count": 89,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "3e000dce-a054-4b44-b34b-904018b7aa30",
      "content": "3.4. Numerical Experiments **73**\n\n\ndimension of the encoded space, one can expect that the expressive power increases. The\n\n\ndecoder takes the embedded features and outputs features in the desired dimension.\n\n\nThe process part contains two propagation blocks. Although the propagation block\n\n\nlooks complicated, one can see it corresponds to the explicit Euler method (Equation 2.58):\n\n\n_T_ ( _t_ + ∆ _t,_ _**x**_ ) _≈_ _T_ ( _t,_ _**x**_ ) + _∇·_ _**C**_ ( _T_ ( _t,_ _**x**_ )) _∇T_ ( _t,_ _**x**_ )∆ _t,_ (3.79)\n\n\nbecause one propagation block is expressed as\n\n\nPropagation _i_ ( _**H**_ [(0)] _,_ _**H**_ [(2)] ) = _**H**_ [(0)] + _**G**_ [˜] _⊙_ _**H**_ [(2)] _⊙_ MLP( _**H**_ [(0)] ) _**G**_ [˜] _∗_ _**H**_ [(0)] _,_ (3.80)\n\n\nwhere _**H**_ [(0)] and _**H**_ [(2)] denotes the rank-0 and rank-2 tensor inputs to the considered prop\n\nagation block ( _i_ = 1 _,_ 2). Thus, one propagation block proceeds time ∆ _t_ because of the",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 97,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 158,
        "char_count": 934,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "80fbe379-7de1-4cdf-b7a7-c338a5b8da08",
      "content": "relationship to the Euler method. By stacking this propagation block _r_ times, we can make\n\n\ntime evolution by _r_ ∆ _t_, making it possible to predict the state after the long time. How\n\never, increasing _r_ may cause longer computation time. Therefore, we keep _r_ = 2 for the\n\n\nexperiment to retain computational efficiency.\n\n\nFor the nonlinear activation function, we used tanh because we expect the target tem\n\nperature field to be smooth. Therefore, we avoid using non-differentiable activation func\n\ntions such as the rectified linear unit (ReLU) (Nair & Hinton, 2010).\n\n\nFor GCN and its variants, we simply replaced the IsoGCN layers with the correspond\n\ning ones. We stacked _m_ (= 2 _,_ 5) layers for GCN, GIN, GCNII, and Cluster-GCN. We used\n\n\nan _m_ hop adjacency matrix for SGCN.\n\n\nFor the TFN and SE(3)-Transformer, we set the hyperparameters to as many parameters\n\n\nas possible that would fit on the GPU because the TFN and SE(3)-Transformer with almost",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 97,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 159,
        "char_count": 969,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "4bccef00-934d-45b4-ac13-eff6daa8e8ee",
      "content": "the same number of parameters as in IsoGCN did not fit on the GPU we used (NVIDIA\n\n\nTesla V100 with 32 GiB memory). The settings of the hyperparameters are shown in\n\n\nTable 3.5.",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 97,
        "chunk_index": 2,
        "chunk_type": "text",
        "token_count": 33,
        "char_count": 177,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "b4db5bdf-0bda-4492-ad80-a54883c1a3ba",
      "content": "**74** 3. IsoGCN: E( _n_ )-Equivariant Graph Convolutional Network\n\n\nTable 3.5: Summary of the hyperparameter setting for both the TFN and SE(3)\n\nTransformer. For the parameters not written in the table, we used the de\n\n[fault setting in the implementation of https://github.com/FabianFuchsML/](https://github.com/FabianFuchsML/se3-transformer-public)\n\n\n[se3-transformer-public.](https://github.com/FabianFuchsML/se3-transformer-public)",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 98,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 43,
        "char_count": 436,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "ee3b72e7-72e5-4478-a7f1-47f862e69600",
      "content": "# hidden layers 1",
      "chunk_metadata": {
        "section_title": "hidden layers 1",
        "section_level": 1,
        "page": 98,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 4,
        "char_count": 17,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "30511198-c649-4933-9c83-5ee393ced626",
      "content": "# NL layers in the self-interaction 1",
      "chunk_metadata": {
        "section_title": "NL layers in the self-interaction 1",
        "section_level": 1,
        "page": 98,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 7,
        "char_count": 37,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "cc3536a7-2d23-48cd-8969-1594c35148d6",
      "content": "# channels 16",
      "chunk_metadata": {
        "section_title": "channels 16",
        "section_level": 1,
        "page": 98,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 3,
        "char_count": 13,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "45e5d4a9-d1bf-47c2-ae4b-4d97b4456b91",
      "content": "# maximum rank of the hidden layers 2",
      "chunk_metadata": {
        "section_title": "maximum rank of the hidden layers 2",
        "section_level": 1,
        "page": 98,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 8,
        "char_count": 37,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "0a137d88-c179-4ed7-9ae7-fbf8575e1392",
      "content": "# nodes in the radial function 32\n\n3.4.2.5 R ESULTS\n\n\nFigure 3.6 and Table 3.6 present the results of the qualitative and quantitative compar\n\nisons for the test dataset. The IsoGCN demonstrably outperforms all other baseline models.\n\n\nMoreover, owing to the computationally efficient E( _n_ )-invariant nature of IsoGCNs, it also\n\n\nachieved a high prediction performance for the meshes that had a significantly larger graph\n\n\nthan those considered in the training dataset. The IsoGCN can scale up to 1M vertices,\n\n\nwhich is practical and is considerably greater than that reported in Sanchez-Gonzalez et al.\n\n\n(2020). Therefore, we conclude that IsoGCN models can be trained on relatively smaller\n\n\nmeshes [5] to save the training time and then used to apply the inference to larger meshes\n\n\nwithout observing significant performance deterioration.\n\n\nTable 3.7 reports the preprocessing and inference computation time using the equivari",
      "chunk_metadata": {
        "section_title": "nodes in the radial function 32",
        "section_level": 1,
        "page": 98,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 141,
        "char_count": 937,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "98dc0cb3-8265-4e89-b8ee-42dc036ddb6d",
      "content": "ant models with _m_ = 2 as the number of hops and FEA using FrontISTR 5.0.0. We varied\n\n\nthe time step (∆ _t_ = 1 _._ 0 _,_ 0 _._ 5) for the FEA computation to compute the _t_ = 1 _._ 0 time evo\n\nlution thus, resulting in different computation times and errors compared to an FEA with\n\n\n∆ _t_ = 0 _._ 01, which was considered as the ground truth. Clearly, the IsoGCN is 3- to 5- times\n\n\nfaster than the FEA with the same level of accuracy, while other equivariant models have\n\n\nalmost the same speed as FrontISTR with ∆ _t_ = 0 _._ 5.\n\n\nThe results show that the inclusion of _**x**_ in the input features of the baseline models did\n\n\nnot improve the performance. In addition, if _**x**_ is included in the input features, a loss of the\n\n\n5 However, it should also be sufficiently large to express sample shapes and fields.",
      "chunk_metadata": {
        "section_title": "nodes in the radial function 32",
        "section_level": 1,
        "page": 98,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 157,
        "char_count": 823,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "790ea508-b8d3-4058-9d00-8c2681f8e7fd",
      "content": "図面上に「Ground truth」と書かれた2つの円筒状物体が示されています。物体表面に網状の格子が描かれており、色は緑、黄、橙、緑の順に変化しています。物体の上部に矩形の穴が開いています。\n\n等高線/等値線図は、特定の物理量（例：温度、圧力）の分布を表します。軸は物理量の単位を示し、等高線上の値は同一物理量を示します。高低の等価線は高値と低値の区別を表し、重要な領域は等価線上で物理量が急激に変化する部分を指します。\n\n図面上に「Ground truth」と書かれた彩色曲面が示され、その下に灰色の曲面が描かれており、両者の関係を示しています。地上の真の状態（Ground truth）を示す彩色曲面と、その状態を推定した灰色曲面が対比されています。\n\n\n\n3.4. Numerical Experiments **75**\n\n等高線/等値線図は、特定の物理量（例：温度、圧力）の分布を表します。軸は物理量の単位を示し、等高線上の値は同一物理量を示します。高低の等価線は高値と低値の区別を表し、重要な領域は等価線上で物理量が急激に変化する部分を指します。\n\nヒートマップの軸は「TEMPERATURE（温度）」です。高温度域は赤色、低温度域が青色に表れています。中央の矩形区域内は温度が最も高いため、赤色が強調され、周囲は青色で低温度を示しています。\n\n\nSGCN SE(3) Trans. IsoGCN (Ours)\n\n\n\n図面上に「SGCN」と「SE(3) Tra」の名称が示され、両方の図は相似した円筒状物体を示しています。左の図（SGCN）は青色と茶色の色調で構成され、右の図「(SE(3)) Tra」は右半分が茶色、左半分が青色に色調が異なります。背景には「and truth」の文字が表示されています。\n\n画像は3種類の柱状物体を示しており、左から「CN」「SE(3) Trans」「IsoGCN (CN)」と名称が付けられています。各物体は網状の表面構造を有し、中央に矩形孔が開いています。物体の色は主に青色と橙色の混合色で構成され、色の分布は物体の形状や孔の位置に応じて変化しています。",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 99,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 24,
        "char_count": 904,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "acb4e18d-ad9a-48d9-b29a-bb793a098080",
      "content": "ヒートマップの軸は「Inference - Ground Truth」で、色は推定値と真値の差異を示します。高濃度の赤色領域は推測が高値に近い場所を示し、低濃度（青色）は低値を推測している場所です。中央に集中的な青色領域が真值と推定の差が最小であることを示しています。\n\nFigure 3.6: (Top) the temperature field of the ground truth and inference results and (bot\n\ntom) the error between the prediction and the ground truth of a test data sample. The error\n\n\nis exaggerated by a factor of 2 for clear visualization.\n\n\ngeneralization capacity for larger shapes compared to the training dataset may result as it\n\n\nextrapolates. The proposed model achieved the best performance compared to the baseline\n\n\nmodels considered. Therefore, we concluded that the essential features regarding the mesh\n\nshapes are included in _**G**_ [˜] .\n\n\nBesides, IsoGCN can scale up to meshes with 1M vertices as shown in Figure 3.7.\n\n\nThe result is surprizing because we trained relatively smaller meshes with several thousand\n\n\nvertices. Since IsoGCN successfully includes the information of the PDE, it can show such\n\n\na high generalizability.",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 99,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 136,
        "char_count": 987,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "d99e2777-980d-4fcf-b406-e6ff9c4705d1",
      "content": "**76** 3. IsoGCN: E( _n_ )-Equivariant Graph Convolutional Network\n\n\n\nヒートマップの軸は「TEMPERATURE（温度）」で、色は温度の高低を示します。高温域は赤色、低温域は青色です。図中央に「IsoGCN（Ours）」と「Ground Truth（FEA）」の名称が表示され、図の右下に拡大した部分が注目点です。\n\nヒートマップの軸は「温度」で、色は温度の高低を示します。高温度域は赤・橙色、低域は緑・青色です。中央の「Ground Truth (FEA)」と右の「IsoGCN (Ours)」のヒートグラフに、温度分布のパターンが見られます。\n\n\n\nヒートマップの軸は「色の深浅」を示します。高域は赤色、低域は青色です。中心部に緑色が集まり、周囲に黄色と赤色が分布しています。右上角に「IsoGCN (Ours)」と「Ground Truth (FEA)」のラベルがあります。\n\nヒートマップの軸は左から右に進む方向に設定されています。左側のサンプルでは、緑色と黄色の領域が密集しており、右側のIsoGCN推論結果では、赤色と橙色の高集中領域が見られ、両者のパターンが異なることが示されています。\n\nヒートマップの軸は左上から右下に進む方向に示されています。高域集中領域は黄色から赤色に移行し、低域は青色から緑色へと移行しています。右上角に「Training samples」の黒い点が存在し、その周囲の色は明るめの黄色と緑です。右側には「IsoGCN inference result」と書かれた白色の文字と、右上に「Figure 3.7: Comparison between (left) samples in the training, computed through FEA, and (right) IsoGCN inferenc...」と書かれている白色のテキストがあります。\n\nヒートマップの軸は横軸と縦軸です。高域集中領域は黄色・橙色に、低域は緑・青色に分布しています。右上角に目立つ赤色斑点が特徴的です。\n\nfield for a mesh, which is much larger than these in the training dataset.",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 100,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 54,
        "char_count": 976,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "61610cbf-61d3-4773-b9d2-0e14551b18cd",
      "content": "3.4. Numerical Experiments **77**\n\n\nTable 3.6: Summary of the test losses (mean squared error _±_ the standard error of the mean\n\n\nin the original scale) of the anisotropic nonlinear heat dataset. Here, if “ _**x**_ ” is “Yes”, _**x**_ is\n\n\nalso in the input feature. OOM denotes the out-of-memory on the applied GPU (32 GiB).\n\n\n**Loss**\n**Method** **# hops** _**x**_\n_×_ 10 _[−]_ [3]\n\n\n2 No 16.921 _±_ 0.040\n\n\n2 Yes 18.483 _±_ 0.025\n\nGIN\n\n5 No 22.961 _±_ 0.056\n\n\n5 Yes 17.637 _±_ 0.046\n\n\n2 No 10.427 _±_ 0.028\n\n\n2 Yes 11.610 _±_ 0.032\n\nGCN\n\n5 No 12.139 _±_ 0.031\n\n\n5 Yes 11.404 _±_ 0.032\n\n\n2 No 9.595 _±_ 0.026\n\n\n2 Yes 9.789 _±_ 0.028\n\nGCNII\n\n5 No 8.377 _±_ 0.024\n\n\n5 Yes 9.172 _±_ 0.028\n\n\n2 No 7.266 _±_ 0.021\n\n\n2 Yes 8.532 _±_ 0.023\n\nCluster-GCN\n\n5 No 8.680 _±_ 0.024\n\n\n5 Yes 10.712 _±_ 0.030\n\n\n2 No 7.317 _±_ 0.021\n\n\n2 Yes 9.083 _±_ 0.026\n\nSGCN\n\n5 No 6.426 _±_ 0.018\n\n\n5 Yes 6.519 _±_ 0.020\n\n\n2 No 15.661 _±_ 0.019\nTFN\n\n5 No OOM\n\n\n2 No 14.164 _±_ 0.018\nSE(3)-Trans.\n\n5 No OOM",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 101,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 187,
        "char_count": 979,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "9c04f47b-7e40-40d4-8a0b-9cabefaf3c0a",
      "content": "2 No 4.674 _±_ 0.014\n**IsoGCN** (Ours)\n\n5 No **2.470** _±_ 0.008",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 101,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 12,
        "char_count": 64,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "0974defe-1dee-48d3-8011-798974396436",
      "content": "**78** 3. IsoGCN: E( _n_ )-Equivariant Graph Convolutional Network\n\n\nTable 3.7: Comparison of computation time. To generate the test data, we sampled CAD\n\n\ndata from the test dataset and then generated the mesh for the graph to expand while re\n\ntaining the element volume at almost the same size. The initial temperature field and the\n\n\nmaterial properties are set randomly using the same methodology as the dataset sample\n\n\ngeneration. For a fair comparison, each computation was run on the same CPU (Intel Xeon\n\n\nE5-2695 v2@2.40GHz) using one core, and we excluded file I/O time from the measured\n\n\ntime. OOM denotes the out-of-memory (500 GiB).\n\n\n_**|V|**_ **= 21** _**,**_ **289** _**|V|**_ **= 155** _**,**_ **019** _**|V|**_ **= 1** _**,**_ **011** _**,**_ **301**\n\n\n**Loss** **Loss** **Loss**\n**Method** **Time [s]** **Time [s]** **Time [s]**\n_×_ 10 _[−]_ [4] _×_ 10 _[−]_ [4] _×_ 10 _[−]_ [4]\n\n\nFrontISTR (∆ _t_ = 1 _._ 0) 10.9 16.7 6.1 181.7 2.9 1656.5",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 102,
        "chunk_index": 0,
        "chunk_type": "table",
        "token_count": 156,
        "char_count": 961,
        "contains_formulas": false,
        "contains_tables": true,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "34bc74af-482e-4a27-98cc-aaf381187c19",
      "content": "FrontISTR (∆ _t_ = 0 _._ 5) 0.8 30.5 0.4 288.0 0.2 2884.2\n\n\nTFN 77.9 46.1 30.1 400.9 OOM OOM\n\n\nSE(3)-Transformer 111.4 31.2 80.3 271.1 OOM OOM\n\n\n**IsoGCN** (Ours) 8.1 **7.4** 4.9 **84.1** 3.9 **648.4**",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 102,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 35,
        "char_count": 201,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "ff3f1b2e-e7bb-48cd-a04f-f87a4e9a855d",
      "content": "3.5. Conclusion **79**\n\n\n3.5 C ONCLUSION\n\n\nIn this chapter, we introduced the GCN-based E( _n_ )- invariant and equivariant mod\n\nels called IsoGCN. We discussed the differential IsoAM, an isometric adjacency matrix\n\n\n(IsoAM) for numerical analysis, that was closely related to the essential differential opera\n\ntors. The experiment results confirmed that the proposed model leveraged the spatial struc\n\ntures and can deal with large-scale graphs. The computation time of the IsoGCN model is\n\n\nsignificantly shorter than the FEA, which other equivariant models cannot achieve. There\n\nfore, IsoGCN must be the first choice to learn physical simulations because of its compu\n\ntational efficiency as well as E( _n_ )- invariance and equivariance. Our demonstrations were\n\n\nconducted on the mesh structured dataset based on the FEA results. However, we expect\n\n\nIsoGCNs to be applied to various domains, such as object detection, molecular property\n\n\nprediction, and physical simulations using particles.",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 103,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 147,
        "char_count": 999,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "851a0623-ed7d-481a-aa61-530b03009ec4",
      "content": "**80** 3. IsoGCN: E( _n_ )-Equivariant Graph Convolutional Network",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 104,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 9,
        "char_count": 66,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "1d93daeb-f6bd-4aa1-9093-23921b993b89",
      "content": "# **Chapter 4** **Physics-Embedded Neural Network:** **Boundary Condition and Implicit** **Method**\n\n4.1 I NTRODUCTION\n\n\nIn Chapter 3, we introduced IsoGCN, a lightweight E( _n_ )-equivariant graph neural net\n\nwork. It can:\n\n\n   - handle an arbitrary mesh thanks to the generalizability of GNN;\n\n\n   - reflect symmetries regarding E( _n_ ) transformation that exists in physical phenom\n\nena; and\n\n\n   - predict faster than conventional numerical analysis methods and complex GNNs\n\n\nbased on linear message passing scheme.\n\n\nHowever, we still miss the following keys to constructing general PDE solvers:\n\n\n   - **Treatment of mixed boundary conditions** : Mixed boundary condition contains\n\n\nDirichlet and Neumann boundary conditions in disjoint boundary regions, as ex\n\npressed in Equations 2.54 and 2.55. The IsoGCN model demonstrated in Sec\n\ntion 3.4.2 considers only adiabatic boundary conditions corresponding to the ho\n\n81",
      "chunk_metadata": {
        "section_title": "**Chapter 4** **Physics-Embedded Neural Network:** **Boundary Condition and Implicit** **Method**",
        "section_level": 1,
        "page": 105,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 132,
        "char_count": 927,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "a9b12559-84c0-4c2f-b42e-c0de0936a7f3",
      "content": "**82** 4. Physics-Embedded Neural Network: Boundary Condition and Implicit Method\n\n\nmogeneous Neumann boundary condition (Equation 3.73). Therefore, we must\n\n\nprovide a provable way to handle mixed boundary conditions.\n\n\n   - **An implicit manner for time evolution** : In Section 3.4.2, we constructed IsoGCN\n\n\nmodels based on the explicit Euler method (Equation 3.80). It can consider interac\n\ntions between vertices _k_ hop away by stacking _k_ IsoGCN layers. However, global\n\n\ninteraction may sometimes occur as in incompressible flow phenomena, where the\n\n\nspeed of sound is regarded as infinity. These global interactions require IsoGCN\n\n\nlayers stacked more than the number of vertices _|V|_, which may result in huge\n\n\ncomputation time. Therefore, we must incorporate implicit time evolution that can\n\n\nconsider global interaction.\n\n\n   - **Demonstration in various PDEs** : We demonstrated IsoGCN’s expressibility using",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 106,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 130,
        "char_count": 928,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "88c2b226-4445-4147-9cb9-af863b43ffca",
      "content": "the heat equation in Section 3.4.2. However, there are many PDEs in addition to the\n\n\nheat equation. Thus, we must show the model can learn various phenomena, such\n\n\nas the advection-diffusion and incompressible flow problems.\n\n\nThus, we introduce _physics-embedded neural networks_ (PENNs), a machine learning\n\n\nframework to address these issues by embedding physics in the models. We build our model\n\n\nbased on IsoGCN to reflect physical symmetry and realize fast prediction. Furthermore, we\n\n\nconstruct a method to consider mixed boundary conditions. Finally, we reconsider a way\n\n\nto stack GNNs based on a nonlinear solver, which naturally introduces the global pooling\n\n\nto GNNs as the global interaction with high interpretability. In numerical experiments, we\n\n\ndemonstrate that our treatment of Neumann boundary conditions improves the predictive\n\n\nperformance of the model, and our method can fulfill Dirichlet boundary conditions with",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 106,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 137,
        "char_count": 944,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "01ea2208-fcac-4e6f-9ef9-3e4287b17027",
      "content": "no error. Our method also achieves state-of-the-art performance compared to a classical,\n\n\nwell-optimized numerical solver and a baseline machine learning model in speed-accuracy\n\n\ntrade-off.\n\n\nFigure 4.1 shows the overview of the proposed model. Our main contributions are\n\n\nsummarized as follows:\n\n\n   - We construct models to satisfy mixed boundary conditions: the _boundary encoder_,\n\n\n_Dirichlet layer_, _pseudoinverse decoder_, and _NeumannIsoGCN_ (NIsoGCN). The",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 106,
        "chunk_index": 2,
        "chunk_type": "text",
        "token_count": 60,
        "char_count": 468,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "1eff5eaa-3e04-4fc7-9ce1-fe1c2229f230",
      "content": "4.2. Related Prior Work **83**\n\n\nconsidered models show provable fulfillment of boundary conditions, while exist\n\ning models cannot.\n\n\n- We propose _neural nonlinear solvers_, which realize global connections to stably\n\n\npredict the state after a long time.\n\n\n- We demonstrate that the proposed model shows state-of-the-art performance in\n\n\nspeed-accuracy trade-off, and all the proposed components are compatible with\n\n\nE( _n_ )-equivariance.\n\n\n\n画像はDirichlet層とNeumannIsoGCNを組み合わせたE(n)-対称グラフニューラルネットワークの仕組みを示しています。グラフの境界条件を考慮するEncoderとBoundary Encoderを経て、Neumann境界条件をエンコードし、Pseudoinverse Decoderを通じて境界条件を再現するプロセスが描かれています。\n\n\n\n\n\nDirichlet boundary\n\ncondition\n\n\n\n\n\n\n\n\n\nboundary condition Input feature boundary condition Encoded feature Output feature\n\n\n\nFigure 4.1: Overview of the proposed method. On decoding input features, we apply\n\n\nboundary encoders to boundary conditions. Thereafter, we apply a nonlinear solver con",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 107,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 102,
        "char_count": 921,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "3f539e1a-447b-4a04-9655-0e97622f79f5",
      "content": "sisting of an E( _n_ )-equivariant graph neural network in the encoded space. Here, we apply\n\n\nencoded boundary conditions for each iteration of the nonlinear solver. After the solver\n\n\nstops, we apply the pseudoinverse decoder to satisfy Dirichlet boundary conditions.\n\n\n4.2 R ELATED P RIOR W ORK\n\n\nWe review machine learning models used to solve PDEs called neural PDE solvers,\n\n\ntypically formulated as _**u**_ ( _t_ _n_ +1 _,_ _**x**_ _i_ ) _≈F_ NN ( _**u**_ )( _t_ _n_ _,_ _**x**_ _i_ ) for ( _t_ _n_ _,_ _**x**_ _i_ ) _∈{t_ 0 _, t_ 1 _, . . . } ×_ Ω,\n\n\nwhere _F_ NN is a machine learning model.\n\n\n4.2.1 P HYSICS -I NFORMED N EURAL N ETWORK (PINN)\n\n\nRaissi et al. (2019) made a pioneering work combining PDE information and neural\n\n\nnetworks, called PINNs, by adding loss to monitor how much the output satisfies the equa\n\ntions. PINNs can be used to solve forward and inverse problems and extract physical",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 107,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 163,
        "char_count": 911,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "be26da9d-d6f0-4736-b7fe-8e41d451a5fb",
      "content": "**84** 4. Physics-Embedded Neural Network: Boundary Condition and Implicit Method\n\n\nstates from measurements (Pang et al., 2019; Mao et al., 2020; Cai et al., 2021). However,\n\n\nPINNs’ outputs should be functions of space because PINNs rely on automatic differentia\n\ntion to obtain loss regarding PDEs. This design constraint significantly limits the model’s\n\n\ngeneralization ability because the solution of a PDE could be entirely different when the\n\n\nshape of the domain or boundary condition changes. Besides, the loss reflecting PDEs helps\n\n\nmodels learn physics at training time; however, prediction by PINN models can be out of\n\n\nphysics because of lacking PDE information inside the model. Therefore, these methods\n\n\nare not applicable in building models that are generalizable over shape and boundary con\n\ndition variations. As seen in Section 4.3, our model contains PDE information inside and\n\n\ndoes not take absolute positions of vertices, thus resulting in high generalizability (See",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 108,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 148,
        "char_count": 994,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "0def5f35-704f-46e3-a014-397de3496940",
      "content": "Figure 4.16).\n\n\n4.2.2 G RAPH N EURAL N ETWORK B ASED PDE S OLVER\n\n\nAs discussed in Sections 2.2.2, 2.2.4, and 3.2.3, one can regard a mesh as a graph and\n\n\nvarious existing studies demonstrated that GNNs can learn physical phenomena, as seen\n\n\nin Alet et al. (2019); Chang & Cheng (2020); Pfaff et al. (2021). Then, Brandstetter et al.\n\n\n(2022) advanced these works by suggesting temporal bundling and pushforward trick for\n\n\nefficient and stable prediction. Their method could also consider boundary conditions by\n\n\nfeeding them to the models as inputs. Here, one could expect the model to learn to satisfy\n\n\nboundary conditions approximately, while there is no guarantee to fulfill hard constraints\n\n\nsuch as Dirichlet conditions. In contrast, our model ensures the satisfaction of boundary\n\n\nconditions. Besides, most GNNs use local connections with a fixed number of message\n\n\npassings, which lacks consideration of global interaction. We suggest an effective way to",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 108,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 152,
        "char_count": 970,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "67e1fdec-2ac3-48d2-93ad-9aa755f15e4c",
      "content": "incorporate a global connection with GNN through the neural nonlinear solver.\n\n\n4.3 M ETHOD\n\n\nWe present our model architecture. Following the study done in Section 3.4, we adopt\n\n\nthe encode-process-decode architecture, proposed by Battaglia et al. (2018), which has\n\n\nbeen applied successfully in various previous works, e.g., Pfaff et al. (2021); Brandstetter\n\n\net al. (2022). Our key concept is to encode input features, including information on bound\n\nary conditions, apply a GNN-based nonlinear solver loop reflecting boundary conditions",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 108,
        "chunk_index": 2,
        "chunk_type": "text",
        "token_count": 78,
        "char_count": 543,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "bb972a9b-d75e-48ce-b032-0a44403b0847",
      "content": "4.3. Method **85**\n\n\nin the encoded space, then decode carefully to satisfy boundary conditions in the output\n\n\nspace. In this section, we continue to use the discrete tensor field (Equation 3.5) expressed\n\n\nas:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n_**H**_ =\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n_**h**_ 1\n\n\n_**h**_ 2\n\n\n...\n\n\n_**h**_\n_|V|_\n\n\n\n_,_ (4.1)\n\n\n\nwhile we do not write the tensor rank explicitly unless needed.\n\n\n4.3.1 D IRICHLET B OUNDARY M ODEL\n\n\nAs demonstrated theoretically and experimentally in literature (Hornik, 1991; Cybenko,\n\n\n1992; Nakkiran et al., 2021), the expressive power of neural networks comes from encoding\n\n\nin a higher-dimensional space, where the corresponding boundary conditions are not trivial.\n\n\nHowever, if there are no boundary condition treatments in layers inside the processor, which\n\n\nresides in the encoded space, the trajectory of the solution can be far from the one with\n\n\nboundary conditions. Therefore, boundary condition treatments in an encoded space are",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 109,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 166,
        "char_count": 997,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "e3cf105b-79af-4dea-bc0f-65ec48b4b337",
      "content": "essential for obtaining reliable neural PDE solvers that fulfill boundary conditions.\n\n\n4.3.1.1 B OUNDARY E NCODER\n\n\nTo ensure the same encoded space between variables and boundary conditions, we use\n\n\nthe same encoder for variables and the corresponding Dirichlet boundary conditions, which\n\n\nwe term the _boundary encoder_, as follows:\n\n\n_**h**_ _i_ = _**f**_ encode ( _**u**_ _i_ ) in Ω (4.2)\n\n\nˆ\n_**h**_ _i_ = _**f**_ encode (ˆ _**u**_ _i_ ) on _∂_ Ω Dirichlet _,_ (4.3)\n\n\nwhere ˆ _**u**_ _i_ is the value of the Dirichlet boundary condition at _**x**_ _i_ _∈_ _∂_ Ω Dirichlet .",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 109,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 96,
        "char_count": 582,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "848704d2-0d25-4bb5-8bf5-be33ed0a7ff5",
      "content": "**86** 4. Physics-Embedded Neural Network: Boundary Condition and Implicit Method\n\n\n4.3.1.2 D IRICHLET L AYER\n\n\nOne can easily apply Dirichlet boundary conditions in the aforementioned encoded\n\n\nspace using the _Dirichlet layer_ defined as:\n\n\n\nDirichletLayer( _**h**_ _i_ ) =\n\n\n\n\n\n_**h**_ _i_ _,_ _**x**_ _i_ _/∈_ _∂_ Ω Dirichlet\nˆ (4.4)\n\n\n\n _**h**_ _i_ _,_ _**x**_ _i_ _∈_ _∂_ Ω Dirichlet _._\n\n\n\nThis process is necessary to return to the state respecting the boundary conditions after\n\n\nsome operations in the processor, which might violate the conditions.\n\n\n4.3.1.3 P SEUDOINVERSE D ECODER\n\n\nAfter the processor layers, we decode the hidden features using functions satisfying:\n\n\n_**f**_ decode _◦_ _**f**_ encode (ˆ _**u**_ _i_ ) = ˆ _**u**_ _i_ on _∂_ Ω Dirichlet _._ (4.5)\n\n\nThis condition ensures that the encoded boundary conditions correspond to the ones in the\n\n\noriginal physical space. Demanding that Equation 4.5 holds for arbitrary ˆ _**u**_ ; we obtain:",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 110,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 151,
        "char_count": 971,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "73fc4db4-7ee9-4fc6-b477-822c6cfff359",
      "content": "_**f**_ decode _◦_ _**f**_ encode = Id _**u**_ _,_ (4.6)\n\n\nwhere Id _**u**_ denotes the identity map from the space of _**u**_ to the same space. By applying\n\n\n_**f**_ encode [+] [, a left inverse function of the encoder, we have:]\n\n\n_**f**_ decode = _**f**_ encode [+] _[,]_ (4.7)\n\n\nwhich we call the _pseudoinverse decoder_ . It is pseudoinverse because _**f**_ encode, in particular\n\n\nencoding in a higher-dimensional space, may not be invertible. Therefore, we construct\n\n\n_**f**_ encode [+] [using pseudoinverse matrices.]\n\n\nWe can construct the pseudoinverse decoders for a wide range of neural network archi\n\ntectures. For instance, the pseudoinverse decoder for an MLP with one hidden layer\n\n\n_**h**_ = _**f**_ ( _**x**_ ) = _σ_ 2 ( _**W**_ 2 _σ_ 1 ( _**W**_ 1 _**x**_ + _**b**_ 1 ) + _**b**_ 2 ) (4.8)",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 110,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 136,
        "char_count": 810,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "b58bda82-7145-48e8-864e-571bc5ffd663",
      "content": "can be constructed as:\n\n\n\n4.3. Method **87**\n\n\n_**f**_ [+] ( _**h**_ ) = _**W**_ 1 [+] _[σ]_ 1 _[−]_ [1] \u0001 _**W**_ 2 [+] _[σ]_ 2 _[−]_ [1] [(] _**[h]**_ [)] _[ −]_ _**[b]**_ [2] \u0002 _−_ _**b**_ 1 _,_ (4.9)\n\n\n\nwhere _**W**_ [+] is the pseudoinverse matrix of _**W**_, satisfying _**W**_ [+] _**W**_ = _**I**_, and _σ_ is an invertible\n\n\nactivation function whose Dom( _σ_ ) = Im( _σ_ ) = R. We can confirm that _**f**_ [+] is in fact the\n\n\npseudoinverse of _**f**_ as:\n\n\n_**f**_ [+] _◦_ _**f**_ ( _**x**_ ) = _**W**_ 1 [+] _[σ]_ 1 _[−]_ [1] \u0001 _**W**_ 2 [+] _[σ]_ 2 _[−]_ [1] [(] _[σ]_ [2] [(] _**[W]**_ [2] _[σ]_ [1] [(] _**[W]**_ [1] _**[x]**_ [ +] _**[ b]**_ [1] [) +] _**[ b]**_ [2] [))] _[ −]_ _**[b]**_ [2] \u0002 _−_ _**b**_ 1\n\n\n= _**W**_ 1 [+] _[σ]_ 1 _[−]_ [1] \u0001 _**W**_ 2 [+] _**[W]**_ [2] _[σ]_ [1] [(] _**[W]**_ [1] _**[x]**_ [ +] _**[ b]**_ [1] [) +] _**[ b]**_ [2] _[−]_ _**[b]**_ [2] \u0002 _−_ _**b**_ 1",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 111,
        "chunk_index": 0,
        "chunk_type": "formula",
        "token_count": 178,
        "char_count": 905,
        "contains_formulas": true,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "e14c71ff-a35d-4c18-9d88-2ea7446d92e0",
      "content": "= _**W**_ 1 [+] _[σ]_ 1 _[−]_ [1] [(] _[σ]_ [1] [(] _**[W]**_ [1] _**[x]**_ [ +] _**[ b]**_ [1] [))] _[ −]_ _**[b]**_ [1]\n\n\n= _**W**_ 1 [+] _**[W]**_ [1] _**[x]**_ [ +] _**[ b]**_ [1] _[−]_ _**[b]**_ [1]\n\n\n= _**x**_ _._ (4.10)\n\n\nFor the activation function, we may choose LeakyReLU\n\n\n\n(4.11)\n_ax_ ( _x <_ 0) _,_\n\n\n\nLeakyReLU( _x_ ) =\n\n\n\n\n\n\n\n\n_x_ ( _x ≥_ 0)\n\n\n\n_ax_ ( _x <_ 0)\n\n\n\n\n\nwhere set _a_ = 0 _._ 5 because an extreme value of _a_ (e.g., 0.01) could lead to an extreme\n\n\nvalue of gradient for the inverse function. In addition, one may choose activation functions\n\n\nwhose Im( _σ_ ) _̸_ = R, such as tanh. However, in that case, we must ensure that the input\n\n\nvalue to the pseudoinverse decoder is in Im( _σ_ ) (in case of tanh, it is ( _−_ 1 _,_ 1)); otherwise,\n\n\nthe computation would be invalid.\n\n\n4.3.2 N EUMANN B OUNDARY M ODEL\n\n\nMatsunaga et al. (2020) proposed a wall boundary model to deal with Neumann bound",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 111,
        "chunk_index": 1,
        "chunk_type": "formula",
        "token_count": 179,
        "char_count": 926,
        "contains_formulas": true,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "8d0bf660-be15-494e-9fc9-18204d355961",
      "content": "ary conditions for the LSMPS method (Tamai & Koshizuka, 2014) (Section 2.2.4.3), a\n\n\nframework to solve PDEs using particles. The LSMPS method is the origin of the IsoGCN’s\n\n\ngradient operator, so one can imagine that the wall boundary model may introduce a so\n\nphisticated treatment of Neumann boundary conditions into IsoGCN. We modified the wall",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 111,
        "chunk_index": 2,
        "chunk_type": "text",
        "token_count": 55,
        "char_count": 348,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "1cf32d65-cead-4b91-9f92-64d00853949c",
      "content": "**88** 4. Physics-Embedded Neural Network: Boundary Condition and Implicit Method\n\n\nboundary model to adapt to the situation where the vertices are on the Neumann boundary,\n\n\nwhich differs from the situation of particle simulations.\n\n\n4.3.2.1 D EFINITION OF N EUMANN I SO GCN (NI SO GCN)\n\n\nOur formulation of IsoGCN with Neumann boundary conditions, which is termed _Neu-_\n\n\n_mannIsoGCN_ (NIsoGCN), is expressed as:\n\n\nNIsoGCN 0 _→_ 1 ( _**H**_ [(0)] )\n\n\n\n$\"\n\n\n\n_j∈N_ _i_\n\n\n\n:= EquivariantPointwiseMLP\n\n\n\n!\n\n\n\n_**M**_ _i−_ 1",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 112,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 79,
        "char_count": 523,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "4de3bd3b-9947-48ae-a598-d7187018b838",
      "content": "# Y _j∈N_ _i_\n\n_**h**_ [(0)] _j_ _−_ _**h**_ [(0)] _j_ _**x**_ _j_ _−_ _**x**_ _i_\n_∥_ _**x**_ _j_ _−_ _**x**_ _i_ _∥_ _∥_ _**x**_ _j_ _−_ _**x**_ _i_ _∥_ _[w]_ _[ij]_ [ +] _[ w]_ _[i]_ _**[n]**_ _[i]_ _**[g]**_ [ˆ] _[i]_\n\n\n\n_**M**_ _i_ :=\nY\n\n_l∈N_ _i_\n\n\n\n(4.12)\n\n\n_**x**_ _l_ _−_ _**x**_ _i_ _**x**_ _l_ _−_ _**x**_ _i_\n(4.13)\n_∥_ _**x**_ _l_ _−_ _**x**_ _i_ _∥_ _[⊗]_ _∥_ _**x**_ _l_ _−_ _**x**_ _i_ _∥_ _[w]_ _[ij]_ [ +] _[ w]_ _[i]_ _**[n]**_ _[i]_ _[ ⊗]_ _**[n]**_ _[i]_ _[,]_\n\n\n\nwhere ˆ _**g**_ _i_ is the encoded value of the Neumann boundary condition at _**x**_ _i_ and _w_ _i_ _>_ 0 is\n\n\nan untrainable parameter to control the strength of the Neumann constraint. As _w_ _i_ _→∞_,\n\n\nthe model strictly satisfies the given Neumann condition in the direction _**n**_ _i_, while the\n\n\ndirectional derivatives in the direction of ( _**x**_ _j_ _−_ _**x**_ _i_ ) tend to be relatively neglected. Thus,",
      "chunk_metadata": {
        "section_title": "Y _j∈N_ _i_",
        "section_level": 1,
        "page": 112,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 160,
        "char_count": 906,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "8e2461dd-17cc-45ca-9a50-22f0bba13785",
      "content": "we keep the value of _w_ _i_ moderate to consider derivatives in both _**n**_ and _**x**_ directions. In\n\n\nparticular, we set _w_ _i_ = 10 _._ 0, assuming that around ten vertices may virtually exist ”outside”\n\n\nthe boundary on a flat surface in a 3D space.\n\n\nNIsoGCN is a straightforward generalization of the original IsoGCN by letting _**n**_ _i_ = **0**\n\n\nwhen _**x**_ _i_ _/∈_ _∂_ Ω Neumann . This model can also be generalized to vectors or higher rank\n\n\ntensors, similarly to the original IsoGCN’s construction. Therefore, NIsoGCN can express\n\n\nany spatial differential operator, constituting _D_ in PDEs.\n\n\n4.3.2.2 D ERIVATION OF NI SO GCN\n\n\nMatsunaga et al. (2020) derived a gradient model that can treat the Neumann boundary\n\n\ncondition with an arbitrary convergence rate with regard to spatial resolution. Here, we\n\n\nderive our gradient model, i.e., NIsoGCN, in a different way to simplify the discussion\n\n\nbecause we only need the first-order approximation for fast computation.",
      "chunk_metadata": {
        "section_title": "Y _j∈N_ _i_",
        "section_level": 1,
        "page": 112,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 157,
        "char_count": 990,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "bf77cb84-580e-4cc6-ac4c-2bf3a4f9bd53",
      "content": "4.3. Method **89**\n\n\nBefore deriving NIsoGCN, we review introductory linear algebra using simple norma\ntion. Using a orthonormal basis _**e**_ _j_ _∈_ R _[d]_ _**e**_ _j_ _·_ _**e**_ _k_ = _δ_ _jk_ _nj_ =1 [, one can decompose a vector]\n\n\n_**v**_ _∈_ R _[n]_ using:\n\n\n_**v**_ = Y ( _**v**_ _·_ _**e**_ _j_ ) _**e**_ _j_ _._ (4.14)\n\n_j_\n\n\nNow, consider replacing the basis _{_ _**e**_ _j_ _∈_ R _[n]_ _}_ _[n]_ _j_ =1 [with a set of vectors] _**[ B]**_ [ =] _[ {]_ _**[b]**_ _[j]_ _[∈]_\n\nR _[n]_ _}_ _[n]_ _j_ =1 _[′]_ [, called a] _[ frame]_ [, that spans the space but is not necessarily independent (thus,] _[ n]_ _[′]_ _[ ≥]_\n\n\n_n_ ). Using the frame, one can assume _**v**_ is decomposed as:\n\n\n_**v**_ = Y ( _**v**_ _·_ _**b**_ _j_ ) _**Ab**_ _j_ _,_ (4.15)\n\n_j_\n\n\nwhere _**A**_ _∈_ R _[n][×][n]_ is a matrix that corrects the ”overcount” that may occur using the frame",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 113,
        "chunk_index": 0,
        "chunk_type": "formula",
        "token_count": 160,
        "char_count": 873,
        "contains_formulas": true,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "dea42d72-2763-40c6-83a4-fd387485de0c",
      "content": "(for instance, consider expanding (1 _,_ 0) _[⊤]_ with the frame _{_ (1 _,_ 0) _[⊤]_ _,_ ( _−_ 1 _,_ 0) _[⊤]_ _,_ (0 _,_ 1) _[⊤]_ _}_ ). A\n\nset _{_ _**Ab**_ _j_ _}_ _[d]_ _j_ =0 _[′]_ [is called a] _[ dual frame]_ [ for] _**[ B]**_ [. Recalling Equation 2.122, we can find the]\n\n\nconcrete form of _**A**_ considering:\n\n\n_**v**_ = _**A**_ Y ( _**v**_ _·_ _**b**_ _j_ ) _**b**_ _j_\n\n_j_\n\n= _**A**_ Y ( _**b**_ _j_ _⊗_ _**b**_ _j_ ) _**v**_ _._ (4.16)\n\n_j_\n\n\nRequiring that Equation 4.16 holds for any _**v**_ _∈_ R _[d]_, one can conclude _**A**_ = [Q] _j_ [(] _**[b]**_ _[j]_ _[ ⊗]_ _**[b]**_ _[j]_ [)] _[−]_ [1] [.]\n\n\nThen, we obtain\n\n\n\n_−_ 1\nY ( _**v**_ _·_ _**b**_ _j_ ) _**b**_ _j_ _._ (4.17)\n$ _j_\n\n\n\n_**v**_ =\n\n\n\n_**b**_ _l_ _⊗_ _**b**_ _l_",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 113,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 144,
        "char_count": 745,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "839d148f-5389-404e-8841-0ac4a1e8062e",
      "content": "# Y _l_\n\nFor more details on frames, see, e.g., Han et al. (2007).\n\n\nNow, we can derive NIsoGCN at the _i_ th vertex on the Neumann boundary, by letting\n\n\n\n_**x**_ _j_ _−_ _**x**_ _i_\n_**B**_ = _√_ ~~_w_~~ _ij_\n\u001b _∥_ _**x**_ _j_ _−_ _**x**_ _i_ _∥_\n\n\n\n_∪{_ _[√]_ ~~_w_~~ _i_ _**n**_ _i_ _}._ (4.18)\n\n _j∈N_ _i_",
      "chunk_metadata": {
        "section_title": "Y _l_",
        "section_level": 1,
        "page": 113,
        "chunk_index": 0,
        "chunk_type": "formula",
        "token_count": 58,
        "char_count": 310,
        "contains_formulas": true,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "6692f9b9-ed43-4544-9221-0ff9963a0dd7",
      "content": "**90** 4. Physics-Embedded Neural Network: Boundary Condition and Implicit Method\n\n\nIn addition, we assume the approximated gradient of a scalar field _u_ at the _i_ th vertex, _⟨∇u⟩_ _i_,\n\n\nsatisfies the following conditions:\n\n\n_**x**_ _j_ _−_ _**x**_ _i_ _u_ _j_ _−_ _u_ _i_\n_⟨∇u⟩_ _i_ _·_ _[√]_ ~~_w_~~ _ij_ ( _j ∈N_ _i_ ) (4.19)\n_∥_ _**x**_ _j_ _−_ _**x**_ _i_ _∥_ [=] _[ √]_ ~~_[w]_~~ _[ij]_ _∥_ _**x**_ _j_ _−_ _**x**_ _i_ _∥_\n\n\n_⟨∇u⟩_ _i_ _·_ _[√]_ ~~_w_~~ _i_ ~~_**n**_~~ _i_ = _[√]_ ~~_w_~~ _i_ _g_ ˆ _i_ _._ (4.20)\n\n\nEquation 4.19 is a natural assumption because we expect the directional derivative in the\n\n\ndirection of ( _**x**_ _j_ _k_ _−_ _**x**_ _i_ ) _/∥_ _**x**_ _j_ _k_ _−_ _**x**_ _i_ _∥_ should correspond to the slope of _u_ in the same direction.\n\n\nEquation 4.20 is the Neumann boundary condition, which we want to satisfy. Finally,\n\n\nby substituting Equations 4.18, 4.19, and 4.20 into Equation 4.17, we obtain the gradient",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 114,
        "chunk_index": 0,
        "chunk_type": "formula",
        "token_count": 161,
        "char_count": 947,
        "contains_formulas": true,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "f3df7a7f-92d7-4b46-9028-669404eba91e",
      "content": "model considering the Neumann boundry consition as:",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 114,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 7,
        "char_count": 51,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "e0e1166c-a1c5-4a6e-8c75-4f25a329e2cd",
      "content": "# Y _l∈N_ _i_\n\n$ _−_ 1\n\n\n\n_⟨u⟩_ _i_ =\n\n\n\n_l∈N_ _i_\n\n\n\n_**x**_ _l_ _−_ _**x**_ _i_ _**x**_ _l_ _−_ _**x**_ _i_\n_√_ ~~_w_~~ _il_\n_∥_ _**x**_ _l_ _−_ _**x**_ _i_ _∥_ _[⊗√]_ ~~_[w]_~~ _[il]_ _∥_ _**x**_ _l_ _−_ _**x**_ _i_ _∥_\n\n\n\n+ _[√]_ ~~_w_~~ _i_ _g_ ˆ _i_ _√_ ~~_w_~~ _i_ _**n**_ _i_\n\u0014\n\n\n\n$\n\n\n\n_×_",
      "chunk_metadata": {
        "section_title": "Y _l∈N_ _i_",
        "section_level": 1,
        "page": 114,
        "chunk_index": 0,
        "chunk_type": "formula",
        "token_count": 57,
        "char_count": 297,
        "contains_formulas": true,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "e2eb436c-7cad-488e-9eef-7e95933376b9",
      "content": "# Y _j_\n\n_u_ _j_ _−_ _u_ _i_ _**x**_ _j_ _−_ _**x**_ _i_\n_√_ ~~_w_~~ _ij_ _√_ ~~_w_~~ _ij_\n\u0013 _**x**_ _j_ _−_ _**x**_ _i_ _∥_ _**x**_ _j_ _−_ _**x**_ _i_ _∥_\n\n\n\n$ _−_ 1\n\n\n\n=\n\n\n\n_**x**_ _l_ _−_ _**x**_ _i_ _**x**_ _l_ _−_ _**x**_ _i_\n_w_ _il_",
      "chunk_metadata": {
        "section_title": "Y _j_",
        "section_level": 1,
        "page": 114,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 48,
        "char_count": 240,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "f6a66efd-f3a0-4ae9-90c2-e9ad435d57a4",
      "content": "# Y _l∈N_ _i_ _∥_ _**x**_ _l_ _−_ _**x**_ _i_ _∥_ _[⊗]_ _∥_ _**x**_ _l_ _−_ _**x**_ _i_ _∥_\n\n$\n\n\n\n_**x**_ _j_ _−_ _**x**_ _i_\n_∥_ _**x**_ _j_ _−_ _**x**_ _i_ _∥_\n\n\n\n+ _w_ _i_ _g_ ˆ _i_ _**n**_ _i_\n\u0014\n\n\n\n_×_",
      "chunk_metadata": {
        "section_title": "Y _l∈N_ _i_ _∥_ _**x**_ _l_ _−_ _**x**_ _i_ _∥_ _[⊗]_ _∥_ _**x**_ _l_ _−_ _**x**_ _i_ _∥_",
        "section_level": 1,
        "page": 114,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 42,
        "char_count": 205,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "a236cfae-c9dd-4928-aeea-42df6fbde3c2",
      "content": "# Y _j_\n\n_u_ _j_ _−_ _u_ _i_\n_w_ _ij_\n\u0013 _**x**_ _j_ _−_ _**x**_ _i_\n\n\n\n_._ (4.21)\n\n\n\nIf we apply the gradient model to a encoded features, we obtain the gradient model in the\n\n\nNIsoGCN layer, i.e., Equation 4.12. Similar to the Dirichlet encoder and pseudoinverse\n\n\ndecoder, we could define the specific encoder and decoder for the Neumann boundary\n\n\ncondition. However, this is not included in the contributions of our work because it does\n\n\nnot improve the performance of our model, which may be because the Neumann boundary\n\n\ncondition is a soft constraint in contrast to the Dirichlet one and expressive power seems\n\n\nmore important than that inductive bias.",
      "chunk_metadata": {
        "section_title": "Y _j_",
        "section_level": 1,
        "page": 114,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 110,
        "char_count": 662,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "08cc51d6-da78-4b48-b174-155e84661b92",
      "content": "4.3. Method **91**\n\n\n4.3.2.3 G ENERALIZATION OF NI SO GCN\n\n\nTo apply NIsoGCN to _**H**_ [(] _[p]_ [)], a rank _p_ discrete tensor field ( _p ≥_ 1), one can recursively\n\n\ndefine the operation as:\n\n\n\nNIsoGCN _p−_ 1 _→p_ ( _**H**_ :;:;1 [(] _[p]_ [)] _..._ [)]\n\nNIsoGCN _p−_ 1 _→p_ ( _**H**_ :;:;2 [(] _[p]_ [)] _..._ [)]\n\nNIsoGCN _p−_ 1 _→p_ ( _**H**_ :;:;3 [(] _[p]_ [)] _..._ [)]\n\n\n\n\n\n (4.22)\n _[,]_\n\n\n\nNIsoGCN _p→p_ +1 ( _**H**_ [(] _[p]_ [)] ) :=\n\n\n\n\n\n\n\n\n\n\n\nwhere _**H**_ :;:; [(] _[p]_ [)] _i..._ _[∈]_ [R] _[|V|×][d]_ [feature] _[×][n]_ _[p][−]_ [1] [ is the] _[ i]_ [th component of] _**[ H]**_ [(] _[p]_ [)] [ regarding the first spatial]\n\n\nindex, resulting in the rank ( _p_ _−_ 1) discrete tensor field. In case of a three-dimensional rank\n\n\none discrete tensor field _**H**_ [(1)], it can be formulated as:\n\n\n\n_∂_ _**H**_ [(1)] _∂_ _**H**_ [(1)] _∂_ _**H**_ [(1)]\n:;:;1 _[/∂x]_ :;:;1 _[/∂y]_ :;:;1 _[/∂z]_\nE F E F E F",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 115,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 172,
        "char_count": 934,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "1180d36a-511f-491f-8c41-1b27d666ad79",
      "content": "_∂_ _**H**_ [(1)] _∂_ _**H**_ [(1)] _∂_ _**H**_ [(1)]\n:;:;2 _[/∂x]_ :;:;2 _[/∂y]_ :;:;2 _[/∂z]_\nE F E F E F\n\n_∂_ _**H**_ [(1)] _∂_ _**H**_ [(1)] _∂_ _**H**_ [(1)]\n:;:;3 _[/∂x]_ :;:;3 _[/∂y]_ :;:;3 _[/∂z]_\nE F E F E F\n\n\n\n\n\n\n (4.23)\n\n\n\n\n\n\n\n\n\n\n\n\n\nNIsoGCN 1 _→_ 2 ( _**H**_ [(1)] ) :=\n\n\n_≈_\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNIsoGCN 0 _→_ 1 ( _**H**_ :;:;1 [(1)] [)]\n\nNIsoGCN 0 _→_ 1 ( _**H**_ :;:;2 [(1)] [)]\n\nNIsoGCN 0 _→_ 1 ( _**H**_ :;:;3 [(1)] [)]\n\n\n\n= _∇⊗_ _**H**_ [(1)] _,_ (4.24)\n\n\nwhich corresponds to the Jacobian tensor field of _**H**_ [(1)] . Similarly, NIsoGCN to decrease\n\n\ntensor rank can be defined as:\n\n\nNIsoGCN _p→p−_ 1 ( _**H**_ [(] _[p]_ [)] ) :=NIsoGCN _p−_ 1 _→p_ ( _**H**_ :;:;1 [(] _[p]_ [)] _..._ [)]\n\n+ NIsoGCN _p−_ 1 _→p_ ( _**H**_ :;:;2 [(] _[p]_ [)] _..._ [)]\n\n+ NIsoGCN _p−_ 1 _→p_ ( _**H**_ :;:;3 [(] _[p]_ [)] _..._ [)] _[.]_ (4.25)\n\n\nAs discussed in Section 3.3.4.1, IsoGCNs (NIsoGCNs) correspond to spatial differen",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 115,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 185,
        "char_count": 948,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "ec9445a6-3f8c-4613-bc58-6fa3e7864f9b",
      "content": "tial operators. Because NIsoGCN contains a learnable neural network (Equation 4.12),\n\n\nthe component learns to predict the derivative of the corresponding tensor rank in an en",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 115,
        "chunk_index": 2,
        "chunk_type": "text",
        "token_count": 26,
        "char_count": 175,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "5154f451-176e-4ad6-9fda-15ba92e0bcea",
      "content": "**92** 4. Physics-Embedded Neural Network: Boundary Condition and Implicit Method\n\n\ncoded space. This feature of NIsoGCNs enables us to construct machine learning models\n\n\ncorresponding to PDE in the encoded space.\n\n\n4.3.3 N EURAL N ONLINEAR S OLVER\n\n\n4.3.3.1 I MPLICIT E ULER M ETHOD IN E NCODED S PACE\n\n\nAs reviewed in Section 2.2.1, one can regard solving PDEs as optimization. To con\n\nstruct a neural PDE solver using the implicit Euler method in a high-dimensional encoded\n\n\nspace, we first define the residual and the nonlinear problem in the encoded space based on\n\n\nEquation 2.63 as:\n\n\n_**R**_ NIsoGCN ( _**H**_ _[′]_ ) := _**H**_ _[′]_ _−_ _**H**_ ( _t_ ) _−D_ NIsoGCN ( _**H**_ _[′]_ )∆ _t_ (4.26)\n\n\nSolve _[′]_ _**H**_ _**[R]**_ [NIsoGCN] [(] _**[H]**_ _[′]_ [) =] **[0]** _[,]_ (4.27)\n\n\nwhere _**H**_ ( _t_ ) and _**H**_ _[′]_ are discrete tensor fields, and _D_ NIsoGCN is an E( _n_ )-equivariant GNN",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 116,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 152,
        "char_count": 913,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "2eebd1a7-5cc2-418a-8923-8997f4617e09",
      "content": "reflecting the structure of _D_ using differential operators provided by NIsoGCN (See Sec\n\ntion 4.4 for the concrete examples of _D_ NIsoGCN ). Equation 4.27 corresponds to solving a\n\n\nPDE in a high-dimensional encoded space, where we can utilize the expressibility of neural\n\n\nnetworks.\n\n\nOne may consider solving Equation 4.27 by using the Newton–Raphson method. How\n\never, it may consume huge memory because we embed the input feature into a high\n\ndimensional space, resulting in a large matrix to solve. In addition, we must use GPUs\n\n\nto accelerate the training of models, which makes memory limitation more strict. Fur\n\nthermore, solving linear systems makes the computation graph extremely long, leading to\n\n\nunstable backpropagation. There are various existing studies to challenge this type of prob\n\nlem, e.g., Neural ODE (Chen et al., 2018) and implicit GNN (Gu et al., 2020). Besides,\n\n\nadopting limited-memory quasi-Newton methods (e.g., Liu & Nocedal (1989)) might be in",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 116,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 152,
        "char_count": 983,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "4eb7d0ed-759d-4a9c-bfac-bb3a40b58576",
      "content": "teresting as they are supposed to facilitate incorporating global interactions. Nevertheless,\n\n\nwe applied the gradient descent method in this research for simplicity and computational\n\n\nefficiency. Based on Equation 2.70, gradient descent in the encoded space can be expressed",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 116,
        "chunk_index": 2,
        "chunk_type": "text",
        "token_count": 38,
        "char_count": 277,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "6afe0b64-a8a0-42ba-be35-2ce0e984ae8f",
      "content": "as:\n\n\n\n4.3. Method **93**\n\n\n_**H**_ [[0]] := _**H**_ ( _t_ ) (4.28)\n\n\n_**H**_ [[] _[i]_ [+1]] := _**H**_ [[] _[i]_ []] _−_ _α_ [[] _[i]_ []] _**R**_ NIsoGCN ( _**H**_ [[] _[i]_ []] ) _i >_ 0 _,_ (4.29)\n\n\n\nwhere _**H**_ [[] _[i]_ []] denotes the approximated solution at _i_ th step of the iterative nonlinear solver\n\n\n(as in Section 2.2.3), not a rank- _i_ discrete tensor field.\n\n\n4.3.3.2 B ARZILAI –B ORWEIN M ETHOD FOR N EURAL N ONLINEAR S OLVER\n\n\nAs discussed in Section 2.2.3.3, _α_ [[] _[i]_ []] are determined by the line search, requiring addi\n\ntional computational resource. However, using a small constant value of _α_ results in the\n\n\nexplicit Euler method, which corresponds to simply stacking the GNN layers. Therefore,\n\n\nwe adopt the Barzilai–Borwein method (Barzilai & Borwein, 1988) to approximate _α_ [[] _[i]_ []] in\n\n\nEquation 4.29. In our case, by applying Equation 2.81, the step size _α_ [[] _[i]_ []] of gradient descent\n\n\nis approximated as:",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 117,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 163,
        "char_count": 965,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "c939da24-6564-405c-a9ef-db57884fd002",
      "content": "_α_ [[] _[i]_ []] _≈_ _α_ [[] _[i]_ []]\nBB [:=]\n\n\n\n\u0003 _**H**_ [[] _[i]_ []] _−_ _**H**_ [[] _[i][−]_ [1]] [\u0004] _·_ \u0003 _**R**_ NIsoGCN ( _**H**_ [[] _[i]_ []] ) _−_ _**R**_ NIsoGCN ( _**H**_ [[] _[i][−]_ [1]] ) \u0004\n\n_._ (4.30)\n_∥_ _**R**_ NIsoGCN ( _**H**_ [[] _[i]_ []] ) _−_ _**R**_ NIsoGCN ( _**H**_ [[] _[i][−]_ [1]] ) _∥_ [2]\n\n\n\nHere, _·_ denotes the inner product between two discrete tensor fields with the same shape,\n\n\ni.e.:\n\n\n_**H**_ [(] _[p]_ [)] _·_ _**G**_ [(] _[p]_ [)] = Y _H_ _i_ ; _g_ ; _k_ 1 _k_ 2 _...k_ _p_ _G_ _i_ ; _g_ ; _k_ 1 _k_ 2 _...k_ _p_ _∈_ R _,_ (4.31)\n\n_igk_ 1 _k_ 2 _...k_ _p_\n\n\nfor rank- _p_ discrete tensor fields _**H**_ [(] _[p]_ [)] and _**G**_ [(] _[p]_ [)] . Besides, _∥_ _**H**_ _∥_ [2] := _**H**_ _·_ _**H**_ . The inner\n\n\nproduct used here corresponds to that for rank- _p_ continuous tensor fields, _⟨_ _**h**_ _,_ _**g**_ _⟩_ (where\n\n\n_**h**_ _,_ _**g**_ : Ω _→_ R _[d]_ [feature] _[×][n]_ _[p]_ ), because:",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 117,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 182,
        "char_count": 945,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "43ff3415-979b-422a-bc0e-539c9d077ff5",
      "content": "_⟨_ _**h**_ _,_ _**g**_ _⟩_ = _**h**_ ( _**x**_ ) _·_ _**g**_ ( _**x**_ ) _d_ Ω( _**x**_ ) (4.32)\n\n[ Ω\n\n_≈_ Y _**h**_ ( _**x**_ _i_ ) _·_ _**g**_ ( _**x**_ _i_ ) _V_ _i_ (4.33)\n\n\n_i_\n\n= Y _H_ _i_ ; _g_ ; _k_ 1 _k_ 2 _...k_ _p_ _G_ _i_ ; _g_ ; _k_ 1 _k_ 2 _...k_ _p_ _V_ _i_ [effective] _,_ (4.34)\n\n_igk_ 1 _k_ 2 _...k_ _p_",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 117,
        "chunk_index": 2,
        "chunk_type": "text",
        "token_count": 74,
        "char_count": 322,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "aad8263a-e9d6-4c3f-bd35-0a412aed9f8d",
      "content": "**94** 4. Physics-Embedded Neural Network: Boundary Condition and Implicit Method\n\n\nwherer _V_ _i_ [effective] denotes the effective volume of the _i_ th vertex (Equation 3.74). One must\n\n\nnote that _α_ BB defined here is E( _n_ )-invariant because it is computed using the contraction\n\n\nbetween tensors. Therefore, the gradient descent update:\n\n\n_**H**_ [[] _[i]_ [+1]] := _**H**_ [[] _[i]_ []] _−_ _α_ BB [[] _[i]_ []] _**[R]**_ [NIsoGCN] [(] _**[H]**_ [[] _[i]_ []] [)] (4.35)\n\n\nis E( _n_ )-equivariant.\n\n\nIn addition, one can see that computing _α_ BB [[] _[i]_ []] [corresponds to global pooling because the]\n\n\ninner product is taken all over the mesh (graph). With that view, one can find similarities\n\n\nbetween Equation 4.35 and deep sets (Zaheer et al., 2017). A deep set layer is expressed\n\n\nas:\n\n\nDeepSet( _**H**_ ) := _σ_ \u0001 _λ_ _**H**_ + _γ_ **1** _|V|_ GlobalPooling( _**H**_ ) \u0002 _,_ (4.36)",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 118,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 148,
        "char_count": 902,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "fe93c815-8936-4c71-860b-8fc060d27dd1",
      "content": "where _λ_ and _γ_ are trainable parameters, GlobalPooling : R _[|V|×][d]_ [feature] _[×][n]_ _[p]_ _→_ R [1] _[×][d]_ [feature] _[×][n]_ _[p]_\n\n\ndenotes an operation that aggregates all information in a graph, such as max, mean, and\n\n\nsum, and **1** = (1 _,_ 1 _, . . .,_ 1) _[T]_ _∈_ R _[|V|×]_ [1] . Deep set is a successful method to learn point cloud\n\n\ndata and has a strong background regarding permutation equivariance. However, E( _n_ )\n\nequivariance is not considered in their model. Our gradient descent update (Equation 4.35)\n\n\nsuccessfully incorporate the strength of the deep set model with E( _n_ )-equivariance and\n\n\ninterpretability in terms of the implicit Euler method.\n\n\n4.3.3.3 F ORMULATION OF N EURAL N ONLINEAR S OLVER\n\n\nOur aim is to use Equation 4.35, approximating the nonlinear differential operator\n\n\n_D_ in Equation 2.60 with NIsoGCN. By doing this, we expect the processor, the core of",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 118,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 150,
        "char_count": 913,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "1301c7d3-466b-4315-bb9b-0380528867d1",
      "content": "the encode-decode-processor Architecture, to consider both local and global information,\n\n\nwhich may have an advantage over simply stacking GNNs corresponding to the explicit\n\n\nmethod as discussed in Section 2.2.2. Combinations of solvers and neural networks are\n\n\nalready suggested in, e.g., NeuralODE (Chen et al., 2018). The novelty of our study is the\n\n\nextension of existing methods for solving PDEs with spatial structure and the incorporation\n\n\nof global pooling into the solver in an E( _n_ )-equivariant way, enabling us to capture global\n\n\ninteraction, which we refer to as the _neural nonlinear solver_ .",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 118,
        "chunk_index": 2,
        "chunk_type": "text",
        "token_count": 93,
        "char_count": 615,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "13edde20-75e5-4c7a-b6db-998c991ae6be",
      "content": "4.4. Numerical Experiments **95**\n\n\nFinally, the update from the state at the _i_ th iteration _**H**_ [[] _[i]_ []] to the ( _i_ + 1)th in the neural\n\n\nnonlinear solver is expressed as:\n\n\n_**H**_ [[] _[i]_ [+1]] = DirichletLayer \u0011 _**H**_ [[] _[i]_ []] _−_ _α_ BB [[] _[i]_ []] \u0003 _**H**_ [[] _[i]_ []] _−_ _**H**_ [[0]] _−D_ NIsoGCN ( _**H**_ [[] _[i]_ []] )∆ _t_ \u0004 [\u0012] _,_ (4.37)\n\n\nwhere _**H**_ [[0]] is the encoded _**U**_ ( _t_ ). Here, Equation 4.37 enforces hidden features to satisfy\n\n\nthe encoded PDE, including boundary conditions, motivating us to call our model _physics-_\n\n\n_embedded neural networks_ because it embeds physics (PDEs) in the model rather than in\n\n\nthe loss.\n\n\n4.4 N UMERICAL E XPERIMENTS\n\n\nUsing numerical experiments, we demonstrate the proposed model’s validity, express\n\nibility, and computational efficiency. We use three types of datasets:\n\n\n1. the gradient dataset to verify the correctness of NIsoGCN; and",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 119,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 154,
        "char_count": 941,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "347d6192-78a0-4881-82fa-3ad038f1bc7e",
      "content": "2. the advection-diffusion dataset to demonstrate capacity of the model for various\n\n\nPDE parameters; and\n\n\n3. the incompressible flow dataset to demonstrate the speed and accuracy of the model.\n\n\nWe also present ablation study results to corroborate the effectiveness of the proposed\n\n\nmethod. The implementation of our model is based on the original IsoGCN’s code. [1] Our\n\n\nimplementation is available online. [2]\n\n\n4.4.1 G RADIENT D ATASET\n\n\nAs done in Section 3.4.1, we conducted experiments to predict the gradient field from\n\n\na given scalar field to verify the expressive power of NIsoGCN.\n\n\n4.4.1.1 T AKS D EFINITION\n\n\nWe generated cuboid-shaped meshes randomly with 10 to 20 cells in the _X_, _Y_, and _Z_\n\n\ndirections. We then generated random scalar fields over these meshes using polynomials of",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 119,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 126,
        "char_count": 807,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "5c7cde5e-4e52-4984-83d9-cb5ef21f7556",
      "content": "1 [https://github.com/yellowshippo/isogcn-iclr2021, Apache License 2.0.](https://github.com/yellowshippo/isogcn-iclr2021)\n2 [https://github.com/yellowshippo/penn-neurips2022, Apache License 2.0.](https://github.com/yellowshippo/penn-neurips2022)",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 119,
        "chunk_index": 2,
        "chunk_type": "text",
        "token_count": 10,
        "char_count": 245,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "068a5501-6a93-4af9-81f5-15c5abf07927",
      "content": "**96** 4. Physics-Embedded Neural Network: Boundary Condition and Implicit Method\n\n\ndegree 10 and computed their gradient fields analytically. Our training, validation, and test\n\n\ndatasets consisted of 100 samples.\n\n\n4.4.1.2 M ODEL A RCHITECTURE\n\n\nFigure 4.2 shows the architectures we used for the gradient dataset. The dataset is up\n\nloaded online. [3] We followed the instruction of Horie et al. (2021) (in particular, Appendix\n\n\nD.1 of their paper) to make the features and models equivariant. To facilitate a fair compar\n\nison, we made input information for both models equivalent, except for _**M**_ _[−]_ [1] in Equation\n\n\nEquation 4.13, which is a part of our novelty. For both models, we used Adam (Kingma &\n\n\nBa, 2014) as an optimizer with the default setting. Training for both models took around\n\n\nten minutes using one GPU (NVIDIA A100 for NVLink 40GiB HBM2). Figure 4.2 shows\n\n\nmodel architectures used for the experiment.\n\n\n\n(a)\n\n\n\n(b)",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 120,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 149,
        "char_count": 950,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "6bc2c4ed-2c3e-4724-b9dc-b4d580794389",
      "content": "|, gˆ|Col2|n|Col4|\n|---|---|---|---|\n|||||\n|MLP<br>2, 8, 16, 16<br>LeakyReLU, LeakyReLU, Identity|MLP<br>2, 8, 16, 16<br>LeakyReLU, LeakyReLU, Identity|MLP<br>2, 8, 16, 16<br>LeakyReLU, LeakyReLU, Identity|MLP<br>2, 8, 16, 16<br>LeakyReLU, LeakyReLU, Identity|\n|||||\n|IsoGCN<br>16, 16<br>Identity|IsoGCN<br>16, 16<br>Identity|IsoGCN<br>16, 16<br>Identity|IsoGCN<br>16, 16<br>Identity|\n|||||\n|Concatenation|Concatenation|Concatenation|Concatenation|\n|||||\n|MLP<br>17, 1<br>Identity|MLP<br>17, 1<br>Identity|MLP<br>17, 1<br>Identity|MLP<br>17, 1<br>Identity|\n|||||\n|_r _|_r _|_r _|_r _|",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 120,
        "chunk_index": 1,
        "chunk_type": "table",
        "token_count": 45,
        "char_count": 584,
        "contains_formulas": false,
        "contains_tables": true,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "29df4b0b-0d57-49cc-83df-15d1f5d1302a",
      "content": "|Col1|Col2|M −1|Col4|\n|---|---|---|---|\n|||||\n|MLP<br>1, 8, 16, 16<br>LeakyReLU, LeakyReLU, Identity|MLP<br>1, 8, 16, 16<br>LeakyReLU, LeakyReLU, Identity|MLP<br>1, 8, 16, 16<br>LeakyReLU, LeakyReLU, Identity|MLP<br>1, 8, 16, 16<br>LeakyReLU, LeakyReLU, Identity|\n|||||\n|NIsoGCN<br>16, 16<br>Identity|NIsoGCN<br>16, 16<br>Identity|NIsoGCN<br>16, 16<br>Identity|NIsoGCN<br>16, 16<br>Identity|\n|||||\n|MLP<br>1, 16<br>Identity|MLP<br>1, 16<br>Identity|MLP<br>1, 16<br>Identity|MLP<br>1, 16<br>Identity|\n|||||\n|_r _|_r _|_r _|_r _|\n\n\n\nFigure 4.2: Architecture used for (a) original IsoGCN and (b) NIsoGCN training. In each\n\n\ntrainable cell, we put the number of units in each layer along with the activation functions\n\n\nused.\n\n\n3 [https://savanna.ritc.jp/˜horiem/penn_neurips2022/data/grad/grad_data.](https://savanna.ritc.jp/~horiem/penn_neurips2022/data/grad/grad_data.tar.gz)\n\n[tar.gz](https://savanna.ritc.jp/~horiem/penn_neurips2022/data/grad/grad_data.tar.gz)",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 120,
        "chunk_index": 2,
        "chunk_type": "table",
        "token_count": 77,
        "char_count": 961,
        "contains_formulas": false,
        "contains_tables": true,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "792c1918-4319-4831-ba30-031c7c0062bf",
      "content": "4.4. Numerical Experiments **97**\n\n\n4.4.1.3 R ESULTS\n\n\nTable 4.1 and Figure 4.3 show that the proposed NIsoGCN improves gradient predic\n\ntion, especially near the boundary, showing that our model successfully considers Neumann\n\n\nboundary conditions.\n\n\nTable 4.1: MSE loss ( _±_ the standard error of the mean) on test dataset of gradient predic\n\ntion. ˆ _g_ Neumann is the loss computed only on the boundary where the Neuman condition is\n\n\nset.\n\n\nMethod _∇φ_ ( _×_ 10 _[−]_ [3] ) _g_ ˆ Neumann ( _×_ 10 _[−]_ [3] )\n\n\nOriginal IsoGCN 192 _._ 72 _±_ 1 _._ 69 1390 _._ 95 _±_ 7 _._ 93\n\n\n**NIsoGCN** (Ours) 6 _._ 70 _±_ 0 _._ 15 3 _._ 52 _±_ 0 _._ 02\n\n立方体内部の矢量場を示した図で、立方体の各辺に平行な矢量が存在し、各矢量の大きさは同じであることが特徴的です。矢量は立方体全体に均一に分布しており、向きも一致しています。\n\n図は3D直方体の内部を示しています。矢印は直方体内に均一に分布し、各点の矢印の向きと大きさは一致しています。これにより、直立方体内のベクトル場が均一であることが示されています。\n\n立方体内部の矢量場を示した図で、矢印の向きは立方体の各面の法線方向に一致し、大きさは立方体内の位置に応じて変化しています。立方体中央付近では矢量の大きさが最大で、四角形の四辺に近づくと矢量が徐々に弱まります。",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 121,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 122,
        "char_count": 929,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "76e3a287-3455-44c3-b99e-64f305d1f7fa",
      "content": "等高線/等値線図は、3D物体の表面の高低を表す図法です。軸は水平方向（x軸）と垂直方向（y軸）、値は表面の高さを示します。等高線上の高低は、表面の傾斜度を反映し、重要な領域（例：最大値や最小値の所在）を示すために、高低の区別が重要です。\n\n画像は立方体の立体図示で、灰色の平面が立方体の面を示しています。立方体の各辺は白色の線で描かれており、平面と立方体の接点は灰色の平面と白色の線の交点で示されています。右側には色調の色板が表示されており、色板の色は灰色から白い順番に変化しています。\n\n\nGround truth Original IsoGCN NIsoGCN\n\nヒートマップの軸は「Gradient magnitude（梯度の大きさ）」で、0.0e+00から4.00e00の範囲を示しています。高集中領域は右上 quadrant（赤色）に位置し、低濃度領域は左下 quadrant（青色）です。中央に「1.5」の値が目立たず、周囲は「0.5」と「2.0」の間で均一に分布しています。\n\nこのグラフは「Difference of gradient magnitude（勾配のmagnitudeの差）」を表しています。縦軸は0から3.0e+00（30）までの値を示し、横軸には変数名がありません。色の深浅は勾配magnitudeの大小を反映し、最大値30に近い深紫色が目立つピークを示しています。全体の傾向は右上に右肩上がりで、最大差30が右端に位置しています。\n\n\nFigure 4.3: Gradient field (top) and the magnitude of error between the predicted gradient\n\n\nand the ground truth (bottom) of a test data sample, sliced on the center of the mesh.",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 121,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 46,
        "char_count": 814,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "4ca60e81-fefd-44ca-8604-f73d01e5e29b",
      "content": "**98** 4. Physics-Embedded Neural Network: Boundary Condition and Implicit Method\n\n\n4.4.2 A DVECTION -D IFFUSION D ATASET\n\n\nTo test the generalization ability of PENNs regarding PDE’s parameters and time series,\n\n\nwe run an experiment with the advection-diffusion dataset.\n\n\n4.4.2.1 T ASK D EFINITION\n\n\nThe governing equation regarding the temperature field _T_ used for the experiment is\n\n\nexpressed as:\n\n\n\n\n\n ( _t,_ _**x**_ ) _∈_ (0 _,_ 1) _×_ Ω (4.38)\n _[· ∇][T]_ [ +] _[ D][∇· ∇][T]_\n\n\n\n_∂T_\n\n_∂t_ [=] _[ −][c]_\n\n\n\n\n\n\n\n\n\n\n\n1\n\n\n0\n\n\n0\n\n\n\n_T_ ( _t_ = 0 _,_ _**x**_ ) = 0 _**x**_ _∈_ Ω (4.39)\n\n\n_T_ = _T_ [ˆ] ( _t,_ _**x**_ ) _∈_ _∂_ Ω Dirichlet (4.40)\n\n\n_∇T ·_ _**n**_ = 0 ( _t,_ _**x**_ ) _∈_ _∂_ Ω Neumann _,_ (4.41)\n\n\nwhere _c ∈_ R is the magnitude of a known velocity field, and _D ∈_ R is the diffusion\n\n\ncoefficient. We set Ω= _{_ _**x**_ _∈_ R [3] _|_ 0 _< x_ 1 _<_ 1 _∧_ 0 _< x_ 2 _<_ 1 _∧_ 0 _< x_ 3 _<_ 0 _._ 01 _}_,",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 122,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 185,
        "char_count": 935,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "a96b28b3-e646-4c69-a872-3bcbe4aaacf6",
      "content": "_∂_ Ω Dirichlet = _{_ _**x**_ _∈_ _∂_ Ω _| x_ 1 = 0 _}_ and _∂_ Ω Neumann = _∂_ Ω _\\ ∂_ Ω Dirichlet .\n\n\n4.4.2.2 D ATASET\n\n\nWe varied _c_ and _D_ from 0.0 to 1.0, eliminating the condition _c_ = _D_ = 0 _._ 0 because\n\nnothing drives the phenomena, and and varied _T_ [ˆ] from 0.1 to 1.0 with ∆ _t_ = 10 _[−]_ [3] We\n\n\ngenerated fine meshes, ran numerical analysis with a classical solver, OpenFOAM, [4] and\n\n\ninterpolated the obtained temperature fields onto coarser meshes so that we can obtain\n\n\nhigh-quality ground truth data. We split the generated data into training, validation, and\n\n\ntest dataset containing 960, 120, and 120 samples. The dataset is uploaded online. [5]\n\n\n4 [https://www.openfoam.com/](https://www.openfoam.com/)\n5 [https://savanna.ritc.jp/˜horiem/penn_neurips2022/data/ad/ad_](https://savanna.ritc.jp/~horiem/penn_neurips2022/data/ad/ad_preprocessed.tar.gz)\n\n[preprocessed.tar.gz](https://savanna.ritc.jp/~horiem/penn_neurips2022/data/ad/ad_preprocessed.tar.gz)",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 122,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 129,
        "char_count": 985,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "9ba738b6-0643-4ee1-9009-6c5d7c5771f4",
      "content": "4.4. Numerical Experiments **99**\n\n\n4.4.2.3 M ODEL A RCHITECTURE\n\n\nThe strategy to construct PENN for the advection-diffusion dataset is consistent with\n\n\none for the incompressible flow dataset (see Section 4.4.3.3). The input features of the\n\n\nmodel are:\n\n\n   - _T_ ( _t_ = 0 _._ 0): The initial temperature field\n\n\n   - _T_ [ˆ] : The Dirichlet boundary condition for the temperature field\n\n\n   - ( _c,_ 0 _,_ 0) _[⊤]_ : The velocity field\n\n\n   - _c_ : The magnitude of the velocity\n\n\n   - _D_ : The diffusion coefficient\n\n\n   - _e_ _[−]_ [0] _[.]_ [5] _[d]_ _, e_ _[−]_ [1] _[.]_ [0] _[d]_ _, e_ _[−]_ [2] _[.]_ [0] _[d]_ : Features computed from _d_, the distance from the Dirichlet\n\n\nboundary\n\n\nand the output features are:\n\n\n   - _T_ ( _t_ = 0 _._ 25): The temperature field at _t_ = 0 _._ 25\n\n\n   - _T_ ( _t_ = 0 _._ 50): The temperature field at _t_ = 0 _._ 50\n\n\n   - _T_ ( _t_ = 0 _._ 75): The temperature field at _t_ = 0 _._ 75",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 123,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 174,
        "char_count": 938,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "6b113cb7-c6af-40b6-901c-f0d850712384",
      "content": "- _T_ ( _t_ = 1 _._ 00): The temperature field at _t_ = 1 _._ 00\n\n\nThe encoded governing equation is expressed as:\n\n\n_**H**_ _T_ ( _t_ + ∆ _t_ ) = _**H**_ _T_ ( _t_ ) + _D_ NIsoGCN;A   - D ( _**H**_ _T_ ) ( _t_ + ∆ _t_ ) (4.42)\n\n\n_D_ NIsoGCN;A  - D ( _**H**_ _T_ ) : = _−_ _**H**_ _**c**_ _·_ NIsoGCN 0 _→_ 1 ( _**H**_ _T_ ) + _**H**_ _D_ NIsoGCN 0 _→_ 1 _→_ 0 ( _**H**_ _T_ ) _,_\n\n\n(4.43)\n\n\nwhere encoded discrete tensor fields corresponds to the following:\n\n\n   - _**H**_ _T_ : Encoded rank-0 discrete tensor field of _T_\n\n\n   - _**H**_ _D_ : Encoded rank-0 discrete tensor field of _c_, _D_, _e_ _[−]_ [0] _[.]_ [5] _[d]_, _e_ _[−]_ [1] _[.]_ [0] _[d]_, and _e_ _[−]_ [2] _[.]_ [0] _[d]_",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 123,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 142,
        "char_count": 690,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "1286d952-978b-415f-aab1-a028e54a6aee",
      "content": "**100** 4. Physics-Embedded Neural Network: Boundary Condition and Implicit Method\n\n\n   - _**H**_ _**c**_ : Encoded rank-1 discrete tensor field of _**c**_\n\n\nThe corresponding neural nonlinear solver is:\n\n\n_**H**_ _T_ [[] _[i]_ [+1]] = _**H**_ _T_ [[] _[i]_ []] _[−]_ _[α]_ BB [[] _[i]_ []] _**H**_ _T_ [[] _[i]_ []] _[−]_ _**[H]**_ _T_ [[0]] _[−D]_ [NIsoGCN;A] [-] [D] [(] _**[H]**_ _T_ [[] _[i]_ []] [)∆] _[t]_ _,_ (4.44)\ni j\n\n\nBecause the task is to predict time series data, we adopt autoregressive architecture for\n\n\nthe nonlinear neural solver, i.e., input the output of the solver of the previous step (which\n\n\nis in the encoded space) to predict the encoded feature of the next step (see Figure 4.4).\n\n\nFigures 4.5 and 4.6 present the detailed architecture of the PENN model for the advection\n\ndiffusion dataset experiment.\n\n\nTo confirm the PENN’s effectiveness, we ran the ablation study on the following set\n\ntings:",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 124,
        "chunk_index": 0,
        "chunk_type": "formula",
        "token_count": 149,
        "char_count": 925,
        "contains_formulas": true,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "cd4e26a9-e37d-43a0-9ffc-3f704dd5cc43",
      "content": "(A) Without encoded boundary: In the nonlinear loop, we decode features to apply\n\n\nboundary conditions to fulfill Dirichlet conditions in the original physical space\n\n\n(B) Without boundary condition in the neural nonlinear solver: We removed the Dirich\n\nlet layer in the nonlinear loop. Instead, we added the Dirichlet layer after the (non\n\npseudoinverse) decoder.\n\n\n(C) Without neural nonlinear solver: We removed the nonlinear solver from the model\n\n\nand used the explicit time-stepping instead\n\n\n(D) Without boundary condition input: We removed the boundary condition from input\n\n\nfeatures\n\n\n(E) Without Dirichlet layer: We removed the Dirichlet layer. Instead, we let the model\n\n\nlearn to satisfy boundary conditions during training.\n\n\n(F) Without pseudoinverse decoder: We removed the pseudoinverse decoder and used\n\n\nsimple MLPs for decoders.\n\n\n(G) Without pseudoinverse decoder with Dirichlet boundary layer after decoding: Same\n\n\nas above, but with Dirichlet layer after decoding.",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 124,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 141,
        "char_count": 988,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "7e1e639f-fbda-40c1-ae4b-6aa24f945159",
      "content": "4.4. Numerical Experiments **101**\n\n\nThe training is performed for up to ten hours using the Adam optimizer for each setting.\n\n\n|T(t = 0.00)|Col2|\n|---|---|\n|Encoding||\n\n\n\n\n\n\n|T95S+aPn34vy8f/cNCzw=<ltexi>H (t = 0.00)<br>T|Col2|\n|---|---|\n|||\n|||\n\n\n\nフローチャートの開始は「Neural Nonlinear Solver」で、終了は「T(t = 1.00)」です。主要な分岐は、各時間ステップ（t = 0.25、0.50など）で「H_T(t)」を計算し、それから「Decoding」を経て「T」を推定する処理です。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.4: The concept of the neural nonlinear solver for time series data with autoregres\n\nsive architecture. The solver’s output is fed to the same solver to obtain the state at the next\n\n\ntime step (bold red arrow). Please note that this architecture can be applied to arbitrary\n\n\ntime series lengths.",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 125,
        "chunk_index": 0,
        "chunk_type": "table",
        "token_count": 90,
        "char_count": 702,
        "contains_formulas": false,
        "contains_tables": true,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "9bbc34b0-500b-48d3-9b44-cc512aa70e48",
      "content": "**102** 4. Physics-Embedded Neural Network: Boundary Condition and Implicit Method\n\n\n\n_T_ ( _t_ = 0 _._ 00) _T_ ˆ ( _c,_ 0 _,_ 0) _[>]_\n\n\n\n\n\n\n\n\n\n\n|T(t = 0.00)|Col2|\n|---|---|\n|||\n\n\n|Tˆ|Col2|\n|---|---|\n|||\n\n\n|c NPK879/fyDGprM=<latexi>D e−0.5d e−1.0d e−2.0d|Col2|\n|---|---|\n|||\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n|(c, 0, 0)>|Col2|\n|---|---|\n|||\n|MLP<br>1, 64, 64<br>tanh, Identity|MLP<br>1, 64, 64<br>tanh, Identity|\n|||",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 126,
        "chunk_index": 0,
        "chunk_type": "table",
        "token_count": 53,
        "char_count": 399,
        "contains_formulas": false,
        "contains_tables": true,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "28b2cf89-90f7-4dba-9a3b-67a845dbd043",
      "content": "|MLP *1 MLP MLP<br>BoundaryEncoder<br>1, 16, 64 1, 64, 64 5, 64, 64<br>(Weight share with *1)<br>LeakyReLU, Identity tanh, Identity tanh, Identity<br>H[0 ](t = 0.00)<br>H[0 ](t) T<br>T<br>H T[i] b9X/uf1t6P+vjVNqTpsrzgwA7LE<laexi>Hˆ T H c H D<br>Neural Nonlinear Solver<br>H[i+1]<br>T<br>After 8 iterations<br>Dirichlet Layer|MLP<br>5, 64, 64<br>tanh, Identity|Col3|\n|---|---|---|\n|BoundaryEncoder<br>(Weight share with *1)<br>Neural Nonlinear Solver<br>After 8 iterations<br>MLP<br>1, 16, 64<br>LeakyReLU, Identity<br>*1<br>MLP<br>1, 64, 64<br>tanh, Identity<br>MLP<br>5, 64, 64<br>tanh, Identity<br>Dirichlet Layer<br>**_H_**[_i_]<br>_T_<br>**_H_**[_i_+1]<br>_T_<br>**_H_**[0]<br>_T_ (_t_ = 0_._00)<br>**_H_**[0]<br>_T_ (_t_)<br> ˆ<br>**_H_**_T_<br>**_Hc_**<br>**_H_**_D_|||\n||||\n\n\n画像には「Pseudoinverse decoder（Weight share with *1）」というブロックが示されており、その下に「T(t = 0.25)」「T(t=0.50)」「T(=0.75)」と「T(=1.00)」という数値が並んでいます。この図は、Pseudoinverseデコーダの入力信号「T(t)」の値を示している様子です。",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 126,
        "chunk_index": 1,
        "chunk_type": "table",
        "token_count": 72,
        "char_count": 955,
        "contains_formulas": false,
        "contains_tables": true,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "6a253437-3a6a-4bae-8203-1ac65c7e75a1",
      "content": "Figure 4.5: The overview of the PENN architecture for the advection-diffusion dataset.\n\n\nGray boxes with continuous (dotted) lines are trainable (untrainable) components. Arrows\n\n\nwith dotted lines correspond to the loop. In each trainable cell, we put the number of units\n\n\nin each layer along with the activation functions used. The bold red arrow corresponds to\n\n\nthe one in Figure 4.4.",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 126,
        "chunk_index": 2,
        "chunk_type": "text",
        "token_count": 60,
        "char_count": 389,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "73b55bf8-dd58-4a01-8182-f90d7bf8488c",
      "content": "フローチャートの開始は「H_T」から始まり、主要な分岐は「MLP」と「Dirichlet Layer」です。処理の流れは、MLPを経過した後は「NIsoGCN_0→1→0」に進み、DIRICHLET LAYERを通じて「H_c」と「H_D」に分岐し、それらを加算し、最後に「Calculate Eq (4.30)」へと繋がります。\n\n\n\n4.4. Numerical Experiments **103**\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n_−_ NIsoGCN 0 _!_ 1 ( _**H**_ _T_ [)]\n\n\n\n\n\n\n\n_−_ _**H**_ _**c**_ _·_ NIsoGCN 0 _!_ 1 ( _**H**_ _T_ [[] _[i]_ []] [)]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n_T_ [[] _[i]_ []] [) =:] _[ D]_ [NIsoGCN;A] _[−]_ [D] [(] _**[H]**_ _T_ [[] _[i]_ []]\n\n\n\n_T_ [)]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.6: The overview of the PENN architecture for the advection-diffusion dataset.\n\n\nGray boxes with continuous (dotted) lines are trainable (untrainable) components. In each\n\n\ntrainable cell, we put the number of units in each layer along with the activation functions\n\n\nused.",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 127,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 93,
        "char_count": 754,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "18286a21-b5d0-4c0b-813f-14660f68622a",
      "content": "**104** 4. Physics-Embedded Neural Network: Boundary Condition and Implicit Method\n\n\n4.4.2.4 R ESULTS\n\n\nTable 4.2 presents the results of the ablation study. We found that the PENN model with\n\n\nall the proposed components achieved the best performance, showing that all the compo\n\nnents we introduced contributed to performance. Because the boundary condition applied\n\n\nis relatively simple compared to the incompressible flow dataset (Section 4.4.3), the con\n\nfiguration without the Dirichlet layer (Model (E)) showed the second best performance;\n\n\nhowever, the fulfillment of the Dirichlet condition of that model is not rigorous.\n\n\nFigures 4.7, 4.8, and 4.9 show the visual comparison of the prediction with the PENN\n\n\nmodel against the ground truth. As seen in the figures, one can see that our model is\n\n\ncapable of predicting time series under various boundary conditions and PDE parameters,\n\n\ne.g., pure advection (Figure 4.7), pure diffusion (Figure 4.8), and mixed advection and",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 128,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 148,
        "char_count": 987,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "f836dba9-1c3d-428a-b3d6-b3629efe18c3",
      "content": "diffusion (Figure 4.9).\n\n\nTable 4.2: MSE loss ( _±_ the standard error of the mean) on test dataset of the advection\n\ndiffusion dataset.\n\n\nˆ\nMethod _T_ ( _×_ 10 _[−]_ [4] ) _T_ Dirichlet ( _×_ 10 _[−]_ [4] )\n\n\n(A) Without encoded boundary 54 _._ 191 _±_ 6 _._ 36 0 _._ 0000 _±_ 0 _._ 0000\n\n\n(B) Without boundary condition\n390 _._ 828 _±_ 24 _._ 58 0 _._ 0000 _±_ 0 _._ 0000\nin the neural nonlinear solver\n\n\n(C) Without neural nonlinear solver 6 _._ 630 _±_ 1 _._ 21 0 _._ 0000 _±_ 0 _._ 0000\n\n\n(D) Without boundary condition input 465 _._ 492 _±_ 26 _._ 47 868 _._ 7009 _±_ 15 _._ 5447\n\n\n(E) Without Dirichlet layer 2 _._ 860 _±_ 2 _._ 46 1 _._ 1703 _±_ 0 _._ 0328\n\n\n(F) Without pseudoinverse decoder 44 _._ 947 _±_ 6 _._ 00 9 _._ 7130 _±_ 0 _._ 1201\n\n\n(G) Without pseudoinverse decoder\n4 _._ 907 _±_ 4 _._ 87 0 _._ 0000 _±_ 0 _._ 0000\nwith Dirichlet layer after decoding\n\n\n**PENN** **1** _**.**_ **795** _±_ 1 _._ 33 0 _._ 0000 _±_ 0 _._ 0000",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 128,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 193,
        "char_count": 943,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "fb39ed3f-b514-48f1-b0e7-909c6c712e4d",
      "content": "4.4. Numerical Experiments **105**\n\nヒートマップの横軸は「t = 0.25」を示し、縦軸には格子線が描かれています。左側に緑色の領域が現れ、右側には深蓝色の区域が広がっています。緑の領域は左端に集中し、右端まで延びています。右側の深蓝色区域は左側の緑域と対比し、左端から右端にかけて均一に分布しています。\n\nヒートマップの軸は縦軸と横軸で、緑色から青色に色が薄くなる方向を示す。高域集中領域は左上隅に位置し、低域は右下隅に分布。右上隅には明るい青色の帯状パターンが目立つ。\n\nヒートマップの横軸は「t = 0.50」を示しています。左側は緑色で、右側は深蓝色で、境界線が明確に区別されています。緑の領域は左側に広がり、深蓝色は右側に集中しています。この配置から、左側の緑が右側の深蓝色へと徐々に変化している様子が示されています。\n\nヒートマップの軸は横軸と縦軸で、緑色と青色が高域、赤色が低域を示します。右上角に緑と青の領域が広がり、左下角に青と赤の領域があります。中央に青色と赤色の境界が明確に分かれています。\n\nヒートマップの横軸は「t = 0.75」を示し、縦軸には格子線が描かれています。左半分は緑色で、右半分が青色です。青色の領域は右端に位置し、左端から右端まで一定の範囲に延びています。この分布から、右端の青色領域が高値の集中領域であることが推測されます。\n\nヒートマップの軸は横軸と縦軸で、左から右、上から下に位置づけられています。高値域は緑色、中間値は青色、低値領域は黄色と赤色に分かれています。右端と上端に赤色が広がり、左端と下端に青色が集まっています。右上隅に黄色の区域が目立っています。\n\nヒートマップの横軸は「Temperature（温度）」を示しています。左側の図では、温度が0.0から1.0の範囲で緑色が広がり、右側では緑が薄く、右端に薄い青色の領域が現れています。右図は左図と同様に温度軸を示し、緑の分布と右端の青色領域が一致しています。\n\nヒートマップの横軸は「Temperature（温度）」です。左から右へ温度が上昇します。右端に「1.0」と刻まれ、左端には「0.0」と刻まれています。",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 129,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 18,
        "char_count": 929,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "98e84f66-c0f8-4460-8744-065c83e7a395",
      "content": "高域（緑色）と低域（青色・深青色）が明確に区別され、右端の高域が目立っています。左端の低域は薄い青色で、中央から右に広がる青色域が特徴的です。\n\n\nFigure 4.7: Visual comparison on a test sample between (left) ground truth obtained from\n\n\nOpenFOAM computation with fine spatial-temporal resolution and (right) prediction by\n\nPENN. Here, _c_ = 0 _._ 9, _D_ = 0 _._ 0, and _T_ [ˆ] = 0 _._ 4.",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 129,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 44,
        "char_count": 315,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "6ee3f3fa-d9fa-46ba-9e3c-1c5ef4636e94",
      "content": "**106** 4. Physics-Embedded Neural Network: Boundary Condition and Implicit Method\n\nヒートマップの横軸は「t = 0.25」を示し、縦軸には格子線が描かれています。左側は緑色で、右側は深蓝色に色が変化しています。左から右へと色が深く濃くなる様子が目立っています。\n\nヒートマップの軸は横軸と縦軸で、緑色から深蓝色に色が変化しています。高域集中領域は左上隅と右下隅に位置し、低域は中央から右上に分布しています。右上隅に「2023」の数字が目立っています。\n\nヒートマップの横軸は「t = 0.50」を示し、縦軸には格子線が描かれています。左側の緑色領域は低値域で、右側の深蓝色領域は高値区域に集中しています。中央の青色領域が中間値を示しています。全体的に左から右へ値が上昇する傾向が見られます。\n\nヒートマップの軸は横軸と縦軸で、緑色から青色に色が深くなるように配置されています。高値域は右上隅に集中的に分布し、左下隅には低値領域が広がっています。中央部分には目立たない平坦な区域が存在します。\n\nヒートマップの横軸は「t = 0.75」を示し、縦軸には緑色から青色、再到深蓝色の色段階が存在します。左端（緑）から右端（深blue）に移動する過程で、色の濃度が徐々に低下し、右端に集中的に深蓝色が広がっています。この様子は、時間tが0から1に近づくにつれて、特定の領域（右端）に熱量（濃度）が高まっていくことを示しています。\n\nヒートマップの軸は横軸と縦軸で、緑色から青色に色が深まる順序で数字を示しています。高値域は右上 quadrant（右上隅）に集中的に、左下 quadrant（左下隅）は低値領域に分布しています。中央部分は色が薄く、値の変動が見られません。\n\nヒートマップの横軸は「温度」を示し、縦軸には時間（t = 1.00）を表します。左側の緑色領域は温度が低く、右側の深蓝色領域は高温域です。左から右へ温度が徐々に上昇する傾向が見られます。\n\nヒートマップの軸は「Temperature（温度）」で、0.0から1.0の範囲を示しています。高温度域（緑色）は右上隅に集中的に分布し、左下隅（低温度域）は青色で広がっています。中央部分（茶色）が中程度の温度を示し、周囲の色差が目立たない状況です。",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 130,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 28,
        "char_count": 995,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "375915f1-d576-439f-8272-4035ec97b478",
      "content": "Figure 4.8: Visual comparison on a test sample between (left) ground truth obtained from\n\n\nOpenFOAM computation with fine spatial-temporal resolution and (right) prediction by\n\nPENN. Here, _c_ = 0 _._ 0, _D_ = 0 _._ 4, and _T_ [ˆ] = 0 _._ 3.",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 130,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 43,
        "char_count": 241,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "d3021257-cee1-412f-9419-ded747645aa9",
      "content": "4.4. Numerical Experiments **107**\n\nヒートマップの横軸は「t = 0.25」を示し、縦軸に格子線が配置されています。左から右に色が変化し、赤色から黄色、緑色、青色、深蓝色へと移動します。右端の深蓝色領域が最も高値域に集中し、左端の赤色領域が最低値領域に集中しています。中央の青色領域は中間値を示す位置に位置しています。\n\nヒートマップの軸は縦と横の格子線で構成され、左上から右下への色の変化を示しています。高値域は右上隅から左下隅にかけて、右上部が最も高めで、左下部が低めに分布しています。左上隅と右下隅は深蓝色で、中央部分は緑色から黄色色に色段階的に変化しています。\n\nヒートマップの横軸は「t = 0.50」を示し、縦軸には格子線が配置されています。左端から右端にかけて、赤色から青緑色に色が変化し、色の深浅に応じて値の高低が示されています。右端の色が最も深く、左端が最も浅く、この色の変化に従って高値域と低値域能見分けることができます。\n\nヒートマップの軸は横軸と縦軸で、色の深浅が値の大小を示します。高値域は赤色・橙色に集中し、低温域は青色・緑色に集まります。右上角の赤色区域が最高値領域で、左下角の青色領域が最低値区域に位置しています。\n\nヒートマップの横軸は「t = 0.75」を示し、縦軸には格子線が配置されています。左端から右端にかけて、赤色から緑色に色が変化しています。右端の緑地帯が最も高値域に集中し、左端の赤色地帶が低値領域に位置しています。中央から右にかけての橙黄色帯は中間値を示しています。\n\nヒートマップの軸は横軸と縦軸で、緑色から赤色に色が変化します。高値域は右上隅に集まり、左下隅が低値領域です。中央に黄色の帯が現れ、その周囲に緑と赤の色が交互に分布しています。\n\nヒートマップの横軸は「温度」を示し、縦軸には「t = 1.00」が記載されています。色の濃度は温度の高低を反映し、左から右へ温度が上昇する傾向が見られます。右端の色が最も鮮やかで、温度の高い領域が示されています。\n\nヒートマップの軸は「Temperature（温度）」で、0.0から1.0の範囲を示しています。高温度域は右端の黄色から橙色に移行し、左端の赤色に近い黄色域が低温度域です。中央の橙色帯は温度の変化が緩やかに現れています。",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 131,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 20,
        "char_count": 984,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "044349ce-3a7d-4fdf-9508-4d1481609364",
      "content": "Figure 4.9: Visual comparison on a test sample between (left) ground truth obtained from\n\n\nOpenFOAM computation with fine spatial-temporal resolution and (right) prediction by\n\nPENN. Here, _c_ = 0 _._ 6, _D_ = 0 _._ 3, and _T_ [ˆ] = 0 _._ 8.",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 131,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 43,
        "char_count": 241,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "52cadb7a-29be-4081-a565-d0b535327b8a",
      "content": "**108** 4. Physics-Embedded Neural Network: Boundary Condition and Implicit Method\n\n\n4.4.3 I NCOMPRESSIBLE F LOW D ATASET\n\n\nWe tested the expressive power our model by learning incompressible flow in complex\n\n\nshapes.\n\n\n4.4.3.1 T ASK D EFINITION\n\n\nThe incompressible Navier–Stokes equations, the governing equations of incompress\n\nible flow, are expressed as:\n\n\n\n_∂_ _**u**_\n\n\n\n( _t,_ _**x**_ ) _∈_ (0 _, T_ ) _×_ Ω (4.45)\nRe _[∇· ∇]_ _**[u]**_ _[ −∇][p]_\n\n\n\n_∂_ _**u**_ [1]\n\n_∂t_ [=] _[ −]_ [(] _**[u]**_ _[ · ∇]_ [)] _**[u]**_ [ +]\n\n\n\n_**u**_ = ˆ _**u**_ ( _t,_ _**x**_ ) _∈_ _∂_ Ω [(] Dirichlet _**[u]**_ [)] (4.46)\n\u0003 _∇_ _**u**_ + ( _∇_ _**u**_ ) _[T]_ [\u0004] _**n**_ = **0** ( _t,_ _**x**_ ) _∈_ _∂_ Ω [(] Neumann _**[u]**_ [)] _[.]_ (4.47)\n\n\nWe also consider the following incompressible condition:\n\n\n_∇·_ _**u**_ = 0 ( _t,_ _**x**_ ) _∈_ (0 _, T_ ) _×_ Ω _,_ (4.48)\n\n\nwhich may be problematic when solving these equations numerically. Therefore, it is com",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 132,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 165,
        "char_count": 959,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "864e1dfc-f1f7-4fde-b873-efa087807d4d",
      "content": "mon to divide the equations into two: one to obtain pressure and one to compute velocity.\n\n\nThere are many methods to make such a division; for instance, the fractional step method\n\n\nderives the Poisson equation for pressure as follows:\n\n\n_∇· ∇p_ ( _t_ + ∆ _t,_ _**x**_ ) = [1] (4.49)\n\n∆ _t_ [(] _[∇·]_ [ ˜] _**[u]**_ [)(] _[t,]_ _**[ x]**_ [)] _[,]_\n\n\nwhere\n\n\n˜\n_**u**_ = _**u**_ _−_ ∆ _t_ _**u**_ _· ∇_ _**u**_ _−_ [1] (4.50)\n\u0013 Re _[∇· ∇]_ _**[u]**_ \u0014\n\n\nis called the intermediate velocity. Once we solve the equation, we can compute the time\n\n\nevolution of velocity as follows:\n\n\n_**u**_ ( _t_ + ∆ _t,_ _**x**_ ) = ˜ _**u**_ ( _t,_ _**x**_ ) _−_ ∆ _t∇p_ ( _t_ + ∆ _t,_ _**x**_ ) _._ (4.51)",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 132,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 132,
        "char_count": 692,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "1d682477-a9f6-4076-9ea5-ebbc2c7e07a7",
      "content": "4.4. Numerical Experiments **109**\n\n\nBecause the fractional step method requires solving the Poisson equation for pressure,\n\n\nwe also need the boundary conditions for pressure as well:\n\n\n_p_ = 0 ( _t,_ _**x**_ ) _∈_ _∂_ Ω [(] Dirichlet _[p]_ [)] (4.52)\n\n\n_∇p ·_ _**n**_ = 0 ( _t,_ _**x**_ ) _∈_ _∂_ Ω [(] Neumann _[p]_ [)] _[.]_ (4.53)\n\n\nOur machine learning task is also based on the same assumption: motivating pressure pre\n\ndiction in addition to velocity with boundary conditions of both. The task was to predict\n\n\nflow velocity and pressure fields at _t_ = 4 _._ 0 using information available before numerical\n\n\nanalysis, e.g., initial conditions and the geometries of the meshes.\n\n\n4.4.3.2 D ATASET\n\n\nTo generate the dataset, we first generated pseudo-2D shapes, with one cell in the Z\n\n\ndirection, by changing design parameters, starting from three template shapes. Thereafter,\n\n\nwe performed numerical analysis using OpenFOAM [6] with ∆ _t_ = 10 _[−]_ [3], and the initial",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 133,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 160,
        "char_count": 980,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "652b4631-70ef-4299-85a0-8b89e267a9ea",
      "content": "conditions were the solutions of potential flow, which can be computed quickly and stably\n\n\nusing the classical solver. The linear solvers used were generalized geometric-algebraic\n\n\nmulti-grid for _p_ and the smooth solver with the Gauss–Siedel smoother for _**u**_ .\n\n\nTo confirm the expressive power of the proposed model, we used coarse input meshes\n\n\nfor machine learning models. We generated these coarse meshes by setting cell sizes\n\n\nroughly four times larger than the original numerical analysis. We obtained ground truth\n\n\nvariables using interpolation. Training, validation, and test datasets consisted of 203, 25,\n\n\nand 25 samples, respectively. We generated the dataset by randomly rotating and translat\n\ning test samples to monitor the generalization ability of machine learning models.\n\n\nWe generated numerical analysis results using various shapes of the computational do\n\nmain, starting from three template shapes and changing their design parameters as shown",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 133,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 141,
        "char_count": 976,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "973061fc-acf9-4c23-aeb5-4b076917e82e",
      "content": "in Figure 4.10. For each design parameter, we varied from 0 to 1.0 with a step size of 0.1,\n\n\nyielding 11 shapes for type A and 121 shapes for type B and C. The boundary conditions\n\n\nwere set as shown in Figures 4.11 and 4.12. These design and boundary conditions were\n\n\n6 [https://www.openfoam.com/](https://www.openfoam.com/)",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 133,
        "chunk_index": 2,
        "chunk_type": "text",
        "token_count": 53,
        "char_count": 327,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "82815785-95df-47ba-bf17-60e4f796720c",
      "content": "**110** 4. Physics-Embedded Neural Network: Boundary Condition and Implicit Method\n\n\nchosen to have the characteristic length of 1.0 and flow speed of 1.0. The viscosity was set\n\n\nto 10 _[−]_ [3], resulting in Reynolds number Re _∼_ 10 [3] .\n\n\nThe linear solvers used were generalized geometric-algebraic multi-grid for _p_ and the\n\n\nsmooth solver with the Gauss–Siedel smoother for _**u**_ . Numerical analysis to generate each\n\n\nsample took up to one hour using CPU one core (Intel Xeon CPU E5-2695 v2@2.40GHz).\n\n\nThe dataset is uploaded online. [7]\n\n\n\n画像はIntel Xeon CPU E5-2695 v2 @ 2.60GHzの単一コアの動作を示すブロック図です。図面上に灰色の矩形が示され、各矩形には「a1」や「b2」という文字が記載され、矩形の左右方向に青い矢印が示されています。矩形の高さは1.0と示されています。\n\n画像は3つのブロックA、B、Cを示しています。各ブロックは縦1.0、横1.0の長方形で、AとBには水平方向の矢印a1、b1、b2が存在し、Cには垂直方向の矢巻きが示されています。\n\n図Bと図Cは灰色の図形を基にし、図Bには「b1」と「b2」、図Cには「c1」と「c2」の単位が示されています。両図には上下の矢印と左右の矢印が存在し、矢印の方向と単位は図の左下の座標系に従って示されています。\n\n\n\n\n\n図Bと図Cは、1.0の高さの灰色矩形を基準に、図Bでは「b1」方向に右向きの青色矢印、図Cでは「c1」方向と「c2」方向に各1.0長さの青色右向き矢印を示す。左下に座標軸「x」「y」「z」を示した。",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 134,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 97,
        "char_count": 992,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "1fdcda0f-edd3-4453-9f56-54dc1c20477e",
      "content": "Figure 4.10: Three template shapes used to generate the dataset. _a_ 1, _b_ 1, _b_ 2, _c_ 1, and _c_ 2 are\n\n\nthe design parameters.\n\n\n7 [https://savanna.ritc.jp/˜horiem/penn_neurips2022/data/fluid/fluid_](https://savanna.ritc.jp/~horiem/penn_neurips2022/data/fluid/fluid_data.tar.gz.parta[a-e])\n\n[data.tar.gz.parta[a-e]](https://savanna.ritc.jp/~horiem/penn_neurips2022/data/fluid/fluid_data.tar.gz.parta[a-e])",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 134,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 28,
        "char_count": 410,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "0619d82b-0912-478f-8a91-6f68e65d0273",
      "content": "4.4. Numerical Experiments **111**\n\n\n\n画像は流体の境界条件を示す図で、図Aと図Bの境界面上に「u = 0」を示し、境界面上の法線方向に「[∇u + (∇u)ᵀ] n = 0」という条件を示しています。図AはL型の境界面、図Bは矩形の境界面を表しています。\n\n\n\n\n\n\n\n\n\n画像は3種類の形状（A、B、C）を示し、各形状の境界面上に「∇u + (∇u)ᵀn = 0」という式が書かれている。形状Aは矩形で、形状BはT字型で、形状Cは長方形で、境界面上の「n」は法線方向を示している。\n\n図Bと図Cは、灰色背景の図形を基にした図示で、図Bは「u = 0」の条件を満たす図形で「[∇u + (∇u)ᵀn = 0]」の式が示されています。図Cも同様の条件と式が示され、左下に座標軸の図示が存在します。\n\n図面上に灰色の図形が表示され、図形の周囲に橙色の線が描かれており、各図形に「u = 0」の文字が書かれています。右側には「[∇u + (∇u)ᵀ]n = 0」という式が示されています。左下には「u = (1, 0, 0)ᵀ」の文字と、X、Y、Z軸を示した座標系の図が描かれています。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nand dotted lines correspond to Dirichlet and Neumann boundaries.\n\n\n\n図面上に灰色の図形が示されており、図形の周囲には「∇p・n = 0」と「p = 0」の文字が書かれています。この図形は流体力学の境界条件を表しており、周囲の法線方向（n）と流体の速度勾配（∇p）の内積がゼロになることを示しています。\n\n\n\n\n\n図Aは単純な図形を示し、図Bと図Cは同じ図形の2種類を示しています。各図の右側に「p = 0」の文字が表示され、左側に「∇p・n = 0」という式が示されています。\n\n図Bと図Cは、灰色背景の図形で囲まれた領域を示し、図Bの図域の右端に「p = 0」の文字が表示され、図Cの左端に同様の文字が示されています。図の左下隅には、X軸とY軸を表す矢印が描かれています。",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 135,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 57,
        "char_count": 914,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "57f362d2-9c28-4537-999e-a45f0b4482b1",
      "content": "図面上に灰色の図形が示され、図形の周囲に赤い虚線が描かれており「p = 0」と「∇p・n = 0」の条件が記載されています。左下に座標軸（x、y、z）の矢印が示されています。この図は「p」に関する条件を用いてデータセットを生成するための主な要素と関係を示しています。\n\nand dotted lines correspond to Dirichlet and Neumann boundaries.",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 135,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 14,
        "char_count": 199,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "662bef59-42e2-426b-9801-9a1d60905397",
      "content": "**112** 4. Physics-Embedded Neural Network: Boundary Condition and Implicit Method\n\n\n4.4.3.3 M ACHINE L EARNING M ODELS\n\n\nWe constructed the PENN model corresponding to the incompressible Navier–Stokes\n\n\nequation. In particular, we adopted the fractional step method, where the pressure field was\n\n\nalso obtained as a PDE solution along with the velocity field. We encoded each feature in a\n\n\n4, 8, or 16-dimensional space. After features were encoded, we applied a neural nonlinear\n\n\nsolver containing NIsoGCNs and Dirichlet layers, reflecting the fractional step method\n\n\n(See Equations 4.50 and 4.51). Inside the nonlinear solver’s loop, we had a subloop that\n\n\nsolved the Poisson equation for pressure, which also reflected the considered PDE (See\n\n\nEquation 4.49). We looped the solver for pressure five times and four or eight times for\n\n\nvelocity. After these loops stopped, we decoded the hidden features to obtain predictions",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 136,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 140,
        "char_count": 934,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "6b0889b8-c642-47c7-9ea0-8d61080ee3e2",
      "content": "for velocity and pressure, using the corresponding pseudoinverse decoders.\n\n\nFor the state-of-the-art baseline model, we selected MP-PDE (Brandstetter et al., 2022)\n\n\nas it also provides a way to deal with boundary conditions. We used the authors’ code [8]\n\n\nwith minimum modification to adapt to the task. We tested various time window sizes such\n\n\nas 2, 4, 10, and 20, where one step corresponds to time step size ∆ _t_ = 0 _._ 1. With changes\n\n\nin time window size, we changed the number of hops considered in one operation of the\n\n\nGNN of the baseline to have almost the same number of hops visible from the model when\n\n\npredicting the state at _t_ = 4 _._ 0. The numbers of hidden features, 32, 64, and 128, were\n\n\ntested. All models were trained for up to 24 hours using one GPU (NVIDIA A100 for\n\n\nNVLink 40GiB HBM2).\n\n\nThe strategy to construct PENN for the incompressible flow dataset is the following:\n\n\n   - Consider the encoded version of the governing equation",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 136,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 168,
        "char_count": 972,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "9b44f570-1197-49f1-8d05-2b5f209407de",
      "content": "- Apply the neural nonlinear solver containing the Dirichlet layer and the NIsoGCN\n\n\nto the encoded equation\n\n\n   - Decode the hidden feature using the pseudoinverse decoder.\n\n\nReflecting the fractional step method, we build PENN using spatial differential operators\n\n\nprovided by NIsoGCN. We use a simple linear encoder for the velocity and the associated\n\n\nDirichlet boundary conditions. For pressure and its Dirichlet constraint, we use a simple\n\n\n8 [https://github.com/brandstetter-johannes/MP-Neural-PDE-Solvers](https://github.com/brandstetter-johannes/MP-Neural-PDE-Solvers)",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 136,
        "chunk_index": 2,
        "chunk_type": "text",
        "token_count": 68,
        "char_count": 581,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "18c7b06c-6167-469d-92f4-4be387a76b57",
      "content": "4.4. Numerical Experiments **113**\n\n\nMLP with one hidden layer. We encode each feature in a 16-dimensional space. After fea\n\ntures are encoded, we apply a neural nonlinear solver containing NIsoGCNs and Dirichlet\n\n\nlayers, reflecting the fractional step method (Equations 4.50 and 4.51).\n\n\nThe encoded equations are expressed as:\n\n\n[NIsoGCN 1 _→_ 0 _◦_ NIsoGCN 0 _→_ 1 ( _**H**_ _p_ )]( _t_ + ∆ _t,_ _**x**_ )\n\n\n\n= [1]\n\n∆ _t_\n\n\n\nNIsoGCN 1 _→_ 0 _**H**_ ˜ _**u**_ ( _t,_ _**x**_ ) (4.54)\ni \u0011 \u0012j\n\n\n\n_**H**_ ˜ _**u**_ := _**H**_ _**u**_ _−_ ∆ _t_ _**H**_ _**u**_ _·_ NIsoGCN 1 _→_ 2 ( _**H**_ _**u**_ )\n\u0015\n\n_−_ [1]\n\n(4.55)\nRe [NIsoGCN] [2] _[→]_ [1] _[ ◦]_ [NIsoGCN] [1] _[→]_ [2] [ (] _**[H]**_ _**[u]**_ [)] \u0016\n\n_**H**_ _**u**_ ( _t_ + ∆ _t,_ _**x**_ ) = _**H**_ [˜] _**u**_ ( _t,_ _**x**_ ) _−_ ∆ _t_ NIsoGCN 0 _→_ 1 ( _**H**_ _p_ ) ( _t_ + ∆ _t,_ _**x**_ ) _,_ (4.56)\n\n\nwhere _**H**_ _**u**_ is the encoded rank-1 discrete tensor field of _**u**_ and _**H**_ _p_ is the encoded rank-0",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 137,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 183,
        "char_count": 983,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "08dc7994-6fdd-413c-a4d4-ea9701ed4e29",
      "content": "discrete tensor field of _p_ . Note that these equations correspond to Equations 4.49, 4.50,\n\n\nand 4.51, by regarding IsoGCNs as spatial derivative operators. The corresponding neural\n\n\nnonlinear solvers are expressed as:\n\n\n_**H**_ _**u**_ [[] _[i]_ [+1]] = _**H**_ _**u**_ [[] _[i]_ []] _[−]_ _[α]_ BB [[] _[i]_ []] \u0003 _**H**_ _**u**_ [[] _[i]_ [)]] _[−]_ _**[H]**_ _**u**_ [(0)] _[−D]_ [NIsoGCN;NS] \u0001 _**H**_ _**u**_ [[] _[i]_ [)]] _[,]_ _**[ H]**_ _p_ [[] _[i]_ [+1]] \u0002 ∆ _t_ \u0004 (4.57)\n\n\n_D_ NIsoGCN;NS \u0001 _**H**_ _**u**_ [[] _[i]_ []] _[,]_ _**[ H]**_ _p_ [[] _[i]_ [+1]] \u0002 := _**H**_ _**u**_ [[] _[i]_ []] _[·]_ [ NIsoGCN] [1] _[→]_ [2] \u0001 _**H**_ _**u**_ [[] _[i]_ []] \u0002\n\u0015\n\n\n\nfor _**H**_ _**u**_ and\n\n\n\n_−_ [1] _**H**_ [[] _[i]_ []]\n\n\u0001 _**u**_ \u0002\nRe [NIsoGCN] [2] _[→]_ [1] _[ ◦]_ [NIsoGCN] [1] _[→]_ [2]\n\n+ NIsoGCN \u0001 _**H**_ _p_ [[] _[i]_ [+1]] \u0002 [\u0016] _,_ (4.58)",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 137,
        "chunk_index": 1,
        "chunk_type": "formula",
        "token_count": 151,
        "char_count": 863,
        "contains_formulas": true,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "6a6a1eea-e545-4f7d-aefc-ea3be75d60e0",
      "content": "_**H**_ _p_ [[] _[i]_ [;] _[j]_ [+1]] = _**H**_ _p_ [[] _[i]_ [;] _[j]_ []] _−_ _α_ BB [[] _[i]_ [;] _[j]_ []] _[D]_ [NIsoGCN;pressure] [(] _**[H]**_ _p_ [[] _[i]_ [;] _[j]_ []] ) (4.59)\n\n\n\n1\n_D_ _**H**_ [[] _[i]_ [;] _[j]_ []] : = _**H**_ [(;] _[,j]_ [)]\nNIsoGCN;pressure \u0001 _p_ \u0002 ∆ _t_ [NIsoGCN] [1] _[→]_ [0] _[ ◦]_ [NIsoGCN] [0] _[→]_ [1] \u0001 _p_ \u0002\n\u0013\n\n\n\nˆ\n\n_−_ [1] _**h**_ [[] _**u**_ _[i]_ []] _,_ (4.60)\n\n∆ _t_ [NIsoGCN] [1] _[→]_ [0] \u0011 \u0012 [\u0014]",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 137,
        "chunk_index": 2,
        "chunk_type": "text",
        "token_count": 88,
        "char_count": 445,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "400d0be5-e2e2-4779-8e74-1e391483f497",
      "content": "**114** 4. Physics-Embedded Neural Network: Boundary Condition and Implicit Method\n\n\nfor _**H**_ _p_, where _**H**_ _**u**_ [[0]] [=] _**[ H]**_ _**u**_ [(] _[t]_ [)][,] _**[ H]**_ _p_ [[0;0]] = _**H**_ _p_ ( _t_ ), and _**H**_ _p_ [[] _[i]_ [;0]] = _**H**_ _p_ [[] _[i]_ []] [. Figures 4.13, 4.14, and]\n\n\n4.15 present the PENN model architecture used for the incompressible flow dataset.\n\n\nThe input features of the model are:\n\n\n   - _**u**_ ( _t_ = 0 _._ 0): The initial velocity field, the solulsion of potential flow\n\n\n  - ˆ _**u**_ : The Dirichlet boundary condition for velocity\n\n\n   - _p_ ( _t_ = 0 _._ 0): The initial pressure field\n\n\n   - ˆ _p_ : The Dirichlet boundary condition for pressure\n\n\n   - _e_ _[−]_ [0] _[.]_ [5] _[d]_ _, e_ _[−]_ [1] _[.]_ [0] _[d]_ _, e_ _[−]_ [2] _[.]_ [0] _[d]_ : Features computed from _d_, the distance from the wall bound\n\nary condition\n\n\nand the output features are:\n\n\n   - _**u**_ ( _t_ = 4 _._ 0): The velocity field at _t_ = 4 _._ 0",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 138,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 175,
        "char_count": 980,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "89ddcbe8-e175-468a-96c6-eabbc63527e6",
      "content": "- _p_ ( _t_ = 4 _._ 0): The pressure field at _t_ = 4 _._ 0\n\n\nAs seen in Figure 4.14, we have a subloop that solves the Poisson equation for pressure\n\n\nin the nonlinear solver’s loop for velocity. We looped the solver for pressure five times and\n\n\neight times for velocity. After these loops stopped, we decoded the hidden features to obtain\n\n\npredictions for velocity and pressure, using the corresponding pseudoinverse decoders.\n\n\nTo facilitate the smoothness of pressure and velocity fields, we apply GCN layers cor\n\nresponding to numerical viscosity in the standard numerical analysis method. Here, please\n\n\nnote that the PENN model consists of components that accept arbitrary input lengths, e.g.,\n\n\npointwise MLPs, deep sets, and NIsoGCNs. Thanks to the model’s flexibility, we can apply\n\n\nthe same model to arbitrary meshes similar to other GNNs.\n\n\n4.4.3.4 T RAINING D ETAILS\n\n\nBecause the neural nonlinear solver applies the same layers many times during the loop,",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 138,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 157,
        "char_count": 972,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "39c47c6b-9e9c-40bf-87d1-ecc09670ddcf",
      "content": "the model behaved somehow similar to recurrent neural networks during training, which\n\n\ncould cause instability. To avoid such unwanted behavior, we simply retried training by\n\n\nreducing the learning rate of the Adam optimizer by a factor of 0.5. We found our way of",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 138,
        "chunk_index": 2,
        "chunk_type": "text",
        "token_count": 43,
        "char_count": 266,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "77d2d3a5-2b52-4b08-a572-26b8ba44e6a5",
      "content": "4.4. Numerical Experiments **115**\n\n\n\nブロック図の主要コンポーネントは、MLP（多層パーソナルニューラルネットワーク）、境界エンコーダ（Boundary Encoder）、IsogCN（Isotropic Graph Convolutional Network）、ディリクレ層（Dirichlet Layer）、擬逆解碼器（Pseudoinverse decoder）で構成されています。信号の入出力関係は、入力信号u(t=0.0)とû(t)をMLP1（1, 16）で処理し、出力信号H[u]^[0]とĤ[u]^[i]を生成します。同様にp(t)とp̃(t)=e^{-0.5d}e^{-1.0d}-e^{-2.0}dの信号もMLP2（1、8、16）、Boundary Encoder（Weight share with *2）を経過し、H[p]^[[0]]とH^{\\hat{p}}_p^[i+1]を出力します。これらの信号は、Neural Nonlinear Solverに送り込まれ、8回の iteration 后にH[u]^{\\hat{i+1}}とH[p]^{\\tilde{i}}が生成されます。次に、各信号はディリクリッド層を通じて再処理会され、最後にU(t=4.0)=u(t)、P(t)=p(t)=U(t)の形式で出力されます。\n\n\n\nブロック図の上部に「u(t = 0.0)」という入力信号があり、この信号はMLP（多層パーセプトロン）に送られます。MLPの入力層は1個で、出力層には16個のニューラル要素が存在し、この層は「Identity」（単純な線形変換）として機能します。\n\n\n\n\n\nブロック図の上部に「\\(\\hat{u}\\)」という信号が入力され、その信号は「BoundaryEncoder（Weight share with *1）」というコンポーネントに送られる。BoundaryEncoderはこの信号を処理し、出力する関係が示されています。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n|H[0]<br>u|Col2|\n|---|---|\n|||\n\n\n|Col1|Col2|\n|---|---|\n|||",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 139,
        "chunk_index": 0,
        "chunk_type": "formula",
        "token_count": 33,
        "char_count": 944,
        "contains_formulas": true,
        "contains_tables": true,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "4437d934-2bb3-4957-b2d3-686a70950620",
      "content": "Figure 4.13: The overview of the PENN architecture for the incompressible flow dataset.\n\n\nGray boxes with continuous (dotted) lines are trainable (untrainable) components. Arrows\n\n\nwith dotted lines correspond to the loop. In each trainable cell, we put the number of units\n\n\nin each layer along with the activation functions used.\n\n\ntraining useful compared to using the learning rate schedule because sometimes the loss\n\n\nvalue of PENN can be extremely high, resulting in difficulty to reach convergence with\n\n\na lower learning rate after such an explosion. Therefore, we applied early stopping and\n\n\nrestarted training using a lower learning rate from the epoch with the best validation loss.\n\n\nOur initial learning rate was 5 _._ 0 _×_ 10 _[−]_ [4], and we restarted the training twice, which\n\n\nwas done automatically, within the 24-hour training period of PENN. For the ablation\n\n\nstudy, we used the same setting for all models. For PENN and ablation models, we used",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 139,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 154,
        "char_count": 971,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "f46c5b49-7a11-4f1d-a920-e7bb602782e7",
      "content": "Adam (Kingma & Ba, 2014) as an optimizer. For MP-PDE solvers, we used the default\n\n\nsetting written in the paper and the code.",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 139,
        "chunk_index": 2,
        "chunk_type": "text",
        "token_count": 23,
        "char_count": 126,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "43e48400-795c-4168-9b9b-86b2bd75ce8f",
      "content": "**116** 4. Physics-Embedded Neural Network: Boundary Condition and Implicit Method\n\n\n\nブロック図の主要コンポーネントは、多層パーソナルニューラルネットワーク（MLP）、ディリクレ層（DirichletLayer）、NISoGCN（Neural Isotropic Spatial Graph Convolutional Network）、ガウシアンカーネル（GCN）などです。信号の入出力関係は、入力層から出力層へと進む際、各層間で「Multiplcation（乗算）」「Concatenation（接合）」「Addition（加算）」などの演算が行われ、最終的に「Neural Nonlinear Solver for Pressure Poisson Equation（圧力ポアソン方程式のニュートラル非線形解法器）」に到達します。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n|Col1|Col2|Col3|\n|---|---|---|\n||||\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.14: The neural nonlinear solver for velocity. Gray boxes with continuous (dot\n\nted) lines are trainable (untrainable) components. Arrows with dotted lines correspond to\n\n\nthe loop. In each trainable cell, we put the number of units in each layer along with the\n\n\nactivation functions used.",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 140,
        "chunk_index": 0,
        "chunk_type": "table",
        "token_count": 71,
        "char_count": 887,
        "contains_formulas": false,
        "contains_tables": true,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "96ec9915-13d2-457f-a5a9-8a8f01bd7bef",
      "content": "4.4. Numerical Experiments **117**\n\n\n\n\n\n\n\n\n|H[i;j]<br>p|Col2|\n|---|---|\n|||\n\n\n|Hˆ<br>p|Col2|\n|---|---|\n|||\n\n\n|Dirichlet Layer|Col2|\n|---|---|\n|||\n\n\n|NIsoGCN<br>0→1→0<br>16, 16, 16<br>tanh, Identity|Col2|\n|---|---|\n||_r · r_**_H_**[_i_<br>_p_|\n\n\n|1 r :· H˜[i]<br>∆t u|Col2|\n|---|---|\n|||\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n:= _−D_ NIsoGCN;pressure ( _**H**_ _p_ [[] _[i]_ [;] _[j]_ []] )\n\n\n|Addition|Col2|\n|---|---|\n||_−_<br>✓<br>_r · r_**_H_**[_i_;_j_]<br>_p_<br>_−_1<br>∆_tr ·_ ˆ<br>**_H_**[_i_]<br>**_u_**<br>◆<br>:=_ −D_|\n\n\n\n\n\n\n\n_p_ )\n\n\n\n\n\n\n|Calculate Eq (9)|Col2|\n|---|---|\n||**_H_**[_i_;_j_]<br>_p_<br>_−↵_[_i_;_j_]<br>BB _D_NIsoGCN;pressure(**_H_**|\n\n\n\n\n\n\n\nFigure 4.15: The neural nonlinear solver for pressure. Gray boxes with continuous (dotted)\n\n\nlines are trainable (untrainable) components. In each trainable cell, we put the number of\n\n\nunits in each layer along with the activation functions used.",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 141,
        "chunk_index": 0,
        "chunk_type": "table",
        "token_count": 94,
        "char_count": 892,
        "contains_formulas": false,
        "contains_tables": true,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "5362f87c-536b-46be-9df6-71a78bb0757e",
      "content": "**118** 4. Physics-Embedded Neural Network: Boundary Condition and Implicit Method\n\n\n4.4.3.5 R ESULTS\n\n\nTable 4.3 and Figure 4.16 show the comparison between MP-PDE and PENN. The pre\n\ndictive performances of both models are at almost the same level when evaluated on the\n\n\noriginal test dataset. The results show the great expressive power of the MP-PDE model\n\n\nbecause we kept most settings at default as much as possible and applied no task-specific\n\n\ntuning. However, when evaluating them on the transformed dataset, the predictive perfor\n\nmance of MP-PDE significantly degrades. Nevertheless, PENN shows the same loss value\n\n\nup to the numerical error, confirming our proposed components are compatible with E( _n_ )\n\nequivariance. In addition, PENN exhibits no error on the Dirichlet boundaries, showing\n\n\nthat our treatment of Dirichlet boundary conditions is rigorous.\n\n\nFigure 4.17 shows the speed-accuracy trade-off for OpenFOAM, MP-PDE, and PENN.",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 142,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 142,
        "char_count": 956,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "9fea3236-e776-4588-bd66-cb3fda1b1ec3",
      "content": "We varied mesh cell size, the time step size, linear sover settings for OpenFOAM to have\n\n\ndifferent computation speeds and accuracy. The proposed model achieved the best perfor\n\nmance in speed-accuracy trade-off between all the tested methods under fair comparison\n\n\nconditions.\n\n\nTable 4.3: MSE loss ( _±_ the standard error of the mean) on test dataset of incompressible\n\n\nflow. If ”Trans.” is ”Yes,” it means evaluation is done on randomly rotated and transformed\n\n\ntest dataset. ˆ _·_ Dirichlet is the loss computed only on the boundary where the Dirichlet condi\n\ntion is set for each _**u**_ and _p_ . MP-PDE’s results are based on the time window size equaling\n\n\n40 as it showed the best performance in the tested MP-PDEs. For complete results, see\n\n\nTable 4.5.\n\n\n_**u**_ _p_ _**u**_ ˆ Dirichlet _p_ ˆ Dirichlet\nMethod Trans.\n( _×_ 10 _[−]_ [4] ) ( _×_ 10 _[−]_ [3] ) ( _×_ 10 _[−]_ [4] ) ( _×_ 10 _[−]_ [3] )",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 142,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 160,
        "char_count": 916,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "86d9dead-9da1-4bc4-9a99-82aeab482ee7",
      "content": "MP-PDE No **1** _**.**_ **30** _±_ 0 _._ 01 1 _._ 32 _±_ 0 _._ 01 0 _._ 45 _±_ 0 _._ 01 0 _._ 28 _±_ 0 _._ 02\n\nTW = 20\n\nYes 1953 _._ 62 _±_ 7 _._ 62 281 _._ 86 _±_ 0 _._ 78 924 _._ 73 _±_ 6 _._ 14 202 _._ 97 _±_ 3 _._ 81\n\n\nNo 4 _._ 36 _±_ 0 _._ 03 **1** _**.**_ **17** _±_ 0 _._ 01 **0** _**.**_ **00** _±_ 0 _._ 00 **0** _**.**_ **00** _±_ 0 _._ 00\n**PENN** (Ours)\n\nYes **4** _**.**_ **36** _±_ 0 _._ 03 **1** _**.**_ **17** _±_ 0 _._ 01 **0** _**.**_ **00** _±_ 0 _._ 00 **0** _**.**_ **00** _±_ 0 _._ 00",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 142,
        "chunk_index": 2,
        "chunk_type": "text",
        "token_count": 122,
        "char_count": 506,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "49e3f4eb-d384-412b-a201-ca93654f0cbc",
      "content": "4.4. Numerical Experiments **119**\n\nヒートマップの軸は横軸と縦軸で、温度の高低を示します。高温度域は赤色・橙色に、低温域は青色・緑色に分布します。中央に星形の高温度区域が見られ、周囲に低温度域が広がっています。\n\nヒートマップの軸は横軸と縦軸で、温度の高低を示します。高温度域は赤色・橙色で集中的に、低温域は青色・緑色で分布します。中央に明るい色域が現れ、周囲は明るさの変化が見られます。\n\nヒートマップの横軸は「時間」を示し、縦軸には「温度」が刻まれています。高温域（赤・橙色）は上部中央に広がり、低温域（青・緑）は下部に集まります。右側の色条は1.0から1.5e+00の温度範囲を示しています。\n\nヒートマップの軸はX、Y、Zで表され、X軸が右方向、Y軸上方、Z軸下方を示します。高域は赤色で、低域は青色です。中央に黄色と緑色が混在し、周囲に赤色と青色が広がっているため、中心部が高域、周辺が低域のパターンが見られます。\n\nヒートマップの軸は縦軸と横軸です。高域集中領域は右上部に位置し、低域は左下部に分布しています。中央に目立つ赤色区域が特徴的です。\n\n図は流体の速度場を示しています。矢印の方向は速度の方向を示し、大きさは流速の大きさを示します。特徴的な領域は、矢印が密集した部分で流速が速いことがわかります。\n\nヒートマップの軸はZ軸とX軸です。高域は緑色、低域は黄色と赤色に分かれ、中心部に青色の高域が集中しています。右上角と左下角に黄色と青色が混在する区域が目立っています。\n\nヒートマップの軸は横軸と縦軸で、温度の高低を示します。高温度域は緑色、低温度域が黄色から赤色に移行します。各図の中央に藍色の点が目立っており、これが最も高温の集中領域です。右下の図では赤色域が拡大し、中央の藍色点の周囲に赤色が広がっている様子が目立ちます。\n\nヒートマップの横軸は「1.0e+00」から「0.2」まで、色の深浅が値の大小を示します。各図形の中央に深緑・青色の斑点が現れ、周囲は黄色・橙色に広がり、右上角に赤色の高値域が存在します。\n\n\nGround truth MP-PDE (TW=20) PENN (Ours)",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 143,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 19,
        "char_count": 929,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "c2167807-962c-4443-98ec-4aa929fd0166",
      "content": "ヒートマップの軸は「Velocity magnitude（速度の大きさ）」で、0から1.5e+00（150）までの値を示します。紫色から青緑色に色が変化し、紫色部分は速度が最大（約15）で、色が浅くなるほど速度が減少します。右上角の紫色区域が速度の高集中領域で、左下角の青色区域は速度の低集中領域です。\n\nヒートマップの軸は「Pressure（圧力）」で、範囲は-1.0e+00（-100）から10.0（10）です。紫色から黄色に色が変化し、紫色は高圧（正の圧）を示し、黄色は低圧を示します。中央に黄色が集中的に分布し、周辺に紫色が広がっています。\n\nヒートマップの軸はX、Y、Zです。高域は緑・青色、低域は黄色・橙色に分かれ、中央に深青色の高域が現れ、上下に橙色の低域が広がっています。\n\nヒートマップの軸は横軸と縦軸です。高域集中領域は中部に位置し、低域は上部と下部に分布します。右上角と左下角に目立つ赤色区域が特徴的です。\n\nヒートマップの軸は「Pressure（圧力）」です。高圧域は黄色・橙色に、中低域は緑・黄緑に、低域（負圧）は青・深青に分布します。中央に深青色の圧低域が目立っています。\n\n\nFigure 4.16: Comparison of the velocity field (top two rows) and the pressure field (bottom\n\n\ntwo rows) without (first and third rows) and with (second and fourth rows) random rotation\n\n\nand translation. PENN prediction is consistent under rotation and translation due to the\n\n\nE( _n_ )-equivariance nature of the model, while MP-PDE’s predictive performance degrades\n\n\nunder transformations.",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 143,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 63,
        "char_count": 895,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "4cead8c6-bc8c-48ba-b71b-6cb7f96b6ed3",
      "content": "**120** 4. Physics-Embedded Neural Network: Boundary Condition and Implicit Method\n\n\n10 [3]\n\n\n10 [2]\n\n\n10 [1]\n\n\n\nこのグラフは流体シミュレーションの結果を示しています。横軸は時間（秒）を示し、縦軸には流体の速度（m/s）を表しています。各色の図形は異なるシミュレータ（PENN、MP-PDE、OpenFOAMなど）の出力値を示しており、緑色の「PENN（Ours）」が右下に急激に下落し、紫色の「MP・PDE」が中間でピークを上げて右下方に傾斜します。右上に分布した赤色の三角形は「MP·PDE（Trans.）」の結果で、右上隅に集中的に分布しています。全体而言、PENNの結果は右下方で最大値が現れ、MP・MPDEは中央から右下方へと速度が低下し、MP·MPDE（Transformed）は右上方に分布する傾向が見られます。\n\n\n\n10 [0]\n\n10 _[−]_ [4] 10 _[−]_ [3] 10 _[−]_ [2] 10 _[−]_ [1] 10 [0]\n\n\nTotal MSE\n\n\nFigure 4.17: Comparison of computation time and total MSE loss ( _**u**_ and _p_ ) on the test\n\n\ndataset (with and without transformation) between OpenFOAM, MP-PDE, and PENN. The\n\n\nerror bar represents the standard error of the mean. All computation was done using one\n\n\ncore of Intel Xeon CPU E5-2695 v2@2.40GHz. Data used to plot this figure are shown in\n\n\nTables 4.6, 4.7, and 4.8.",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 144,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 100,
        "char_count": 888,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "3ca38557-825e-440b-b207-a1cd0e960399",
      "content": "4.4. Numerical Experiments **121**\n\n\n4.4.3.6 A BLATION S TUDY R ESULTS\n\n\nSimilar to the advection-diffusion dataset case, we validate the effectiveness of our\n\n\nmodel through an ablation study on the following settings:\n\n\n(A) Without encoded boundary: In the nonlinear loop, we decode features to apply\n\n\nboundary conditions to fulfill Dirichlet conditions in the original physical space\n\n\n(B) Without boundary condition in the neural nonlinear solver: We removed the Dirich\n\nlet layer in the nonlinear loop. Instead, we added the Dirichlet layer after the (non\n\npseudoinverse) decoder.\n\n\n(C) Without neural nonlinear solver: We removed the nonlinear solver from the model\n\n\nand used the explicit time-stepping instead\n\n\n(D) Without boundary condition input: We removed the boundary condition from input\n\n\nfeatures\n\n\n(E) Without Dirichlet layer: We removed the Dirichlet layer. Instead, we let the model\n\n\nlearn to satisfy boundary conditions during training.",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 145,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 139,
        "char_count": 959,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "9d4384ab-1f47-4284-89ce-2a4849d5abd8",
      "content": "(F) Without pseudoinverse decoder: We removed the pseudoinverse decoder and used\n\n\nsimple MLPs for decoders.\n\n\n(G) Without pseudoinverse decoder with Dirichlet boundary layer after decoding: Same\n\n\nas above, but with Dirichlet layer after decoding.\n\n\nTable 4.4 presents the results of the ablation study. Comparison between models with\n\n\nand without the proposed components shows that the proposed components, i.e., the bound\n\nary encoder, Dirichlet layer, pseudoinverse decoder, and neural nonlinear solver, signifi\n\ncantly improve the models. The neural nonlinear solver in the encoded space turned out\n\n\nto have the biggest impact on the performance, while the Dirichlet layer ensured reliable\n\n\nmodels that strictly respect Dirichlet boundary conditions.\n\n\nComparison with Model (A) shows that the nonlinear loop in the encoded space is in\n\nevitable for machine learning. This result is quite convincing because if the loop is made in",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 145,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 137,
        "char_count": 938,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "8c8cba12-134b-4cb5-aa26-d3726598aed1",
      "content": "**122** 4. Physics-Embedded Neural Network: Boundary Condition and Implicit Method\n\n\nthe original space, the advantage of the expressive power of the neural networks cannot be\n\n\nleveraged. Comparison with Model (C) confirms that the concept of the solver is effective\n\n\ncompared to simply stacking GNNs, corresponding to the explicit method.\n\n\nIf the boundary condition input is excluded (Model (D)), the performance degrades in\n\n\nline with Brandstetter et al. (2022). That model also has an error on the Dirichlet bound\n\naries. Model (E) shows a similar result, improving performance using the information of\n\n\nthe boundary conditions. If the pseudoinverse decoder is excluded (Model (F)), the out\n\nput may not satisfy the Dirichlet boundary conditions as well. Besides, the decoder has\n\n\nmore effect than expected because PENN is better than Model (G). Both models satisfy\n\n\nthe Dirichlet boundary condition, while PENN has significant improvement. This may be",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 146,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 144,
        "char_count": 962,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "f605133b-6695-47b9-8b63-920615451120",
      "content": "because the pseudoinverse decoder facilitates the spatial continuity of the outputs in addi\n\ntion to the fulfillment of the Dirichlet boundary condition. In other words, using a simple\n\n\ndecoder and the Dirichlet layer after that may cause spatial discontinuity of outputs. Visual\n\n\ncomparison of part of the ablation study is shown in Figure 4.18.\n\n\nTable 4.4: Ablation study on the incompressible flow dataset. The value represents MSE\n\n\nloss ( _±_ standard error of the mean) on the test dataset. ”Divergent” means the implicit\n\n\nsolver does not converge and the loss gets extreme value ( _∼_ 10 [14] ).\n\n\n_**u**_ _p_ _**u**_ ˆ Dirichlet _p_ ˆ Dirichlet\nMethod\n( _×_ 10 _[−]_ [4] ) ( _×_ 10 _[−]_ [3] ) ( _×_ 10 _[−]_ [4] ) ( _×_ 10 _[−]_ [3] )\n\n\nWithout encoded boundary Divergent Divergent Divergent Divergent\n\n\nWithout boundary condition\n65 _._ 10 _±_ 0 _._ 38 21 _._ 70 _±_ 0 _._ 09 0 _._ 00 _±_ 0 _._ 00 0 _._ 00 _±_ 0 _._ 00\nin the neural nonlinear solver",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 146,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 174,
        "char_count": 964,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "248549da-7173-4e4a-b6c8-bfcd1a284b0c",
      "content": "Without neural nonlinear solver 31 _._ 03 _±_ 0 _._ 19 9 _._ 81 _±_ 0 _._ 04 **0** _**.**_ **00** _±_ 0 _._ 00 **0** _**.**_ **00** _±_ 0 _._ 00\n\n\nWithout boundary condition input 20 _._ 08 _±_ 0 _._ 21 3 _._ 61 _±_ 0 _._ 02 59 _._ 60 _±_ 0 _._ 89 1 _._ 43 _±_ 0 _._ 05\n\n\nWithout Dirichlet layer 8 _._ 22 _±_ 0 _._ 07 1 _._ 41 _±_ 0 _._ 01 18 _._ 20 _±_ 0 _._ 28 0 _._ 38 _±_ 0 _._ 01\n\n\nWithout pseudoinverse decoder 8 _._ 91 _±_ 0 _._ 06 2 _._ 36 _±_ 0 _._ 02 1 _._ 97 _±_ 0 _._ 06 **0** _**.**_ **00** _±_ 0 _._ 00\n\n\nWithout pseudoinverse decoder\n6 _._ 65 _±_ 0 _._ 05 1 _._ 71 _±_ 0 _._ 01 **0** _**.**_ **00** _±_ 0 _._ 00 **0** _**.**_ **00** _±_ 0 _._ 00\nwith Dirichlet layer after decoding\n\n\n**PENN** **4** _**.**_ **36** _±_ 0 _._ 03 **1** _**.**_ **17** _±_ 0 _._ 01 **0** _**.**_ **00** _±_ 0 _._ 00 **0** _**.**_ **00** _±_ 0 _._ 00",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 146,
        "chunk_index": 2,
        "chunk_type": "text",
        "token_count": 191,
        "char_count": 843,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "f7baffbc-44ef-4c66-8a29-ad511be9d87f",
      "content": "4.4. Numerical Experiments **123**\n\nヒートマップの軸は横軸と縦軸です。高域は赤色、低域は青色で表示されます。左図の(i)は水平に広がり、右図(ii)は中央に集中的に高域が見られます。右側の図は縦方向に分布し、中央に青色の点が目立っています。\n\nヒートマップの軸は横軸と縦軸で、温度の高低を示します。高集中領域は赤・橙色、低領域は緑・青色です。目立つパターンは、左上角・右下角に高温域が集中し、中央に低温域が現れ、周囲に緑色が広がる状態です。\n\nヒートマップの軸は横軸と縦軸です。高域は赤色、低域は青色で表示されます。各図の中央に明確な赤色斑点が現れ、周囲に青色が広がっている様子が目立っています。\n\nヒートマップの軸は横軸と縦軸です。高集中領域は赤・橙色域で、低領域は緑・青色域です。目立つパターンは、左上・右下・左下・右上に分布し、中央に集中的な青色区域が見られます。\n\nヒートマップの軸は横軸と縦軸です。高域は赤色、低域は青色で表示されます。各図の中央に目立つ赤色斑点が見られます。\n\nヒートマップの軸は横軸と縦軸です。高域集中領域は赤・橙色域で、低域は緑・青色域です。目立つパターンは、縦方向に高域域が広がり、横方向に低域域の中心が移動する様子です。\n\nヒートマップの軸は「u magnitude（uの大きさ）」です。高集中領域は赤・橙色域で、低集中域は緑・青色域です。右上角に「(iv)」と「(iii)」のラベルがあります。\n\nヒートマップの軸は「u」と「p」で、色域は1から1.5e+00（1.2）と-1.0e-00到0.8（0.4）の範囲です。高集中領域は左上角の赤色域と右下角の緑色域で、低集中は中央の青色域です。右上角と左下角に明確なパターンが見られます。\n\n\nFigure 4.18: Visual comparison of the ablation study of (i) ground truth, (ii) the model\n\n\nwithout the neural nonlinear solver (Model (C)), (iii) the model without pseudoinverse",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 147,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 40,
        "char_count": 934,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "ca6134b2-7eb3-4fe7-81aa-cccb221c0baa",
      "content": "decoder with Dirichlet layer after decoding (Model (G)), and (iv) PENN. It can be observed\n\n\nthat PENN improves the prediction smoothness, especially for the velocity field.",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 147,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 26,
        "char_count": 173,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "103c6a34-3afb-47f6-a08b-5671fc3e4dca",
      "content": "**124** 4. Physics-Embedded Neural Network: Boundary Condition and Implicit Method\n\n\n4.4.3.7 D ETAILED R ESULTS\n\n\nTable 4.5 presents the detailed results of the comparison between MP-PDE and PENN.\n\n\nInterestingly, the performance of MP-PDE gets better as the time window size increases.\n\n\nTherefore, our future direction may be to incorporate MP-PDE’s temporal bundling and\n\n\npushforward trick into PENN to enable us to predict the state after a far longer time than\n\n\nwe do in the present work.\n\n\nTables 4.6 and 4.7 show the speed and accuracy of the machine learning models tested.\n\n\nPENN models show excellent performance with a lot smaller number of parameters com\n\npared to MP-PDE models. It is achieved due to efficient parameter sharing in the proposed\n\n\nmodel, e.g., the same weights are used repeatedly in the neural nonlinear encoder. Also, as\n\n\npointed out in Ravanbakhsh et al. (2017), there is a strong connection between parameter",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 148,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 148,
        "char_count": 944,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "a395f105-ce06-4093-8ba1-a0d71809a622",
      "content": "sharing and equivariance. PENN has equivariance in, e.g., permutation, time translation,\n\n\nand E( _n_ ) through parameter sharing, which is in line with them.\n\n\nTable 4.8 presents the speed and accuracy with various settings of OpenFOAM to seek\n\n\na speed-accuracy tradeoff. We tested three configurations of linear solvers:\n\n\n   - Generalized geometric-algebraic multi-grid (GAMG) for _p_ and the smooth solver\n\n\nfor _**u**_\n\n\n   - Generalized geometric-algebraic multi-grid (GAMG) for both _p_ and _**u**_\n\n\n   - The smooth solver for _p_ and _**u**_\n\n\nIn addition, we tested different resolutions for space and time by changing:\n\n\n   - The number of divisions per unit length: 22.5, 45.0, 90.0\n\n\n   - Time step size: 0.001, 0.005, 0.010, 0.050\n\n\nGround truth is computed using the number of divisions per unit length of 90.0 and time\n\n\nstep size of 0.001; thus, this combination is eliminated from the comparison because the\n\n\nMSE error is underestimated (in particular, zero).",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 148,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 147,
        "char_count": 979,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "12338825-3626-4fbe-b29c-ec4dc55e0fe7",
      "content": "4.4. Numerical Experiments **125**\n\n\nTable 4.5: MSE loss ( _±_ the standard error of the mean) on test dataset of incompressible\n\n\nflow. If ”Trans.” is ”Yes”, it means evaluation on randomly rotated and transformed test\n\n\ndataset. _n_ denotes the number of hidden features, _r_ denotes the number of iterations in the\n\n\nneural nonlinear solver used in PENN models, and TW denotes the time window size used\n\n\nin MP-PDE models.\n\n\n_**u**_ _p_ _**u**_ ˆ Dirichlet _p_ ˆ Dirichlet\nMethod Trans.\n( _×_ 10 _[−]_ [4] ) ( _×_ 10 _[−]_ [3] ) ( _×_ 10 _[−]_ [4] ) ( _×_ 10 _[−]_ [3] )\nPENN No 4 _._ 36 _±_ 0 _._ 03 1 _._ 17 _±_ 0 _._ 01 0 _._ 00 _±_ 0 _._ 00 0 _._ 00 _±_ 0 _._ 00\n\n_n_ = 16 _, r_ = 8 Yes 4 _._ 36 _±_ 0 _._ 03 1 _._ 17 _±_ 0 _._ 01 0 _._ 00 _±_ 0 _._ 00 0 _._ 00 _±_ 0 _._ 00\nPENN No 29 _._ 09 _±_ 0 _._ 17 11 _._ 35 _±_ 0 _._ 04 0 _._ 00 _±_ 0 _._ 00 0 _._ 00 _±_ 0 _._ 00",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 149,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 199,
        "char_count": 879,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "e54118e9-e9a2-4ef1-909c-d11b1e484f67",
      "content": "_n_ = 16 _, r_ = 4 Yes 29 _._ 09 _±_ 0 _._ 17 11 _._ 35 _±_ 0 _._ 04 0 _._ 00 _±_ 0 _._ 00 0 _._ 00 _±_ 0 _._ 00\nPENN No 177 _._ 42 _±_ 0 _._ 93 35 _._ 70 _±_ 0 _._ 12 0 _._ 00 _±_ 0 _._ 00 0 _._ 00 _±_ 0 _._ 00\n\n_n_ = 8 _, r_ = 8 Yes 177 _._ 42 _±_ 0 _._ 93 35 _._ 70 _±_ 0 _._ 12 0 _._ 00 _±_ 0 _._ 00 0 _._ 00 _±_ 0 _._ 00\nPENN No 26 _._ 82 _±_ 0 _._ 16 7 _._ 86 _±_ 0 _._ 03 0 _._ 00 _±_ 0 _._ 00 0 _._ 00 _±_ 0 _._ 00\n\n_n_ = 8 _, r_ = 4 Yes 26 _._ 82 _±_ 0 _._ 16 7 _._ 86 _±_ 0 _._ 03 0 _._ 00 _±_ 0 _._ 00 0 _._ 00 _±_ 0 _._ 00\nPENN No 92 _._ 80 _±_ 0 _._ 52 31 _._ 47 _±_ 0 _._ 13 0 _._ 00 _±_ 0 _._ 00 0 _._ 00 _±_ 0 _._ 00\n\n_n_ = 4 _, r_ = 8 Yes 92 _._ 80 _±_ 0 _._ 52 31 _._ 47 _±_ 0 _._ 13 0 _._ 00 _±_ 0 _._ 00 0 _._ 00 _±_ 0 _._ 00\n\nPENN No 120 _._ 35 _±_ 0 _._ 65 35 _._ 53 _±_ 0 _._ 12 0 _._ 00 _±_ 0 _._ 00 0 _._ 00 _±_ 0 _._ 00",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 149,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 264,
        "char_count": 845,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "a27af60a-0ee2-41d0-8cd4-8b11dfefdf13",
      "content": "_n_ = 4 _, r_ = 4 Yes 120 _._ 35 _±_ 0 _._ 65 35 _._ 53 _±_ 0 _._ 12 0 _._ 00 _±_ 0 _._ 00 0 _._ 00 _±_ 0 _._ 00\nMP-PDE No 1 _._ 30 _±_ 0 _._ 01 1 _._ 32 _±_ 0 _._ 01 0 _._ 45 _±_ 0 _._ 01 0 _._ 28 _±_ 0 _._ 02\n\n_n_ = 128 _,_ TW = 20 Yes 1953 _._ 62 _±_ 7 _._ 62 281 _._ 86 _±_ 0 _._ 78 924 _._ 73 _±_ 6 _._ 14 202 _._ 97 _±_ 3 _._ 81\nMP-PDE No 12 _._ 08 _±_ 0 _._ 11 6 _._ 49 _±_ 0 _._ 03 1 _._ 36 _±_ 0 _._ 01 2 _._ 57 _±_ 0 _._ 05\n\n_n_ = 128 _,_ TW = 10 Yes 1468 _._ 12 _±_ 5 _._ 75 192 _._ 97 _±_ 0 _._ 57 767 _._ 17 _±_ 4 _._ 36 51 _._ 87 _±_ 1 _._ 07\nMP-PDE No 32 _._ 07 _±_ 0 _._ 33 6 _._ 22 _±_ 0 _._ 05 0 _._ 85 _±_ 0 _._ 01 0 _._ 92 _±_ 0 _._ 03\n\n_n_ = 128 _,_ TW = 4 Yes 2068 _._ 99 _±_ 8 _._ 30 180 _._ 54 _±_ 0 _._ 57 284 _._ 72 _±_ 1 _._ 69 59 _._ 21 _±_ 1 _._ 32\nMP-PDE No 58 _._ 88 _±_ 0 _._ 60 9 _._ 62 _±_ 0 _._ 07 1 _._ 02 _±_ 0 _._ 02 2 _._ 83 _±_ 0 _._ 10\n\n_n_ = 128 _,_ TW = 2 Yes 1853 _._ 27 _±_ 7 _._ 89 219 _._ 59 _±_ 0 _._ 53 965 _._ 90 _±_ 28 _._ 61 358 _._ 53 _±_ 2 _._ 13",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 149,
        "chunk_index": 2,
        "chunk_type": "text",
        "token_count": 300,
        "char_count": 1000,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "e3b43f1d-cf59-4af5-9072-f65b0cf2bac6",
      "content": "MP-PDE No 6 _._ 09 _±_ 0 _._ 05 5 _._ 39 _±_ 0 _._ 03 1 _._ 65 _±_ 0 _._ 02 2 _._ 16 _±_ 0 _._ 08\n\n_n_ = 64 _,_ TW = 20 Yes 1969 _._ 34 _±_ 7 _._ 50 388 _._ 54 _±_ 1 _._ 12 720 _._ 35 _±_ 5 _._ 15 218 _._ 06 _±_ 8 _._ 01\nMP-PDE No 38 _._ 54 _±_ 0 _._ 32 31 _._ 33 _±_ 0 _._ 09 2 _._ 04 _±_ 0 _._ 02 5 _._ 87 _±_ 0 _._ 09\n\n_n_ = 64 _,_ TW = 10 Yes 2738 _._ 84 _±_ 9 _._ 37 171 _._ 32 _±_ 0 _._ 60 417 _._ 57 _±_ 2 _._ 49 28 _._ 34 _±_ 0 _._ 92\nMP-PDE No 125 _._ 09 _±_ 1 _._ 11 21 _._ 93 _±_ 0 _._ 09 2 _._ 27 _±_ 0 _._ 03 5 _._ 92 _±_ 0 _._ 16\n\n_n_ = 64 _,_ TW = 2 Yes 1402 _._ 01 _±_ 6 _._ 03 435 _._ 75 _±_ 2 _._ 41 384 _._ 30 _±_ 4 _._ 13 57 _._ 26 _±_ 1 _._ 90\nMP-PDE No 32 _._ 46 _±_ 0 _._ 24 17 _._ 40 _±_ 0 _._ 07 5 _._ 92 _±_ 0 _._ 05 5 _._ 94 _±_ 0 _._ 17\n\n_n_ = 32 _,_ TW = 20 Yes 2201 _._ 16 _±_ 7 _._ 59 351 _._ 66 _±_ 0 _._ 82 429 _._ 30 _±_ 3 _._ 27 562 _._ 16 _±_ 11 _._ 62\nMP-PDE No 115 _._ 30 _±_ 1 _._ 01 34 _._ 97 _±_ 0 _._ 15 10 _._ 26 _±_ 0 _._ 09 6 _._ 84 _±_ 0 _._ 14",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 149,
        "chunk_index": 3,
        "chunk_type": "text",
        "token_count": 294,
        "char_count": 990,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "139bbe7a-2176-4004-bbd7-61d22cc1079f",
      "content": "_n_ = 32 _,_ TW = 10 Yes 2824 _._ 76 _±_ 8 _._ 60 496 _._ 33 _±_ 1 _._ 33 2276 _._ 11 _±_ 10 _._ 57 488 _._ 50 _±_ 5 _._ 01\nMP-PDE No 272 _._ 73 _±_ 2 _._ 07 94 _._ 27 _±_ 0 _._ 45 11 _._ 50 _±_ 0 _._ 12 35 _._ 76 _±_ 0 _._ 29\n\n_n_ = 32 _,_ TW = 4 Yes 1973 _._ 35 _±_ 8 _._ 29 554 _._ 69 _±_ 4 _._ 26 647 _._ 31 _±_ 7 _._ 40 157 _._ 85 _±_ 8 _._ 41\nMP-PDE No 794 _._ 90 _±_ 4 _._ 68 82 _._ 61 _±_ 0 _._ 40 50 _._ 23 _±_ 0 _._ 91 31 _._ 41 _±_ 1 _._ 88\n\n_n_ = 32 _,_ TW = 2 Yes 3240 _._ 69 _±_ 21 _._ 91 443 _._ 10 _±_ 2 _._ 56 2885 _._ 30 _±_ 41 _._ 17 562 _._ 08 _±_ 19 _._ 28",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 149,
        "chunk_index": 4,
        "chunk_type": "text",
        "token_count": 168,
        "char_count": 577,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "31cf31d4-1277-48d0-ba10-2e6cbb90f1f9",
      "content": "**126** 4. Physics-Embedded Neural Network: Boundary Condition and Implicit Method\n\n\nTable 4.6: MSE loss ( _±_ the standard error of the mean) of PENN models on test dataset of\n\n\nincompressible flow.",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 150,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 31,
        "char_count": 199,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "2e321db7-a24b-46c9-92f3-8e24780f4f6b",
      "content": "# hidden # iteration in Total MSE",
      "chunk_metadata": {
        "section_title": "hidden # iteration in Total MSE",
        "section_level": 1,
        "page": 150,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 7,
        "char_count": 33,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "28affb86-44cc-4c42-a560-dd4af463d93d",
      "content": "# parameter Total time [s]\n\nfeature the neural nonlinear solver ( _×_ 10 _[−]_ [3] )\n\n\n16 8 8,432 1 _._ 61 _±_ 0 _._ 01 5 _._ 33 _±_ 0 _._ 13\n\n\n16 4 8,432 14 _._ 26 _±_ 0 _._ 03 2 _._ 52 _±_ 0 _._ 06\n\n\n8 8 2,100 53 _._ 44 _±_ 0 _._ 11 3 _._ 54 _±_ 0 _._ 08\n\n\n8 4 2,100 10 _._ 54 _±_ 0 _._ 03 2 _._ 16 _±_ 0 _._ 04\n\n\n4 8 596 40 _._ 75 _±_ 0 _._ 10 2 _._ 86 _±_ 0 _._ 06\n\n\n4 4 596 47 _._ 57 _±_ 0 _._ 10 1 _._ 35 _±_ 0 _._ 04\n\n\nTable 4.7: MSE loss ( _±_ the standard error of the mean) of MP-PDE models on test dataset\n\n\nof incompressible flow.",
      "chunk_metadata": {
        "section_title": "parameter Total time [s]",
        "section_level": 1,
        "page": 150,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 139,
        "char_count": 542,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "b982ad77-b1de-4ef3-8536-d9d34c112987",
      "content": "# hidden Total MSE Total MSE (Trans.)\n\nTime window size # parameter Total time [s]\nfeature ( _×_ 10 _[−]_ [3] ) ( _×_ 10 _[−]_ [3] )\n\n\n128 20 709,316 1 _._ 45 _±_ 0 _._ 01 477 _._ 23 _±_ 0 _._ 77 51 _._ 61 _±_ 1 _._ 41\n\n\n128 10 673,484 7 _._ 70 _±_ 0 _._ 02 339 _._ 78 _±_ 0 _._ 57 94 _._ 01 _±_ 2 _._ 66\n\n\n128 4 651,972 9 _._ 43 _±_ 0 _._ 04 387 _._ 44 _±_ 0 _._ 71 137 _._ 32 _±_ 3 _._ 91\n\n\n128 2 644,548 15 _._ 51 _±_ 0 _._ 07 404 _._ 92 _±_ 0 _._ 67 57 _._ 28 _±_ 1 _._ 91\n\n\n64 20 204,004 6 _._ 00 _±_ 0 _._ 02 585 _._ 48 _±_ 0 _._ 95 13 _._ 62 _±_ 0 _._ 38\n\n\n64 10 185,356 35 _._ 19 _±_ 0 _._ 07 445 _._ 20 _±_ 0 _._ 79 23 _._ 73 _±_ 0 _._ 67\n\n\n64 2 174,740 34 _._ 44 _±_ 0 _._ 10 575 _._ 95 _±_ 1 _._ 76 32 _._ 61 _±_ 1 _._ 02\n\n\n32 20 63,964 20 _._ 64 _±_ 0 _._ 05 571 _._ 77 _±_ 0 _._ 79 7 _._ 64 _±_ 0 _._ 24\n\n\n32 10 55,348 46 _._ 50 _±_ 0 _._ 13 778 _._ 80 _±_ 1 _._ 12 12 _._ 93 _±_ 0 _._ 39\n\n\n32 4 49,948 121 _._ 55 _±_ 0 _._ 35 752 _._ 03 _±_ 3 _._ 07 13 _._ 99 _±_ 0 _._ 41",
      "chunk_metadata": {
        "section_title": "hidden Total MSE Total MSE (Trans.)",
        "section_level": 1,
        "page": 150,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 268,
        "char_count": 986,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "a246d7f3-868f-4076-88c5-d41346ef7560",
      "content": "32 2 47,924 162 _._ 10 _±_ 0 _._ 44 767 _._ 17 _±_ 2 _._ 38 4 _._ 55 _±_ 0 _._ 13",
      "chunk_metadata": {
        "section_title": "hidden Total MSE Total MSE (Trans.)",
        "section_level": 1,
        "page": 150,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 24,
        "char_count": 81,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "d614f529-1d63-4701-b6a1-23d837f5b450",
      "content": "4.4. Numerical Experiments **127**\n\n\nTable 4.8: MSE loss ( _±_ the standard error of the mean) of OpenFOAM computations on\n\n\ntest dataset of incompressible flow.",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 151,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 25,
        "char_count": 161,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "93cfe6fc-2064-4c95-af60-6f744727a477",
      "content": "# division\n\nSolver for _**u**_ Solver for _p_ ∆ _t_ Total MSE ( _×_ 10 _[−]_ [3] ) Total time [s]\nper unit length\n\nGAMG Smooth 22.5 0.050 Divergent Divergent\nGAMG Smooth 22.5 0.010 6 _._ 09 _±_ 0 _._ 02 6 _._ 08 _±_ 0 _._ 17\n\nGAMG Smooth 22.5 0.005 6 _._ 04 _±_ 0 _._ 02 11 _._ 57 _±_ 0 _._ 32\n\nGAMG Smooth 22.5 0.001 4 _._ 80 _±_ 0 _._ 02 51 _._ 43 _±_ 1 _._ 39\n\nGAMG Smooth 45.0 0.050 Divergent Divergent\nGAMG Smooth 45.0 0.010 0 _._ 46 _±_ 0 _._ 00 25 _._ 12 _±_ 0 _._ 81\n\nGAMG Smooth 45.0 0.005 0 _._ 78 _±_ 0 _._ 00 46 _._ 71 _±_ 1 _._ 53\n\nGAMG Smooth 45.0 0.001 1 _._ 04 _±_ 0 _._ 00 201 _._ 11 _±_ 6 _._ 29\n\nGAMG Smooth 90.0 0.050 Divergent Divergent\nGAMG Smooth 90.0 0.010 Divergent Divergent\nGAMG Smooth 90.0 0.005 0 _._ 15 _±_ 0 _._ 00 231 _._ 18 _±_ 10 _._ 38\n\nGAMG GAMG 22.5 0.050 Divergent Divergent\nGAMG GAMG 22.5 0.010 6 _._ 05 _±_ 0 _._ 02 6 _._ 41 _±_ 0 _._ 18\n\nGAMG GAMG 22.5 0.005 6 _._ 00 _±_ 0 _._ 02 12 _._ 21 _±_ 0 _._ 34",
      "chunk_metadata": {
        "section_title": "division",
        "section_level": 1,
        "page": 151,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 216,
        "char_count": 944,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "8c033bc9-5231-4b66-ae3e-49ca672f3e46",
      "content": "GAMG GAMG 22.5 0.001 4 _._ 80 _±_ 0 _._ 02 55 _._ 51 _±_ 1 _._ 52\n\nGAMG GAMG 45.0 0.050 Divergent Divergent\nGAMG GAMG 45.0 0.010 0 _._ 46 _±_ 0 _._ 00 26 _._ 00 _±_ 0 _._ 85\n\nGAMG GAMG 45.0 0.005 0 _._ 77 _±_ 0 _._ 00 48 _._ 78 _±_ 1 _._ 57\n\nGAMG GAMG 45.0 0.001 1 _._ 03 _±_ 0 _._ 00 214 _._ 29 _±_ 6 _._ 62\n\nGAMG GAMG 90.0 0.050 Divergent Divergent\nGAMG GAMG 90.0 0.010 Divergent Divergent\nGAMG GAMG 90.0 0.005 0 _._ 14 _±_ 0 _._ 00 238 _._ 94 _±_ 10 _._ 70\n\nSmooth Smooth 22.5 0.050 Divergent Divergent\nSmooth Smooth 22.5 0.010 5 _._ 59 _±_ 0 _._ 02 85 _._ 50 _±_ 3 _._ 05\n\nSmooth Smooth 22.5 0.005 5 _._ 41 _±_ 0 _._ 02 164 _._ 36 _±_ 7 _._ 57\n\nSmooth Smooth 22.5 0.001 4 _._ 19 _±_ 0 _._ 02 765 _._ 50 _±_ 29 _._ 65\n\nSmooth Smooth 45.0 0.050 Divergent Divergent\nSmooth Smooth 45.0 0.010 51 _._ 10 _±_ 0 _._ 05 426 _._ 07 _±_ 22 _._ 51\n\nSmooth Smooth 45.0 0.005 2 _._ 09 _±_ 0 _._ 00 824 _._ 71 _±_ 39 _._ 90\n\nSmooth Smooth 45.0 0.001 1 _._ 12 _±_ 0 _._ 00 3960 _._ 88 _±_ 151 _._ 93",
      "chunk_metadata": {
        "section_title": "division",
        "section_level": 1,
        "page": 151,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 228,
        "char_count": 987,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "f41f98d7-a0ec-43e3-b9bc-abf53c4e928b",
      "content": "Smooth Smooth 90.0 0.050 Divergent Divergent\nSmooth Smooth 90.0 0.010 Divergent Divergent\nSmooth Smooth 90.0 0.005 4493 _._ 78 _±_ 1 _._ 88 3566 _._ 05 _±_ 183 _._ 75",
      "chunk_metadata": {
        "section_title": "division",
        "section_level": 1,
        "page": 151,
        "chunk_index": 2,
        "chunk_type": "text",
        "token_count": 30,
        "char_count": 166,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "d8fa8451-490c-4942-b1aa-a28714f86d2a",
      "content": "**128** 4. Physics-Embedded Neural Network: Boundary Condition and Implicit Method\n\n\n\nこのグラフは、'PENN (Ours)'と'MP-PDE'のデータを比較する scatter plot です。横軸と縦軸は具体的な変数が示されていないため、具体的な意味は不明ですが、両者の関連性を視覚的に示しています。右上 quadrant に 'PENN' のデータ点が集中的に存在し、左下 quadrant には 'MP-PDE's データ点が分布しています。これにより、右上に配置された点が右下に移動する傾向が見られ、相関関係が示唆されます。具体的なピークや増減の詳細は、軸の単位や範囲が不明瞭なため推測できません。\n\n\n\nFigure 4.19: The relationship between the relative MSE of the velocity _**u**_ and inlet velocity.\n\n\n4.4.3.8 E VALUATION OF O UT - OF -D ISTRIBUTION G ENERALIZATION\n\n\nWe evaluated the out-of-distribution generalizability of PENN and MP-PDE. The mod\n\nels with the best accuracy for each method are used for evaluation. The PENN model has\n\n\n16 hidden features and eight iterations in the neural nonlinear solver, and the MP-PDE\n\n\nmodel has 128 hidden features and a time window size of 20.\n\n\nFirst, we tested generalizability for Reynolds numbers. We varied Reynolds numbers",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 152,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 114,
        "char_count": 923,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "12b05b1a-8c6a-45d3-b10e-7bb0a30f084c",
      "content": "from 500 to 2,000 by changing inlet velocity _u_ inlet from 0.5 to 2.0, while it was 1.0 for the\n\n\ntraining dataset. Figures 4.19 and 4.20 show the generalizability regarding inlet velocities,\n\n\nand Figure 4.21 shows the visualization of velocity fields with inlet velocities of 2.0 and 0.5\n\n\nfor each method. For evaluation, we used relative MSE because the magnitude of features\n\n\nmay differ drastically with inlet velocity change.\n\n\nFrom these figures, one can see that PENN has better accuracy in the lower Reynolds\n\n\nnumber range while almost no difference in the higher Reynolds numbers. That may be\n\n\nbecause PENN can deal with boundary conditions rigorously, and training data may contain\n\n\nsubdomains where the Reynolds number is small locally.\n\n\nThen, generalizability regarding shapes is evaluated. We generated ground truth data\n\n\nwith the same procedure as that to generate the training dataset, except that the analy",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 152,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 145,
        "char_count": 930,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "6dc03091-ae69-4e73-b2be-1abbea683c22",
      "content": "sis domains used here are larger. Figures 4.22, 4.23, and Table 4.9 present the evaluation",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 152,
        "chunk_index": 2,
        "chunk_type": "text",
        "token_count": 15,
        "char_count": 90,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "a0af7dd1-cb1a-42ae-a4ed-f750cd09c433",
      "content": "4.4. Numerical Experiments **129**\n\n\n\n\n\nこのグラフでは、MP-PDE（紅色の×）とPENN（青色の点）のデータを比較しています。横軸は未知の変数を示し、縦軸には値の大小が示されています。PENNのデータは左から右に右肩上がりの傾向を示していますが、MP-DPEは右肩下がりに傾向しており、両者には相関関係が見られません。\n\n\n\nFigure 4.20: The relationship between the relative MSE of the pressure _p_ and inlet velocity.\n\nヒートマップの軸は「u_inlet = 2.0」と「v_inlet」です。高域（黄色・橙色）は流速の急激な変化を示し、低域（青・緑）は安定した流速を表します。中央の赤色区域は流体の集積が最も集中的で、周囲の流速変化が顕著です。\n\nヒートマップの軸は横軸と縦軸で、温度の高低を示します。高温度域は黄色・橙色、低温度域が青・緑です。各図形の中央に目立つ赤色斑点が特徴的で、周囲の温度分布を示しています。\n\n図は流体の速度ベクトル場を示しています。右上部で速度が最大（赤色）で、左下部で最小（青色）に分布し、中央で急激に変化する特徴的な領域が見られます。\n\nヒートマップの軸は「u_inlet」で、値は2.0と0.5に分かれています。高域（黄色・橙色）と低域（青色・緑色）が明確に区別され、両方の値で「Ground truth」と「MI」の図が示されています。低域では「MI」との関連が強調され、高域では両者間の関係が検証される様子が見られます。\n\nヒートマップの軸は「MP-PDE」と「PEN」です。高集中領域はMP-PDENの右上、PENの左下にあります。目立つパターンは、MP-PENの右下に黄色の広範囲が見られます。\n\nヒートマップの軸は「u magnitude（uの大きさ）」です。高集中領域は黄色・橙色域で、低集中域は青色域です。右上角に「PENN (Ours)」と「DE」が示されています。\n\n\nFigure 4.21: The visualization of velocity fields with inlet velocities _u_ inlet of 2.0 and 0.5.",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 153,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 47,
        "char_count": 979,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "25d4538f-74ba-4517-b0d3-a9ec67b7b137",
      "content": "results. Here, we did not observe strong generalizability, such as what was observed in Sec\n\ntion 3.4.2. That may be because the global feature introduced by the neural nonlinear solver\n\n\nhighly depends on the size of the analysis domain, resulting in relatively poor generaliza\n\ntion ability regarding the analysis domain size. The performance degradation is more sig\n\nnificant for pressure field prediction than the velocity because it may have stronger global\n\n\ninteractions through the pressure Poisson equation, which is a static problem introducing\n\n\nglobal interaction.\n\n\n\n折れ線グラフでは、横軸が時間や条件を示し、縦軸に値を表します。変化の傾向は、上昇・下降するかを示しており、ピークは最大値の位置を示します。増減は、値が高くなるか低くなるかを表し、グラフ全体の形状に反映されます。",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 153,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 86,
        "char_count": 685,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "7253af01-3518-4c2f-83cb-ec13a7f8682c",
      "content": "ヒートマップの軸は「uのmagnitude（uの大きさ）」で、色は0.0e+00から2.00e00の範囲を示します。上部の「Ground truth（真の値）」は、黄色から赤色にかけて高値域に集中的に分布し、青色から緑色へと低値領域に移行しています。下部のMP-PDEは、緑から黄緑にかけて中程度の高值域に集中し、紫色から青緑へと極端な値の低域に移動しています。前者は真の分布を反映し、後者は物理的制約に従った近似値を示しています。\n\nヒートマップの軸は「uのmagnitude（uの大きさ）」です。高集中領域は、MP-PDEとPENN（Ours）の上部に黄色・橙色域が広がっている部分で、低領域は下部に青・緑域が集中的に分布している部分です。目立つパターンは、PENNの右端に黄色域が突出している点が特徴的です。\n\nMP-PDEとPENNの速度場を視覚化した図です。MP-PDENの矢印は右上方に指向し、中部で最大の大きさを示しています。PENNでは矢印が右下方に偏向し、左中部で最も大きさが大きい特徴的な領域が見られます。\n\n4.5 C ONCLUSION\n\n\nWe have presented an E( _n_ )-equivariant, GNN-based neural PDE solver, PENN, which\n\n\ncan fulfill boundary conditions required for reliable predictions. The model has superiority\n\n\nin embedding the information of PDEs (physics) in the model and speed-accuracy trade\n\noff. Therefore, our model can be a useful standard for realizing reliable, fast, and accurate\n\n\nGNN-based PDE solvers.\n\n\nTable 4.9: MSE loss ( _±_ the standard error of the mean) on the dataset with larger samples.",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 154,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 81,
        "char_count": 964,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "6002c4a8-3424-4d51-aaa3-567138a738aa",
      "content": "ˆ\n_g_ Neumann is the loss computed only on the boundary where the Neuman condition is set.\n\n\nMethod _**u**_ ( _×_ 10 _[−]_ [3] ) _p_ ( _×_ 10 _[−]_ [2] )\nMP-PDE 10 _._ 335 _±_ 0 _._ 033 **4** _**.**_ **002** _±_ 0 _._ 005\n\n**PENN** (Ours) **4** _**.**_ **132** _±_ 0 _._ 009 9 _._ 621 _±_ 0 _._ 009",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 154,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 63,
        "char_count": 298,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "062efe88-10a0-437c-926a-d358cf4556ed",
      "content": "ヒートマップの軸は「pressure（圧力）」です。高圧域は赤色で、低圧領域は青色です。中央に緑色の領域が見られ、周囲には赤色と青色の区域が分布しています。右上角に「MP-PDE」のラベルがあり、左上角には「Ground truth」と書かれています。\n\nヒートマップの軸は「pressure（圧力）」です。高圧域は赤色で、低圧領域は青色です。中央に緑色の領域が見られ、周囲には赤色と青色の区域が分布しています。右上角に「MP-PDE」のラベルがあり、左上角には「Ground truth」と書かれています。\n\nヒートマップの軸は「pressure（圧力）」です。高圧域（赤・橙色）は「Ground truth（真の値）」の右端に集中的に分布し、中低圧（緑・黄色）は中央に広がり、低圦域（青・紫色）は左端に集中しています。右端の真値と左端の予測値（MP-PDE、PENN）の境界線が明確に示されています。\n\nヒートマップの軸は「pressure（圧力）」です。高圧域（赤・橙色）は「Ground truth（真の値）」の右端に集中的に分布し、中低圧（緑・黄色）は中央に広がり、低圦域（青・紫色）は左端に集中しています。右端の真値と左端の予測値（MP-PDE、PENN）の境界線が明確に示されています。\n\nヒートマップの軸は「pressure」（圧力）で、範囲は-0.5から-9.0e-01（約-9%）です。高圧域は黄色・橙色に、低域は緑・青色に分布し、中央に深青色の集中的な低圧領域が見られます。\n\nAlthough the property of our model is preferable, it also limits the applicable domain\n\nヒートマップの軸は「pressure（圧力）」です。MP-PDEとPENN（Ours）の図では、高圧域（赤・橙色）と低圴域（緑・青色）が明確に区別され、図中央に緑色の集中領域が見られ、周囲に橙色・赤色の領域が広がっている様子が示されています。\n\nヒートマップの軸は「pressure（圧力）」です。MP-PDEとPENN（Ours）の図では、高圧域（赤・橙色）と低圴域（緑・青色）が明確に区別され、図中央に緑色の集中領域が見られ、周囲に橙色・赤色の領域が広がっている様子が示されています。",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 155,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 25,
        "char_count": 982,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "9a31eed3-d8d3-4431-8188-d7ca786d0f2c",
      "content": "of the model because we need to be familiar with the concrete form of the PDE of interest",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 155,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 18,
        "char_count": 89,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "98873221-1ffb-4d89-a79b-92dc15fedf08",
      "content": "r sample\ne appli\nr sampl\ne applic\n3.1.7.5.1.2.3.4.5.6.7.8.9.10.11.12.13.14.15.16.17.18.19.20.21.22.23.24.25.26.27.28.29.30.31.32.33.34.35.36.37.38.39.40.41.42.43.44.45.46.47.48.49.50.51.52.53.54.55.56.57.58.59.60.61.62.63.64.65.66.67.68.69.70.71.72.73.74.75.76.77.78.79.80.81.82.83.84.85.86.87.88.89.90.91.92.93.94.95.96.97.98.99..100..101..102..103..104..105..106..107..108..109..110..111..112..113..114..115..116..117..118..119..120..121..122..123..124..125..126..127..128..129..130..131..132..133..134..135..136..137..138..139..140..141..142..143..144..145..146..147..148..149..150..151..152..153..154..155..156..157..158..159..160..161..162..163..164..165..166..167..168..169..170..171..172..173..174..175..176..177..178..179..180..181..182..183..184..185..186..187..188..189..190..191..192..193..194..195..196..197..198..199..200..201..202..203..204..205..206..207..208..209..210..211..212..213..214..215..216..217..218..219..220..221..222..223..224..225..226..227..228..229..230..231..232..233..234..235..236..237..238..239..240..241..242..243..244..245..246..247..248..249..250..251..252..253..254..255..256..257..258..259..260..261..262..263..264..265..266..267..268..269..270..271..272..273..274..275..276..277..278..279..280..281..282..283..284..285..286..287..288..289..290..291..292..293..294..295..296..297..298..299..300..301..302..303..304..305..306..307..308..309..310..311..312..313..314..315..316..317..318..319..320..321..322..323..324..325..326..327..328..329..330..331..332..333..334..335..336..337..338..339..340..341..342..343..344..345..346..347..348..349..350..351..352..353..354..355..356..357..358..359..360..361..362..363..364..365..366..367..368..369..370..371..372..373..374..375..376..377..378..379..380..381..382..383..384..385..386..387..388..389..390..391..392..393..394..395..396..397..398..399..400..401..402..403..404..405..406..407..408..409..410..411..412..413..414..415..416..417..418..419..420..421..422..423..424..425..426..427..428..429..430..431..432..433..434..435..436..437..438..439..440..441..442..443..444..445..446..447..448..449..450..451..452..453..454..455..456..457..458..459..460..461..462..463..464..465..466..467..468..469..470..471..472..473..474..475..476..477..478..479..480..481..482..483..484..485..486..487..488..489..490..491..492..493..494..495..496..497..498..499..500..501..502..503..504..505..506..507..508..509..510..511..512..513..514..515..516..517..518..519..520..521..52",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 155,
        "chunk_index": 2,
        "chunk_type": "text",
        "token_count": 9,
        "char_count": 2443,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "77f410c4-54b1-47a9-9ef1-c97b77b250dc",
      "content": "to construct the effective PENN model. For instance, the proposed model cannot exploit\n\n\nits potential to solve inverse problems where explicit forms of the governing PDE are not\n\n\navailable for such tasks. Therefore, combining PINNs and PENNs could be the next direc\n\ntion of the research community.",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 155,
        "chunk_index": 3,
        "chunk_type": "text",
        "token_count": 47,
        "char_count": 300,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "420d494d-a426-4081-b44a-4ebedd7ab0f3",
      "content": "**132** 4. Physics-Embedded Neural Network: Boundary Condition and Implicit Method",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 156,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 10,
        "char_count": 82,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "cf40465e-cbd7-4ecc-8b61-5b65b1e342c7",
      "content": "# **Chapter 5** **Conclusion**\n\nThe main contribution of this dissertation is the development of a general neural PDE\n\n\nsolvers that are:\n\n\n   - E( _n_ )-equivariant thanks to the use of an IsoGCN (Chapter 3); and\n\n\n   - capable of handling mixed boundary conditions and global interactions by applying\n\n\nthe implicit Euler method (Chapter 4).\n\n\nThrough numerical experiments, we demonstrated that our model is capable of accurately\n\n\npredicting heat phenomena on a mesh that is significantly larger than that used in the train\n\ning phase (Section 3.4). Our approach was also successful in handling various boundary\n\n\nconditions and PDE parameters, in addition to the global interactions that occur in incom\n\npressible flow phenomena (Section 4.4). Hereunder, we revisit the objectives outlined in\n\n\nChapter 1 and evaluate how they were addressed, indicating any existing limitations and\n\n\nsuggesting potential avenues for future work.",
      "chunk_metadata": {
        "section_title": "**Chapter 5** **Conclusion**",
        "section_level": 1,
        "page": 157,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 139,
        "char_count": 935,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "5d26ba50-d536-4155-925c-b2aa14d7c362",
      "content": "**Flexibility to treat arbitrary meshes based on GNNs** In this dissertation, we highlight\n\n\nthe flexibility of GNNs in treating arbitrary meshes. Additionally, GNNs offer other desir\n\nable characteristics, such as permutation equivariance (Section 3.3) and a generalizability\n\n\ncoming from locally-connected nature (Section 3.4). Permutation equivariance is crucial\n\n\nbecause a mesh can be indexed in various ways, each corresponding to a permutation of in\n\ndices. The locally-connected nature of GNNs allows for successful predictions on meshes\n\n\n133",
      "chunk_metadata": {
        "section_title": "**Chapter 5** **Conclusion**",
        "section_level": 1,
        "page": 157,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 76,
        "char_count": 552,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "ebb8e475-7f60-45ef-953b-782d60de1c72",
      "content": "**134** 5. Conclusion\n\n\nlarger than those used during training, as demonstrated in Section 3.4. However, due to\n\n\nthat feature, GNNs may struggle to capture global interactions that occur in fields such as\n\n\nincompressible flow, steady-state analysis, and structural analysis, which will be discussed\n\n\nin a subsequent work.\n\n\n**E(** _**n**_ **)-equivariance to reflect physical symmetries** Chapter 3 introduced the IsoGCN\n\n\nmodel, a GNN with E(n)-equivariance capable of learning mesh-discretized physical phe\n\nnomena from a relatively small dataset. We confirmed that PENN models also have E(n)\n\nequivariance (Section 4.4), which means that their added capability to handle boundary\n\n\nconditions and implicit time evolution is also E(n)-equivariant. However, physical phe\n\nnomena have other symmetries, such as that with respect to unit changes corresponding\n\n\nto scaling. In particular, some PDEs do not change under scaling as long as certain di",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 158,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 136,
        "char_count": 950,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "961e6510-dce5-43b9-842d-cad6fa778980",
      "content": "mensionless quantities, such as the Reynolds number, remain constant. In contrast, our\n\n\ncurrent model depend on scaling because we use volume features to improve predictive\n\n\nperformance. Therefore, a possible future direction could be developing a machine learn\n\ning model that is, not only permutation- and E(n)-, but also scaling-equivariant. Moreover,\n\n\nincorporating the conservation property could also lead to more stable and accurate predic\n\ntions.\n\n\n**Computational efficiency to realize faster predictions than with conventional nu-**\n\n\n**merical analysis methods** The computational efficiency of the IsoGCN model is due\n\n\nto its linear message passing and utilization of the sparse structure of mesh-like graphs\n\n\n(Section 3.3). Additionally, PENN models achieve fast and stable predictions using the\n\n\nBarzilai–Borwein method, a simplified nonlinear solver for implicit time evolution (Sec\n\ntion 4.3). However, the speedup observed in numerical experiments is of two to five times,",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 158,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 138,
        "char_count": 995,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "6c86b850-4cde-4449-b932-e332dd6d83ce",
      "content": "rather than an order of magnitude (Sections 3.4 and 4.4). This might be because we utilized\n\n\ndetailed meshes with a large amount of information. To improve computation speed, a pos\n\nsible future direction would be to reduce the number of degrees of freedom in the input\n\n\nmesh.\n\n\n**Accurate consideration of boundary conditions** In Chapter 4, we presented a frame\n\nwork for dealing with mixed boundary conditions. Our approach rigorously handles Dirich",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 158,
        "chunk_index": 2,
        "chunk_type": "text",
        "token_count": 71,
        "char_count": 454,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "dd15107c-5a10-45e2-9861-db928837e718",
      "content": "**135**\n\n\nlet boundary conditions, but there is room for improvement in satisfying Neumann bound\n\nary conditions, which is currently done with a certain degree of error. This may be due\n\n\nto the discretization error inherent in the chosen spatial differential model. To address this\n\n\nissue, we may consider using higher-order approximations of LSMPS differential operators\n\n\nor exploring alternative formalizations. For instance, the weak formulation used in methods\n\n\nsuch as FEM and FVM might be more accurate in treating Neumann boundary conditions.\n\n\n**Stable prediction over long time steps by accounting for global interactions** The\n\n\nPENN model achieves stable predictions for long-term states due to its use of the implicit\n\n\nEuler method, as described in Chapter 4. However, to reduce the computational cost, we\n\n\nwere forced to introduce considerable approximations in the implicit formulation, which",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 159,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 133,
        "char_count": 912,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "bb05d303-067d-4478-8eda-79284e8b952c",
      "content": "may compromise its benefits. In fact, as shown in Section 4.4.3.8, such approximations\n\n\nmay limit the generalizability of the analysis domain size. To resolve this, we could con\n\nsider using the quasi-Newton method instead of gradient descent to more accurately solve\n\n\nthe implicit equation. Another option may be to use the multigrid method, where the mesh\n\n\nis coarsened within the solver to increase the physical distances visible by the one-hop\n\n\noperation. Additionally, we may also explore the application of the all-to-all connectivity\n\n\nused in the Transformer model (Vaswani et al., 2017).\n\n\nDespite its limitations, the method proposed in this study lays a solid foundation for\n\n\nthe development of practical neural PDE solvers, possessing some desired features, such\n\n\nas the ability to handle arbitrary shapes and boundary conditions. Thus, our work may\n\n\nbe a crucial step towards achieving efficient, accurate, and versatile PDE solvers that can",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 159,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 145,
        "char_count": 961,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "369fb0d0-89ef-4e20-846d-ad66e686bf05",
      "content": "contribute to the further advancement of the productivity of human societies.",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 159,
        "chunk_index": 2,
        "chunk_type": "text",
        "token_count": 11,
        "char_count": 77,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "d8518a94-37f3-4800-a778-5c94a94ca901",
      "content": "**136** 5. Conclusion",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 160,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 3,
        "char_count": 21,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "5114d7be-2dd4-42e5-b528-3bdbe8d9a2a8",
      "content": "# **Bibliography**\n\nEman Ahmed, Alexandre Saint, Abd El Rahman Shabayek, Kseniya Cherenkova, Rig Das,\n\n\nGleb Gusev, Djamila Aouada, and Bjorn Ottersten. A Survey on Deep Learning Ad\n\nvances on Different 3D Data Representations. _arXiv preprint arXiv:1808.01462_, 2018.\n\n\nFerran Alet, Adarsh Keshav Jeewajee, Maria Bauza Villalonga, Alberto Rodriguez, Tomas\n\n\nLozano-Perez, and Leslie Kaelbling. Graph Element Networks: Adaptive, Structured\n\n\nComputation and Memory. In _International Conference on Machine Learning_, 2019.\n\n\nJonathan Barzilai and Jonathan M Borwein. Two-Point Step Size Gradient Methods. _IMA_\n\n\n_Journal of Numerical Analysis_, 8(1):141–148, 1988.\n\n\nIgor I Baskin, Vladimir A Palyulin, and Nikolai S Zefirov. A Neural Device for Searching\n\n\nDirect Correlations Between Structures and Properties of Chemical Compounds. _Journal_\n\n\n_of Chemical Information and Computer Sciences_, 37(4):715–721, 1997.",
      "chunk_metadata": {
        "section_title": "**Bibliography**",
        "section_level": 1,
        "page": 161,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 118,
        "char_count": 917,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "e39b319f-0fc7-45fe-bbb5-1c2989c820c4",
      "content": "Peter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius\n\n\nZambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan\n\n\nFaulkner, et al. Relational Inductive Biases, Deep Learning, and Graph Networks. _arXiv_\n\n\n_preprint arXiv:1806.01261_, 2018.\n\n\nChristopher M Bishop. _Pattern Recognition and Machine Learning_ . Springer, 2006.\n\n\nJohannes Brandstetter, Daniel E. Worrall, and Max Welling. Message Passing Neural PDE\n\n\nSolvers. In _International Conference on Learning Representations_ [, 2022. URL https:](https://openreview.net/forum?id=vSix3HPYKSU)\n\n\n[//openreview.net/forum?id=vSix3HPYKSU.](https://openreview.net/forum?id=vSix3HPYKSU)\n\n\n137",
      "chunk_metadata": {
        "section_title": "**Bibliography**",
        "section_level": 1,
        "page": 161,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 72,
        "char_count": 698,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "43a2098d-5cc0-40c5-a12b-b3d71807a292",
      "content": "**138** BIBLIOGRAPHY\n\n\nShengze Cai, Zhicheng Wang, Frederik Fuest, Young Jin Jeon, Callum Gray, and\n\n\nGeorge Em Karniadakis. Flow Over an Espresso Cup: Inferring 3-D Velocity and\n\n\nPressure Fields from Tomographic Background Oriented Schlieren via Physics-Informed\n\n\nNeural Networks. _Journal of Fluid Mechanics_, 915, 2021.\n\n\nKai-Hung Chang and Chin-Yi Cheng. Learning to Simulate and Design for Structural\n\n\nEngineering. _arXiv preprint arXiv:2003.09103_, 2020.\n\n\nMing Chen, Zhewei Wei, Zengfeng Huang, Bolin Ding, and Yaliang Li. Simple and Deep\n\n\nGraph Convolutional Networks. _arXiv preprint arXiv:2007.02133_, 2020.\n\n\nRicky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural Or\n\ndinary Differential Equations. _Advances in Neural Information Processing Systems_, 31,\n\n\n2018.\n\n\nWei-Lin Chiang, Xuanqing Liu, Si Si, Yang Li, Samy Bengio, and Cho-Jui Hsieh. Cluster\n\nGCN: An Efficient Algorithm for Training Deep and Large Graph Convolutional Net",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 162,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 131,
        "char_count": 971,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "9b301b9a-41c6-480e-acb2-61b7b189db22",
      "content": "works. In _Proceedings of the 25th ACM SIGKDD International Conference on Knowl-_\n\n\n_edge Discovery & Data Mining_, pp. 257–266, 2019.\n\n\nTaco Cohen and Max Welling. Group Equivariant Convolutional Networks. In _Interna-_\n\n\n_tional Conference on Machine Learning_, pp. 2990–2999, 2016.\n\n\nTaco S Cohen, Mario Geiger, Jonas K¨ohler, and Max Welling. Spherical CNNs. In _Inter-_\n\n\n_national Conference on Learning Representations_, 2018.\n\n\nTaco S Cohen, Maurice Weiler, Berkay Kicanaoglu, and Max Welling. Gauge Equivariant\n\n\nConvolutional Networks and the Icosahedral CNN. _International Conference on Ma-_\n\n\n_chine Learning_, 2019.\n\n\nGeorge Cybenko. Approximation by Superpositions of a Sigmoidal Function. _Mathematics_\n\n\n_of Control, Signals and Systems_, 5(4):455, 1992.\n\n\nXiaowen Dong, Dorina Thanou, Laura Toni, Michael Bronstein, and Pascal Frossard.\n\n\nGraph Signal Processing for Machine Learning: A Review and New Perspectives. _IEEE_\n\n\n_Signal Processing Magazine_, 37(6):117–127, 2020.",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 162,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 129,
        "char_count": 993,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "13d80c37-c0fc-499c-8e5d-5f714c2d55bd",
      "content": "Bibliography **139**\n\n\nNadav Dym and Haggai Maron. On the Universality of Rotation Equivariant Point Cloud\n\n\nNetworks. _arXiv preprint arXiv:2010.02449_, 2020.\n\n\nWenqi Fan, Yao Ma, Qing Li, Yuan He, Eric Zhao, Jiliang Tang, and Dawei Yin. Graph\n\n\nNeural Networks for Social Recommendation. In _The World Wide Web Conference_, pp.\n\n\n417–426, 2019.\n\n\nMatthias Fey and Jan E. Lenssen. Fast Graph Representation Learning with PyTorch Geo\n\nmetric. In _ICLR Workshop on Representation Learning on Graphs and Manifolds_, 2019.\n\n\nMatthias Fey, Jan Eric Lenssen, Frank Weichert, and Heinrich M¨uller. SplineCNN: Fast\n\n\nGeometric Deep Learning with Continuous B-Spline Kernels. In _Proceedings of the_\n\n\n_IEEE Conference on Computer Vision and Pattern Recognition_, pp. 869–877, 2018.\n\n\nFabian Fuchs, Daniel Worrall, Volker Fischer, and Max Welling. SE(3)-Transformers:\n\n\n3D Roto-Translation Equivariant Attention Networks. _Advances in Neural Information_\n\n\n_Processing Systems_, 33, 2020.",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 163,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 132,
        "char_count": 980,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "e04da683-647e-4fea-a8cb-3aba2ad29fd4",
      "content": "Christophe Geuzaine and Jean-Franc¸ois Remacle. Gmsh: a Three-Dimensional Finite El\n\nement Mesh Generator with Built-in Pre- and Post-Processing Facilities. _International_\n\n\n_Journal for Numerical Methods in Engineering_, 79(11):1309–1331, 2009.\n\n\nJustin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl.\n\n\nNeural Message Passing for Quantum Chemistry. In _International Conference on Ma-_\n\n\n_chine Learning_, pp. 1263–1272. JMLR. org, 2017.\n\n\nMarco Gori, Gabriele Monfardini, and Franco Scarselli. A New Model for Learning in\n\n\nGraph Domains. In _Proceedings. 2005 IEEE International Joint Conference on Neural_\n\n\n_Networks, 2005._, volume 2, pp. 729–734. IEEE, 2005.\n\n\nFangda Gu, Heng Chang, Wenwu Zhu, Somayeh Sojoudi, and Laurent El Ghaoui. Im\n\nplicit Graph Neural Networks. _Advances in Neural Information Processing Systems_, 33:\n\n\n11984–11995, 2020.\n\n\nDeguang Han, Keri Kornelson, Eric Weber, and David Larson. _Frames for Undergraduates_,",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 163,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 130,
        "char_count": 977,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "01e721d5-e345-48d4-881c-99d3e6d70f0b",
      "content": "volume 40. American Mathematical Soc., 2007.",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 163,
        "chunk_index": 2,
        "chunk_type": "text",
        "token_count": 6,
        "char_count": 44,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "c5b6d9cd-30a0-4993-8f14-8873f1ee4dd1",
      "content": "**140** BIBLIOGRAPHY\n\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for\n\n\nImage Recognition. In _Proceedings of the IEEE Conference on Computer Vision and_\n\n\n_Pattern Recognition_, pp. 770–778, 2016.\n\n\nMasanobu Horie and Naoto Mitsume. Physics-Embedded Neural Networks: Graph Neural\n\n\nPDE Solvers with Mixed Boundary Conditions. In Alice H. Oh, Alekh Agarwal, Danielle\n\n\nBelgrave, and Kyunghyun Cho (eds.), _Advances in Neural Information Processing Sys-_\n\n\n_tems_ [, 2022. URL https://openreview.net/forum?id=B3TOg-YCtzo.](https://openreview.net/forum?id=B3TOg-YCtzo)\n\n\nMasanobu Horie, Naoki Morita, Toshiaki Hishinuma, Yu Ihara, and Naoto Mitsume.\n\n\nIsometric Transformation Invariant and Equivariant Graph Convolutional Networks.\n\n\nIn _International Conference on Learning Representations_, 2021. [URL https://](https://openreview.net/forum?id=FX0vR39SJ5q)\n\n\n[openreview.net/forum?id=FX0vR39SJ5q.](https://openreview.net/forum?id=FX0vR39SJ5q)",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 164,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 100,
        "char_count": 978,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "b25d052e-7b22-4389-ba63-033a1a2d731c",
      "content": "Kurt Hornik. Approximation Capabilities of Multilayer Feedforward Networks. _Neural_\n\n\n_Networks_, 4(2):251–257, 1991.\n\n\nYurie A Ignatieff. Foundations of Rational Continuum Mechanics. In _The Mathematical_\n\n\n_World of Walter Noll_, pp. 107–125. Springer, 1996.\n\n\nYu Ihara, Gaku Hashimoto, and Hiroshi Okuda. Web-Based Integrated Cloud CAE Plat\n\nform for Large-Scale Finite Element Analysis. _Mechanical Engineering Letters_, 3:17–\n\n\n00520, 2017.\n\n\nDiederik P Kingma and Jimmy Ba. Adam: A method for Stochastic Optimization. _arXiv_\n\n\n_preprint arXiv:1412.6980_, 2014.\n\n\nThomas N Kipf and Max Welling. Semi-Supervised Classification with Graph Convolu\n\ntional Networks. In _International Conference on Learning Representations_, 2017. URL\n\n\n[https://openreview.net/forum?id=SJU4ayYgl.](https://openreview.net/forum?id=SJU4ayYgl)\n\n\nJohannes Klicpera, Janek Groß, and Stephan G¨unnemann. Directional Message Passing",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 164,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 103,
        "char_count": 913,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "a463494d-7fcf-4462-b6d8-93eae51e6ec9",
      "content": "for Molecular Graphs. In _International Conference on Learning Representations_, 2020.\n\n\nSebastian Koch, Albert Matveev, Zhongshi Jiang, Francis Williams, Alexey Artemov,\n\n\nEvgeny Burnaev, Marc Alexa, Denis Zorin, and Daniele Panozzo. ABC: A Big CAD",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 164,
        "chunk_index": 2,
        "chunk_type": "text",
        "token_count": 33,
        "char_count": 249,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "715c7b58-cfa1-480c-b278-a83cbee1673f",
      "content": "Bibliography **141**\n\n\nModel Dataset for Geometric Deep Learning. In _The IEEE Conference on Computer_\n\n\n_Vision and Pattern Recognition (CVPR)_, June 2019.\n\n\nRisi Kondor. N-body Networks: a Covariant Hierarchical Neural Network Architecture for\n\n\nLearning Atomic Potentials. _arXiv preprint arXiv:1803.01588_, 2018.\n\n\nAlex Krizhevsky, Geoffrey Hinton, et al. Learning Multiple Layers of Features from Tiny\n\n\nImages. 2009.\n\n\nJulia Ling, Andrew Kurzawski, and Jeremy Templeton. Reynolds Averaged Turbulence\n\n\nModelling using Deep Neural Networks with Embedded Invariance. _Journal of Fluid_\n\n\n_Mechanics_, 807:155–166, 2016.\n\n\nDong C Liu and Jorge Nocedal. On the Limited Memory BFGS Method for Large Scale\n\n\nOptimization. _Mathematical Programming_, 45(1):503–528, 1989.\n\n\nDavid G Luenberger, Yinyu Ye, et al. _Linear and Nonlinear Programming_, volume 2.\n\n\nSpringer, 1984.\n\n\nZhiping Mao, Ameya D Jagtap, and George Em Karniadakis. Physics-Informed Neural",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 165,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 124,
        "char_count": 955,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "a4f34c01-78f6-4205-8f3f-2aeea1dfed14",
      "content": "Networks for High-Speed Flows. _Computer Methods in Applied Mechanics and Engi-_\n\n\n_neering_, 360:112789, 2020.\n\n\nHaggai Maron, Heli Ben-Hamu, Nadav Shamir, and Yaron Lipman. Invariant and Equiv\n\nariant Graph Networks. _arXiv preprint arXiv:1812.09902_, 2018.\n\n\nTakuya Matsunaga, Axel S¨odersten, Kazuya Shibata, and Seiichi Koshizuka. Improved\n\n\nTreatment of Wall Boundary Conditions for a Particle Method with Consistent Spatial\n\n\nDiscretization. _Computer Methods in Applied Mechanics and Engineering_, 358:112624,\n\n\n2020.\n\n\nFederico Monti, Davide Boscaini, Jonathan Masci, Emanuele Rodola, Jan Svoboda, and\n\n\nMichael M Bronstein. Geometric Deep Learning on Graphs and Manifolds using Mix\n\nture Model CNNs. In _Proceedings of the IEEE Conference on Computer Vision and_\n\n\n_Pattern Recognition_, pp. 5115–5124, 2017.",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 165,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 106,
        "char_count": 818,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "63b9999e-9c7f-4036-aaf3-e729ef18264e",
      "content": "**142** BIBLIOGRAPHY\n\n\nNaoki Morita, Kazuo Yonekura, Ichiro Yasuzumi, Mitsuyoshi Tsunori, Gaku Hashimoto,\n\n\nand Hiroshi Okuda. Development of 3 _×_ 3 DOF Blocking Structural Elements to En\n\nhance the Computational Intensity of Iterative Linear Solver. _Mechanical Engineering_\n\n\n_Letters_, 2:16–00082, 2016.\n\n\nVinod Nair and Geoffrey E Hinton. Rectified Linear Units Improve Restricted Boltzmann\n\n\nMachines. In _International Conference on Machine Learning_, pp. 807–814, 2010.\n\n\nPreetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya\n\n\nSutskever. Deep Double Descent: Where Bigger Models and More Data Hurt. _Jour-_\n\n\n_nal of Statistical Mechanics: Theory and Experiment_, 2021(12):124003, 2021.\n\n\nAntonio Ortega, Pascal Frossard, Jelena Kovaˇcevi´c, Jos´e MF Moura, and Pierre Van\n\ndergheynst. Graph Signal Processing: Overview, Challenges, and Applications. _Pro-_\n\n\n_ceedings of the IEEE_, 106(5):808–828, 2018.",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 166,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 121,
        "char_count": 940,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "c126e26a-89f7-472c-a1df-24f41a3982f6",
      "content": "Guofei Pang, Lu Lu, and George Em Karniadakis. fPINNs: Fractional Physics-Informed\n\n\nNeural Networks. _SIAM Journal on Scientific Computing_, 41(4):A2603–A2626, 2019.\n\n\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory\n\n\nChanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Des\n\nmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Te\n\njani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala.\n\n\nPyTorch: An Imperative Style, High-Performance Deep Learning Library. In H. Wal\n\nlach, H. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc, E. Fox, and R. Garnett (eds.), _Ad-_\n\n\n_vances in Neural Information Processing Systems_, pp. 8024–8035. Curran Associates,\n\n\nInc., 2019.\n\n\nTobias Pfaff, Meire Fortunato, Alvaro Sanchez-Gonzalez, and Peter Battaglia. Learn\n\ning Mesh-Based Simulation with Graph Networks. In _International Conference on_",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 166,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 122,
        "char_count": 932,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "f8faa658-1ca3-4fc5-843b-7d256cb0ebdf",
      "content": "_Learning Representations_ [, 2021. URL https://openreview.net/forum?id=](https://openreview.net/forum?id=roNqYL0_XP)\n\n\n[roNqYL0_XP.](https://openreview.net/forum?id=roNqYL0_XP)\n\n\nMaziar Raissi, Paris Perdikaris, and George E Karniadakis. Physics-Informed Neural Net\n\nworks: A Deep Learning Framework for Solving Forward and Inverse Problems Involv",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 166,
        "chunk_index": 2,
        "chunk_type": "text",
        "token_count": 30,
        "char_count": 348,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "c18ab338-d270-42af-8c0d-3072e5bc2a75",
      "content": "Bibliography **143**\n\n\ning Nonlinear Partial Differential Equations. _Journal of Computational Physics_, 378:\n\n\n686–707, 2019.\n\n\nSiamak Ravanbakhsh, Jeff Schneider, and Barnab´as P´oczos. Equivariance Through\n\n\nParameter-Sharing. In Doina Precup and Yee Whye Teh (eds.), _International Confer-_\n\n\n_ence on Machine Learning_, volume 70 of _Proceedings of Machine Learning Research_,\n\n\npp. 2892–2901. PMLR, 06–11 Aug 2017. [URL https://proceedings.mlr.](https://proceedings.mlr.press/v70/ravanbakhsh17a.html)\n\n\n[press/v70/ravanbakhsh17a.html.](https://proceedings.mlr.press/v70/ravanbakhsh17a.html)\n\n\nAlvaro Sanchez-Gonzalez, Nicolas Heess, Jost Tobias Springenberg, Josh Merel, Martin\n\n\nRiedmiller, Raia Hadsell, and Peter Battaglia. Graph Networks as Learnable Physics\n\n\nEngines for Inference and Control. _arXiv preprint arXiv:1806.01242_, 2018.\n\n\nAlvaro Sanchez-Gonzalez, Victor Bapst, Kyle Cranmer, and Peter Battaglia. Hamiltonian",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 167,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 95,
        "char_count": 934,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "58db0672-0232-4f95-b7fd-a52289a97248",
      "content": "Graph Networks with ODE Integrators. _arXiv preprint arXiv:1909.12790_, 2019.\n\n\nAlvaro Sanchez-Gonzalez, Jonathan Godwin, Tobias Pfaff, Rex Ying, Jure Leskovec, and\n\n\nPeter W Battaglia. Learning to Simulate Complex Physics with Graph Networks. _arXiv_\n\n\n_preprint arXiv:2002.09405_, 2020.\n\n\nFranco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Mon\n\nfardini. The Graph Neural Network Model. _IEEE Transactions on Neural Networks_, 20\n\n\n(1):61–80, 2008.\n\n\nAlessandro Sperduti and Antonina Starita. Supervised Neural Networks for the Classifica\n\ntion of Structures. _IEEE Transactions on Neural Networks_, 8(3):714–735, 1997.\n\n\nTasuku Tamai and Seiichi Koshizuka. Least Squares Moving Particle Semi-Implicit\n\n\nMethod. _Computational Particle Mechanics_, 1(3):277–305, 2014.\n\n\nNathaniel Thomas, Tess Smidt, Steven Kearnes, Lusann Yang, Li Li, Kai Kohlhoff, and\n\n\nPatrick Riley. Tensor Field Networks: Rotation-and Translation-Equivariant Neural Net",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 167,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 120,
        "char_count": 970,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "e107f9e9-41be-4560-8465-d5082976d537",
      "content": "works for 3d Point Clouds. _arXiv preprint arXiv:1802.08219_, 2018.\n\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N\n\n\nGomez, Łukasz Kaiser, and Illia Polosukhin. Attention is All You Need. _Advances_\n\n\n_in Neural Information Processing Systems_, 30, 2017.",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 167,
        "chunk_index": 2,
        "chunk_type": "text",
        "token_count": 40,
        "char_count": 290,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "23c9ce9d-7c97-4eb8-8173-0367d37c4cf0",
      "content": "**144** BIBLIOGRAPHY\n\n\nRui Wang, Robin Walters, and Rose Yu. Incorporating Symmetry into Deep Dynamics\n\n\nModels for Improved Generalization. In _International Conference on Learning Repre-_\n\n\n_sentations_ [, 2021. URL https://openreview.net/forum?id=wta_8Hx2KD.](https://openreview.net/forum?id=wta_8Hx2KD)\n\n\nMaurice Weiler, Mario Geiger, Max Welling, Wouter Boomsma, and Taco S Cohen. 3d\n\n\nSteerable CNNs: Learning Rotationally Equivariant Features in Volumetric Data. In\n\n\n_Advances in Neural Information Processing Systems_, pp. 10381–10392, 2018.\n\n\nFelix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian Weinberger.\n\n\nSimplifying Graph Convolutional Networks. In _International Conference on Machine_\n\n\n_Learning_, pp. 6861–6871. PMLR, 2019.\n\n\nKeyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How Powerful are Graph\n\n\nNeural Networks? _arXiv preprint arXiv:1810.00826_, 2018.\n\n\nJiaxuan You, Rex Ying, and Jure Leskovec. Position-Aware Graph Neural Networks. _arXiv_",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 168,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 119,
        "char_count": 1000,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "a41c2f3a-849f-4a07-9b54-a40f95139074",
      "content": "_preprint arXiv:1906.04817_, 2019.\n\n\nManzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R Salakhut\n\ndinov, and Alexander J Smola. Deep Sets. _Advances in Neural Information Processing_\n\n\n_Systems_, 30, 2017.",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 168,
        "chunk_index": 1,
        "chunk_type": "text",
        "token_count": 29,
        "char_count": 228,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "5269a053-dddf-42b3-abd5-9c936426def0",
      "content": "# **Index**\n\nE( _n_ )-equivariant pointwise MLP, 52\n\n\nactivation function, 12\n\n\nadjacency matrix, 10\n\n\nbias, 12\n\n\nboundary encoder, 85\n\n\ncontraction, 47\n\n\nconvolution, 47\n\n\ndegree matrix, 10\n\n\nDirichlet boundary condition, 22\n\n\nDirichlet layer, 86\n\n\ndiscrete tensor field, 42, 43\n\n\nedge feature, 12\n\n\nequivariance, 18\n\n\nEuclidean group, 18\n\n\nexplicit Euler method, 23\n\n\nFDM, 29\n\n\nFEM, 32\n\n\nframe, 89\n\n\ndual frame, 89\n\n\nGCN, 15\n\n\ngeneral linear group, 17\n\n\nGNN, 9\n\n\n\ngradient descent, 26\n\n\ngraph, 9\n\n\ndirected graph, 9\n\n\nundirected graph, 9\n\n\ngraph Laplacian matrix, 10\n\n\ngroup, 17\n\n\ngroup action, 17\n\n\nimplicit Euler method, 23\n\n\ninvariance, 18\n\n\nIsoAM, 44\n\n\ndifferential IsoAM, 54\n\n\nIsoGCN, 40, 53\n\n\nNIsoGCN, 88\n\n\nKronecker delta, 20\n\n\nLSMPS method, 36\n\n\nmesh, 23\n\n\nMLP, 12\n\n\nMPNN, 14\n\n\nNeumann boundary condition, 22\n\n\nNewton–Raphson method, 25\n\n\northogonal group, 17\n\n\nPDE, 21\n\n\n145",
      "chunk_metadata": {
        "section_title": "**Index**",
        "section_level": 1,
        "page": 169,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 128,
        "char_count": 885,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    },
    {
      "chunk_id": "ffb90013-3d7e-4634-9d38-90b542d33ce1",
      "content": "**146** INDEX\n\n\nPENN, 82\n\n\npermutaion, 10\n\n\npointwise MLP, 13\n\n\npseudoinverse decoder, 86\n\n\nquasi-Newton method, 25\n\n\nsymmetric group, 18\n\n\nvertex feature, 12\n\n\nweight matrix, 12",
      "chunk_metadata": {
        "section_title": "",
        "section_level": 0,
        "page": 170,
        "chunk_index": 0,
        "chunk_type": "text",
        "token_count": 24,
        "char_count": 178,
        "contains_formulas": false,
        "contains_tables": false,
        "contains_code": false,
        "keywords": [],
        "source_document": "DA010809.pdf"
      }
    }
  ],
  "processing_stats": {
    "total_chunks": 417,
    "total_chars": 292301,
    "avg_chunk_size": 692.3597122302158,
    "chunk_types": {
      "text": 370,
      "table": 24,
      "formula": 23
    },
    "formula_chunks": 23,
    "table_chunks": 26,
    "code_chunks": 0
  }
}