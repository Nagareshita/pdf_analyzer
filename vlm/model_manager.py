"""
SAIL-VL2-2B ãƒ¢ãƒ‡ãƒ«ç®¡ç†ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆæ”¹å–„ç‰ˆï¼‰
RAGã‚·ã‚¹ãƒ†ãƒ ã¨ã®çµ±åˆã‚’è€ƒæ…®ã—ã€æŸ”è»Ÿãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®šã«å¯¾å¿œ
"""

import torch
from pathlib import Path
from transformers import AutoTokenizer, AutoModel, AutoProcessor
from huggingface_hub import snapshot_download
import json
import shutil
from typing import Optional, Dict, Any
from PIL import Image


class ModelManager:
    """SAIL-VL2ãƒ¢ãƒ‡ãƒ«ã®ç®¡ç†ã‚¯ãƒ©ã‚¹ï¼ˆæ”¹å–„ç‰ˆï¼‰"""
    
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆQwen3-1.7Bæ¨å¥¨å€¤ã«æº–æ‹ ï¼‰
    DEFAULT_GENERATION_CONFIG = {
        # Sampling / search
        "do_sample": True,
        "temperature": 0.7,
        "top_p": 0.8,
        "top_k": 20,
        "num_beams": 1,
        "length_penalty": 1.0,
        "no_repeat_ngram_size": 0,
        "repetition_penalty": 1.0,
        "diversity_penalty": 0.0,
        "early_stopping": False,

        # Length control
        "min_new_tokens": 1,
        "max_new_tokens": 512,
    }
    
    # ã‚¿ã‚¹ã‚¯åˆ¥ãƒ—ãƒªã‚»ãƒƒãƒˆ
    # æ³¨æ„: use_cacheã¯modeling_sailvl.pyã§å¼·åˆ¶çš„ã«TrueãŒè¨­å®šã•ã‚Œã‚‹ãŸã‚ã€
    #       ã“ã“ã§ã¯è¨­å®šã—ãªã„ï¼ˆäºŒé‡è¨­å®šã‚¨ãƒ©ãƒ¼å›é¿ï¼‰
    TASK_PRESETS = {
        # æ—¢å­˜äº’æ›ï¼ˆã‚·ãƒ³ãƒ—ãƒ«ï¼‰
        "accurate": {  # æ­£ç¢ºæ€§é‡è¦–/OCRå¯„ã‚Š
            "do_sample": False,
            "num_beams": 4,
            "length_penalty": 1.05,
            "min_new_tokens": 64,
            "max_new_tokens": 1024,
            "no_repeat_ngram_size": 3,
            "repetition_penalty": 1.08,
            "temperature": 0.1,
            "top_p": 0.5,
            "top_k": 10,
        },
        "balanced": {  # ç”»åƒèª¬æ˜ãƒ»ãƒãƒ©ãƒ³ã‚¹å‹
            "do_sample": True,
            "temperature": 0.7,
            "top_p": 0.8,
            "top_k": 20,
            "min_new_tokens": 32,
            "max_new_tokens": 512,
            "repetition_penalty": 1.0,
        },

        # ææ¡ˆãƒ—ãƒªã‚»ãƒƒãƒˆï¼ˆãƒªã‚¯ã‚¨ã‚¹ãƒˆæº–æ‹ ï¼‰
        "ocr": {
            "do_sample": False, "num_beams": 5,
            "length_penalty": 1.1,
            "min_new_tokens": 120, "max_new_tokens": 2048,
            "no_repeat_ngram_size": 4, "repetition_penalty": 1.05
        },
        "qa": {
            "do_sample": False, "num_beams": 4,
            "min_new_tokens": 64, "max_new_tokens": 1024,
            "no_repeat_ngram_size": 4, "repetition_penalty": 1.05
        },
        "code": {
            "do_sample": False, "num_beams": 4,
            "length_penalty": 0.98, "min_new_tokens": 128, "max_new_tokens": 2048,
            "no_repeat_ngram_size": 6, "repetition_penalty": 1.08
        },
        "creative": {
            "do_sample": True, "temperature": 0.9, "top_p": 0.92,
            "min_new_tokens": 80, "max_new_tokens": 1024,
            "repetition_penalty": 1.02
        },
        "summary": {
            "do_sample": False, "num_beams": 3,
            "length_penalty": 0.95, "min_new_tokens": 80, "max_new_tokens": 800,
            "no_repeat_ngram_size": 4
        },
        "json": {
            "do_sample": False, "num_beams": 6,
            "min_new_tokens": 80, "max_new_tokens": 1600,
            "no_repeat_ngram_size": 4, "repetition_penalty": 1.05
        },
    }
    
    def __init__(self, model_path: Optional[str] = None):
        default_path = Path(__file__).resolve().parent / "SAIL-VL2-2B"
        self.model_path = Path(model_path) if model_path else default_path
        self.model = None
        self.tokenizer = None
        self.processor = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
    def setup_model(self, progress_callback=None) -> bool:
        """ãƒ¢ãƒ‡ãƒ«ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ï¼ˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã€è¨­å®šä¿®æ­£ã€èª­ã¿è¾¼ã¿ï¼‰"""
        try:
            # ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            if progress_callback:
                progress_callback("ãƒ¢ãƒ‡ãƒ«ã®ç¢ºèªä¸­...")
            self._download_model_if_needed()
            
            # ã‚¹ãƒ†ãƒƒãƒ—2: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢ï¼ˆå¤ã„è¨­å®šã‚’å‰Šé™¤ï¼‰
            if progress_callback:
                progress_callback("ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ä¸­...")
            self._clear_cache()
            
            # ã‚¹ãƒ†ãƒƒãƒ—3: è¨­å®šä¿®æ­£
            if progress_callback:
                progress_callback("è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿®æ­£ä¸­...")
            self._fix_config()
            self._patch_configuration_sailvl()
            self._patch_processing_sailvl()
            self._patch_modeling_qwen3()
            
            # ã‚¹ãƒ†ãƒƒãƒ—4: ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
            if progress_callback:
                progress_callback("ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’èª­ã¿è¾¼ã¿ä¸­...")
            self._load_tokenizer()
            
            if progress_callback:
                progress_callback("ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼ã‚’èª­ã¿è¾¼ã¿ä¸­...")
            self._load_processor()
            
            if progress_callback:
                progress_callback("ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­ï¼ˆæ•°åˆ†ã‹ã‹ã‚Šã¾ã™ï¼‰...")
            self._load_model()
            
            if progress_callback:
                progress_callback("å®Œäº†")
            
            return True
            
        except Exception as e:
            print(f"ãƒ¢ãƒ‡ãƒ«ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def _download_model_if_needed(self):
        """å¿…è¦ã«å¿œã˜ã¦ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆsafetensorsæ¬ æã‚‚æ¤œçŸ¥ã—ã¦å†å–å¾—ï¼‰"""
        required_files = ["config.json", "tokenizer.json"]

        def has_all_weights(path: Path) -> bool:
            """ãƒ¢ãƒ‡ãƒ«ã‚¦ã‚§ã‚¤ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒå®Œå…¨ã«æƒã£ã¦ã„ã‚‹ã‹ç¢ºèª"""
            index_file = path / "model.safetensors.index.json"
            
            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹å ´åˆã€å‚ç…§ã•ã‚Œã¦ã„ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèª
            if index_file.exists():
                try:
                    with open(index_file, "r", encoding="utf-8") as f:
                        index_data = json.load(f)
                    
                    # weight_mapã‹ã‚‰å¿…è¦ãªãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆã‚’å–å¾—
                    if "weight_map" in index_data:
                        required_weight_files = set(index_data["weight_map"].values())
                        
                        # ã™ã¹ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
                        for weight_file in required_weight_files:
                            if not (path / weight_file).exists():
                                print(f"âš ï¸ æ¬ æãƒ•ã‚¡ã‚¤ãƒ«æ¤œå‡º: {weight_file}")
                                return False
                        
                        print(f"âœ… ã™ã¹ã¦ã®ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ« ({len(required_weight_files)}å€‹) ã‚’ç¢ºèª")
                        return True
                except Exception as e:
                    print(f"âš ï¸ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
                    return False
            
            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ãŒãªã„å ´åˆã¯å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã™
            return any(path.glob("*.safetensors"))

        is_downloaded = (
            self.model_path.exists()
            and all((self.model_path / f).exists() for f in required_files)
            and has_all_weights(self.model_path)
        )

        if not is_downloaded:
            print("ğŸ“¥ ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ã„ã¾ã™... (ç´„4GBã€æ•°åˆ†ã‹ã‹ã‚Šã¾ã™)")
            print(f"ä¿å­˜å…ˆ: {self.model_path}")
            snapshot_download(
                repo_id="BytedanceDouyinContent/SAIL-VL2-2B",
                repo_type="model",
                local_dir=str(self.model_path),
                local_dir_use_symlinks=False,
            )
            print("âœ… ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†")
    
    def _fix_config(self):
        """config.jsonã®Flash Attentionè¨­å®šã¨ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£åã‚’ä¿®æ­£"""
        config_path = self.model_path / "config.json"
        
        with open(config_path, "r", encoding="utf-8") as f:
            config = json.load(f)
        
        modified = False
        
        # Flash Attention â†’ SDPA
        if config.get("_attn_implementation") == "flash_attention_2":
            config["_attn_implementation"] = "sdpa"
            modified = True
        
        if "llm_config" in config and config["llm_config"].get("attn_implementation") == "flash_attention_2":
            config["llm_config"]["attn_implementation"] = "sdpa"
            modified = True
        
        # Qwen2ForCausalLM â†’ Qwen3ForCausalLM (transformers 4.57.3å¯¾å¿œ)
        if "llm_config" in config and "architectures" in config["llm_config"]:
            if "Qwen2ForCausalLM" in config["llm_config"]["architectures"]:
                config["llm_config"]["architectures"] = ["Qwen3ForCausalLM"]
                modified = True
                print("âœ… ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£åã‚’ Qwen3ForCausalLM ã«ä¿®æ­£")
        
        if modified:
            with open(config_path, "w", encoding="utf-8") as f:
                json.dump(config, f, indent=2, ensure_ascii=False)
    
    def _patch_configuration_sailvl(self):
        """configuration_sailvl.pyã®Qwen2â†’Qwen3å¯¾å¿œãƒ‘ãƒƒãƒ"""
        config_file = self.model_path / "configuration_sailvl.py"
        
        if not config_file.exists():
            return
        
        with open(config_file, "r", encoding="utf-8") as f:
            content = f.read()
        
        modified = False
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä¿®æ­£
        if "'Qwen2ForCausalLM'" in content:
            content = content.replace(
                "llm_config = {'architectures': ['Qwen2ForCausalLM']}",
                "llm_config = {'architectures': ['Qwen3ForCausalLM']}"
            )
            modified = True
        
        # æ¡ä»¶åˆ†å²ã«Qwen2ForCausalLMã®ã‚µãƒãƒ¼ãƒˆã‚’è¿½åŠ 
        old_condition = """        if llm_config['architectures'][0] == 'LlamaForCausalLM':
            self.llm_config = LlamaConfig(**llm_config)
        elif llm_config['architectures'][0] == 'Qwen3ForCausalLM':
            self.llm_config = Qwen3Config(**llm_config)
        else:
            raise ValueError('Unsupported architecture: {}'.format(llm_config['architectures'][0]))"""
        
        new_condition = """        if llm_config['architectures'][0] == 'LlamaForCausalLM':
            self.llm_config = LlamaConfig(**llm_config)
        elif llm_config['architectures'][0] in ['Qwen2ForCausalLM', 'Qwen3ForCausalLM']:
            self.llm_config = Qwen3Config(**llm_config)
        else:
            raise ValueError('Unsupported architecture: {}'.format(llm_config['architectures'][0]))"""
        
        if old_condition in content:
            content = content.replace(old_condition, new_condition)
            modified = True
        
        if modified:
            with open(config_file, "w", encoding="utf-8") as f:
                f.write(content)
            print("âœ… configuration_sailvl.py ã‚’ Qwen3 å¯¾å¿œã«ãƒ‘ãƒƒãƒé©ç”¨")
    
    def _patch_processing_sailvl(self):
        """processing_sailvl.pyã®transformers 4.57.3äº’æ›æ€§ãƒ‘ãƒƒãƒ"""
        processing_file = self.model_path / "processing_sailvl.py"
        
        if not processing_file.exists():
            return
        
        with open(processing_file, "r", encoding="utf-8") as f:
            content = f.read()
        
        modified = False
        
        # _validate_images_text_input_orderã®importã‚’å‰Šé™¤
        old_import = "from transformers.processing_utils import ProcessingKwargs, ProcessorMixin, Unpack, _validate_images_text_input_order"
        new_import = "from transformers.processing_utils import ProcessingKwargs, ProcessorMixin, Unpack"
        
        if old_import in content:
            content = content.replace(old_import, new_import)
            modified = True
        
        # _validate_images_text_input_orderé–¢æ•°ã‚’è‡ªå‰å®Ÿè£…ã«ç½®ãæ›ãˆ
        old_validation = "        images, text = _validate_images_text_input_order(images, text)"
        new_validation = """        # Backward compatibility: transformers 4.57.3ã§ã¯ä¸è¦
        # å¼•æ•°ã®é †åºæ¤œè¨¼ã¯çœç•¥ï¼ˆé€šå¸¸ã¯å•é¡Œãªã—ï¼‰
        pass"""
        
        if old_validation in content:
            content = content.replace(old_validation, new_validation)
            modified = True
        
        if modified:
            with open(processing_file, "w", encoding="utf-8") as f:
                f.write(content)
            print("âœ… processing_sailvl.py ã‚’ transformers 4.57.3 å¯¾å¿œã«ãƒ‘ãƒƒãƒé©ç”¨")
    
    def _patch_modeling_qwen3(self):
        """modeling_qwen3.pyã‚’NVIDIA GPUå¯¾å¿œã«ä¿®æ­£"""
        modeling_file = self.model_path / "modeling_qwen3.py"
        
        if not modeling_file.exists():
            return
        
        with open(modeling_file, "r", encoding="utf-8") as f:
            content = f.read()
        
        modified = False
        
        # LossKwargs importå‰Šé™¤ï¼ˆtransformers 4.57.3ã§å»ƒæ­¢ï¼‰
        if "from transformers.utils import (\n    LossKwargs," in content:
            content = content.replace(
                "from transformers.utils import (\n    LossKwargs,",
                "from transformers.utils import ("
            )
            modified = True
            print("âœ… modeling_qwen3.py ã‹ã‚‰ LossKwargs import ã‚’å‰Šé™¤")
        
        # LossKwargsã‚¯ãƒ©ã‚¹ç¶™æ‰¿ã‚’å‰Šé™¤
        if "class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ..." in content:
            content = content.replace(
                "class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...",
                "class KwargsForCausalLM(FlashAttentionKwargs): ..."
            )
            modified = True
            print("âœ… modeling_qwen3.py ã‹ã‚‰ LossKwargs ç¶™æ‰¿ã‚’å‰Šé™¤")
        
        # NVIDIA GPUå¯¾å¿œãƒ‘ãƒƒãƒ
        if "torch_npu.npu_fusion_attention(" in content and "hasattr(torch, 'npu')" not in content:
            old_code = """    head_num = query.shape[1]
    attn_output = torch_npu.npu_fusion_attention(
                    query, key, value, head_num, input_layout="BNSD", 
                    pse=None,
                    atten_mask=atten_mask_npu,
                    scale=1.0 / math.sqrt(query.shape[-1]),
                    pre_tockens=2147483647,
                    next_tockens=2147483647,
                    keep_prob=1
                )[0]

    attn_output = attn_output.transpose(1, 2).contiguous()"""
            
            new_code = """    head_num = query.shape[1]
    
    # NVIDIA GPU ã®å ´åˆã¯æ¨™æº– SDPA ã‚’ä½¿ç”¨
    if not hasattr(torch, 'npu') or not torch.npu.is_available():
        attn_output = torch.nn.functional.scaled_dot_product_attention(
            query,
            key,
            value,
            attn_mask=causal_mask,
            dropout_p=dropout,
            scale=scaling if scaling else 1.0 / math.sqrt(query.shape[-1]),
            is_causal=is_causal,
        )
    else:
        # Huawei NPU ã®å ´åˆ
        attn_output = torch_npu.npu_fusion_attention(
                        query, key, value, head_num, input_layout="BNSD", 
                        pse=None,
                        atten_mask=atten_mask_npu,
                        scale=1.0 / math.sqrt(query.shape[-1]),
                        pre_tockens=2147483647,
                        next_tockens=2147483647,
                        keep_prob=1
                    )[0]

    attn_output = attn_output.transpose(1, 2).contiguous()"""
            
            content = content.replace(old_code, new_code)
            modified = True
            print("âœ… modeling_qwen3.py ã« NVIDIA GPU å¯¾å¿œãƒ‘ãƒƒãƒã‚’é©ç”¨")
        
        if modified:
            with open(modeling_file, "w", encoding="utf-8") as f:
                f.write(content)
    
    def _clear_cache(self):
        """transformersã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ï¼ˆSAIL-VL2-2B é–¢é€£ã‚’ç¢ºå®Ÿã«å‰Šé™¤ï¼‰"""
        base_dir = Path.home() / ".cache" / "huggingface" / "modules" / "transformers_modules"
        # å®Ÿéš›ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªåã¯ SAIL_hyphen_VL2_hyphen_2B ã«ãªã‚‹å ´åˆãŒã‚ã‚‹
        candidates = [
            base_dir / "SAIL-VL2-2B",
            base_dir / "SAIL_hyphen_VL2_hyphen_2B",
        ]
        cleared = False
        for cache_dir in candidates:
            try:
                if cache_dir.exists():
                    shutil.rmtree(cache_dir)
                    print(f"ğŸ—‘ï¸ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’å‰Šé™¤: {cache_dir.name}")
                    cleared = True
            except Exception as e:
                print(f"âš ï¸ ã‚­ãƒ£ãƒƒã‚·ãƒ¥å‰Šé™¤ã«å¤±æ•—ã—ã¾ã—ãŸ: {cache_dir} ({e})")
        
        if not cleared:
            print("â„¹ï¸ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¯å­˜åœ¨ã—ã¾ã›ã‚“ï¼ˆåˆå›å®Ÿè¡Œã¾ãŸã¯ã‚¯ãƒªãƒ¼ãƒ³çŠ¶æ…‹ï¼‰")
    
    def _load_tokenizer(self):
        """ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’èª­ã¿è¾¼ã¿"""
        self.tokenizer = AutoTokenizer.from_pretrained(
            str(self.model_path),
            local_files_only=True,
            trust_remote_code=True,
        )
    
    def _load_processor(self):
        """ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼ã‚’èª­ã¿è¾¼ã¿"""
        self.processor = AutoProcessor.from_pretrained(
            str(self.model_path),
            local_files_only=True,
            trust_remote_code=True,
        )
    
    def _load_model(self):
        """ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        self.model = AutoModel.from_pretrained(
            str(self.model_path),
            local_files_only=True,
            trust_remote_code=True,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            low_cpu_mem_usage=True,
        )
        self.model.eval()
    
    def generate_response(
        self,
        text: str,
        image: Optional[Image.Image] = None,
        max_new_tokens: Optional[int] = None,
        min_new_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
        top_p: Optional[float] = None,
        top_k: Optional[int] = None,
        do_sample: Optional[bool] = None,
        num_beams: Optional[int] = None,
        length_penalty: Optional[float] = None,
        no_repeat_ngram_size: Optional[int] = None,
        diversity_penalty: Optional[float] = None,
        early_stopping: Optional[bool] = None,
        repetition_penalty: Optional[float] = None,
        preset: Optional[str] = None,
    ) -> str:
        """
        ãƒ†ã‚­ã‚¹ãƒˆãƒ»ç”»åƒã‹ã‚‰å¿œç­”ã‚’ç”Ÿæˆï¼ˆãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ç‰ˆï¼‰
        
        Args:
            text: å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ
            image: å…¥åŠ›ç”»åƒï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            max_new_tokens: ç”Ÿæˆã™ã‚‹æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°
            min_new_tokens: ç”Ÿæˆã™ã‚‹æœ€å°ãƒˆãƒ¼ã‚¯ãƒ³æ•°
            temperature: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ¸©åº¦ï¼ˆ0.0-1.0ï¼‰
            top_p: æ ¸ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ç¢ºç‡
            top_k: ä¸Šä½Kå€‹ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è€ƒæ…®
            do_sample: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®æœ‰åŠ¹/ç„¡åŠ¹
            num_beams: ãƒ“ãƒ¼ãƒ æ¢ç´¢ã®ãƒ“ãƒ¼ãƒ æ•°
            length_penalty: ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ãƒšãƒŠãƒ«ãƒ†ã‚£
            no_repeat_ngram_size: n-gramã®ç¹°ã‚Šè¿”ã—æŠ‘åˆ¶ã‚µã‚¤ã‚º
            diversity_penalty: å¤šæ§˜æ€§ãƒšãƒŠãƒ«ãƒ†ã‚£ï¼ˆãƒ“ãƒ¼ãƒ åˆ†å²å‘ã‘ï¼‰
            early_stopping: æ—©æœŸåœæ­¢ãƒ•ãƒ©ã‚°
            repetition_penalty: ç¹°ã‚Šè¿”ã—ãƒšãƒŠãƒ«ãƒ†ã‚£
            preset: ã‚¿ã‚¹ã‚¯åˆ¥ãƒ—ãƒªã‚»ãƒƒãƒˆ ("accurate", "balanced", "creative")
        
        Returns:
            ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
        """
        if self.model is None or self.processor is None:
            raise RuntimeError("ãƒ¢ãƒ‡ãƒ«ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        # å¤‰æ•°ã‚’äº‹å‰å®šç¾©ï¼ˆfinallyå¥ã§ç¢ºå®Ÿã«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã™ã‚‹ãŸã‚ï¼‰
        inputs = None
        outputs = None
        response = None
        
        try:
            # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®šï¼ˆå„ªå…ˆé †ä½: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ < preset < å€‹åˆ¥æŒ‡å®šï¼‰
            gen_config = self.DEFAULT_GENERATION_CONFIG.copy()
            
            # ãƒ—ãƒªã‚»ãƒƒãƒˆé©ç”¨
            if preset and preset in self.TASK_PRESETS:
                gen_config.update(self.TASK_PRESETS[preset])
            
            # å€‹åˆ¥æŒ‡å®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ä¸Šæ›¸ã
            if max_new_tokens is not None:
                gen_config["max_new_tokens"] = max_new_tokens
            if min_new_tokens is not None:
                gen_config["min_new_tokens"] = min_new_tokens
            if temperature is not None:
                gen_config["temperature"] = temperature
            if top_p is not None:
                gen_config["top_p"] = top_p
            if top_k is not None:
                gen_config["top_k"] = top_k
            if do_sample is not None:
                gen_config["do_sample"] = do_sample
            if num_beams is not None:
                gen_config["num_beams"] = num_beams
            if length_penalty is not None:
                gen_config["length_penalty"] = length_penalty
            if no_repeat_ngram_size is not None:
                gen_config["no_repeat_ngram_size"] = no_repeat_ngram_size
            if diversity_penalty is not None:
                gen_config["diversity_penalty"] = diversity_penalty
            if early_stopping is not None:
                gen_config["early_stopping"] = early_stopping
            if repetition_penalty is not None:
                gen_config["repetition_penalty"] = repetition_penalty
            
            # ğŸ”¥ é‡è¦: use_cacheã¯modeling_sailvl.pyã§å¼·åˆ¶çš„ã«TrueãŒè¨­å®šã•ã‚Œã‚‹ãŸã‚ã€
            #         ã“ã“ã§ã¯è¨­å®šã—ãªã„ï¼ˆäºŒé‡è¨­å®šã«ã‚ˆã‚‹"got multiple values"ã‚¨ãƒ©ãƒ¼å›é¿ï¼‰
            # å‚è€ƒ: vlm/SAIL-VL2-2B/modeling_sailvl.py:345 ã§ use_cache=True ãŒãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰
            
            # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ§‹ç¯‰
            content = []
            if image is not None:
                content.append({"type": "image"})
            content.append({"type": "text", "text": text})
            
            messages = [{"role": "user", "content": content}]
            
            # å…¥åŠ›ã‚’æº–å‚™
            text_prompt = self.processor.apply_chat_template(
                messages, add_generation_prompt=True, tokenize=False
            )
            inputs = self.processor(images=image, text=text_prompt, return_tensors="pt")
            inputs = {k: v.to(self.device) for k, v in inputs.items() if isinstance(v, torch.Tensor)}
            
            # æ¨è«–
            with torch.no_grad():
                outputs = self.model.generate(**inputs, **gen_config)
            
            # ãƒ‡ã‚³ãƒ¼ãƒ‰
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆéƒ¨åˆ†ã‚’é™¤å»
            if "<|im_start|>assistant" in response:
                response = response.split("<|im_start|>assistant")[-1].strip()
            
            return response
        
        finally:
            # ğŸ”¥ é‡è¦: ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚’ç¢ºå®Ÿã«å®Ÿè¡Œ
            self._cleanup_after_generation(inputs, outputs)
    
    def _cleanup_after_generation(self, inputs: Optional[Dict] = None, outputs: Optional[torch.Tensor] = None):
        """
        ç”Ÿæˆå¾Œã®ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆOSãƒ€ã‚¦ãƒ³é˜²æ­¢ã®æœ€é‡è¦å‡¦ç†ï¼‰
        """
        import gc
        
        # å…¥åŠ›ãƒ†ãƒ³ã‚½ãƒ«ã®å‰Šé™¤
        if inputs is not None:
            for key in list(inputs.keys()):
                if isinstance(inputs[key], torch.Tensor):
                    del inputs[key]
            del inputs
        
        # å‡ºåŠ›ãƒ†ãƒ³ã‚½ãƒ«ã®å‰Šé™¤
        if outputs is not None:
            del outputs
        
        # ãƒ¢ãƒ‡ãƒ«ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢ï¼ˆpast_key_valuesãªã©ã®ç´¯ç©é˜²æ­¢ï¼‰
        if self.model is not None:
            # KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒå­˜åœ¨ã™ã‚‹å ´åˆã¯ã‚¯ãƒªã‚¢
            if hasattr(self.model, 'past_key_values'):
                self.model.past_key_values = None
        
        # Python GCå®Ÿè¡Œ
        gc.collect()
        
        # CUDA ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢ï¼ˆé‡è¦ï¼ï¼‰
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()  # GPUå‡¦ç†ã®å®Œå…¨åŒæœŸ
    
    def cleanup_model(self):
        """
        ãƒ¢ãƒ‡ãƒ«ã®å®Œå…¨ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼çµ‚äº†æ™‚ç”¨ï¼‰
        """
        import gc
        
        print("VLMãƒ¢ãƒ‡ãƒ«ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—é–‹å§‹...")
        
        # ãƒ¢ãƒ‡ãƒ«ã‚’CPUã«ç§»å‹•ã—ã¦ã‹ã‚‰ãƒ¡ãƒ¢ãƒªè§£æ”¾
        if self.model is not None:
            try:
                self.model.cpu()
            except:
                pass
            del self.model
            self.model = None
        
        # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼å‰Šé™¤
        if self.tokenizer is not None:
            del self.tokenizer
            self.tokenizer = None
        
        # ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼å‰Šé™¤
        if self.processor is not None:
            del self.processor
            self.processor = None
        
        # Python GCå¼·åˆ¶å®Ÿè¡Œ
        gc.collect()
        
        # CUDAå®Œå…¨ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
            torch.cuda.ipc_collect()  # ãƒ—ãƒ­ã‚»ã‚¹é–“å…±æœ‰ãƒ¡ãƒ¢ãƒªã‚‚ã‚¯ãƒªã‚¢
        
        print("VLMãƒ¢ãƒ‡ãƒ«ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†")
    
    def get_device_info(self) -> Dict[str, Any]:
        """ãƒ‡ãƒã‚¤ã‚¹æƒ…å ±ã‚’å–å¾—"""
        info = {
            "device": self.device,
            "cuda_available": torch.cuda.is_available(),
        }
        
        if torch.cuda.is_available():
            info["gpu_name"] = torch.cuda.get_device_name(0)
            info["gpu_memory_total"] = torch.cuda.get_device_properties(0).total_memory / (1024**3)
            info["gpu_memory_allocated"] = torch.cuda.memory_allocated() / (1024**3)
        
        return info
    
    def get_preset_names(self):
        """åˆ©ç”¨å¯èƒ½ãªãƒ—ãƒªã‚»ãƒƒãƒˆåã®ãƒªã‚¹ãƒˆã‚’å–å¾—"""
        return list(self.TASK_PRESETS.keys())
    
    def get_preset_config(self, preset_name: str) -> Optional[Dict]:
        """æŒ‡å®šãƒ—ãƒªã‚»ãƒƒãƒˆã®è¨­å®šã‚’å–å¾—"""
        return self.TASK_PRESETS.get(preset_name)
